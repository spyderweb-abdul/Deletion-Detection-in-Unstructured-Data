{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "missing_link_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Y0rT8ldQ8y1R9_0-jA59qOs_1gadMwDe",
      "authorship_tag": "ABX9TyOuh4I2IOQ9uQ3iRtf0UE0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spyderweb-abdul/Pattern-Recognition-and-Reconstruction-Detecting-Malicious-Deletions-in-Textual-Communications/blob/main/missing_link_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRgFQaGhJTZ_"
      },
      "source": [
        "import os, sys\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#nb_path = '/content/libraries'\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/VGRNN/')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n",
        "\n",
        "#os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "#sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB5xXwYNKQeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7403cbc-e25e-4567-e26b-688e1e597bc6"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import io\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.ndimage import rotate\n",
        "from torch.distributions.uniform import Uniform\n",
        "from torch.distributions.normal import Normal\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "# from torch_geometric import nn as tgnn\n",
        "from input_data import load_data\n",
        "from preprocessing import preprocess_graph, construct_feed_dict, sparse_to_tuple, mask_test_edges\n",
        "import scipy.sparse as sp\n",
        "from scipy.linalg import block_diag\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import tarfile\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import time\n",
        "\n",
        "#!pip uninstall torch-scatter torch-sparse torch-geometric\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.6.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.6.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "import torch_scatter\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_add\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, degree, segregate_self_loops\n",
        "#from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "import scipy.io as sio\n",
        "\n",
        "import inspect\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "!pip install sparse\n",
        "import sparse\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sparse in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sparse) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from sparse) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->sparse) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->sparse) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ryNuuLkKjAv"
      },
      "source": [
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "# utility functions\n",
        "\n",
        "def uniform(size, tensor):\n",
        "    stdv = 1.0 / math.sqrt(size)\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    stdv = math.sqrt(6.0 / (tensor.size(0) + tensor.size(1)))\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)\n",
        "\n",
        "\n",
        "def ones(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(1)\n",
        "\n",
        "\n",
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "def tuple_to_array(lot):\n",
        "    out = np.array(list(lot[0]))\n",
        "    for i in range(1, len(lot)):\n",
        "        out = np.vstack((out, np.array(list(lot[i]))))\n",
        "    \n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-RKqWpKt3Q"
      },
      "source": [
        "# masking functions\n",
        "\n",
        "def mask_edges_det(adjs_list):\n",
        "\n",
        "    adj_train_l, train_edges_l, val_edges_l = [], [], []\n",
        "    val_edges_false_l, test_edges_l, test_edges_false_l = [], [], []\n",
        "    edges_list = []\n",
        "    for i in range(0, len(adjs_list)):\n",
        "        # Function to build test set with 10% positive links\n",
        "        # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "        \n",
        "        adj = adjs_list[i]\n",
        "        # Remove diagonal elements\n",
        "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "        adj.eliminate_zeros()\n",
        "\n",
        "        # Check that diag is zero:\n",
        "        assert np.diag(adj.todense()).sum() == 0\n",
        "        \n",
        "        #get the upper trianglar portion of the matrix.\n",
        "        adj_triu = sp.triu(adj)\n",
        "\n",
        "        #convert the matrix into a tuple of the format: ((([1, 10]), ([1, 1, 1,..., 1, 1, 1])),...)\n",
        "        adj_tuple = sparse_to_tuple(adj_triu)\n",
        "\n",
        "        #get only the 0 index of the tuple. Returns as list: [[1 10],[1 12],[1 4],[20 25]]\n",
        "        #shape: (n, 2)\n",
        "        edges = adj_tuple[0]\n",
        "\n",
        "        #convert the adj sparse matrix to tuple and return the result of the 0 index of the tuple\n",
        "        edges_all = sparse_to_tuple(adj)[0]\n",
        "\n",
        "        #get the number of test set: row number(n)/10\n",
        "        num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "      \n",
        "        #get the number of the validation set: row number(n)/20\n",
        "        num_val = int(np.floor(edges.shape[0] /20.))\n",
        "    \n",
        "        #list numbers of edge index based on the row axis of the edges\n",
        "        #all_edge_idx = range(edges.shape[0])\n",
        "        all_edge_idx = list(range(edges.shape[0]))\n",
        "\n",
        "        #randomize the result\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        #get validation edge index from the randomized edge list. Extract only numbers equal to num_val\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "\n",
        "        #get test edge index from the randomized edge list.\n",
        "        #Extract only numbers equal to [num_val : (num_val + num_test)]\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "\n",
        "        #get the main test edge set by extracting values fom the edge list indexed by the test_edge_idx list\n",
        "        test_edges = edges[test_edge_idx]\n",
        "        \n",
        "        #get the main validation edge set by extracting values fom the edge list indexed by the test_edge_idx list\n",
        "        val_edges = edges[val_edge_idx]\n",
        "\n",
        "        #delete the stacked test and validation edge set (along the axis=0) from the list of edges. \n",
        "        #This will be the training set\n",
        "        # [[162 165], [162 169], [162 172], [171 174]]\n",
        "\n",
        "        train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "        \n",
        "        #append the list of main edges\n",
        "        edges_list.append(edges)\n",
        "        \n",
        "        def ismember(a, b, tol=5):\n",
        "            #Test whether all array elements along a given axis evaluate to True. (np.all)\n",
        "            rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "            return np.any(rows_close)  #np.any evaluate whether any elements evaluate to True\n",
        "\n",
        "        #get false edge test set\n",
        "        test_edges_false = []\n",
        "        #Do while test_egde_false list length is still less than the tst_edge list\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            #get random integers between 0 (lower) and the row size of the adj (higher)\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "\n",
        "            #if right and left values are equal, go back to the top loop\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            #if the tuple of the 2 values are part of edges_all (returns a bool), back to top\n",
        "            if ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            #if the empty test_edges_false list is not None, check the conditions\n",
        "            if test_edges_false:\n",
        "                #if the tuple of the 2 values are part of test_edges_false list, back to top\n",
        "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                    continue\n",
        "            #append result to the test_edges_false list\n",
        "            test_edges_false.append([idx_i, idx_j])  #result sample: [[19, 2], [177, 163], [15, 119], [3, 155],...] \n",
        "\n",
        "        \n",
        "        #get false validation edge set    \n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], val_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], val_edges):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        r\"\"\" The assert keyword lets you test if a condition in your code returns True, \n",
        "        if not, the program will raise an AssertionError.\n",
        "\n",
        "        #we assert the truthfulness of these conditions. \n",
        "        #check to confirm that the values (arg: 1) are bitwise NOT (tilde)\n",
        "        #in the set of values (arg: 2) in the other list.\"\"\"\n",
        "\n",
        "        assert ~ismember(test_edges_false, edges_all)\n",
        "        assert ~ismember(val_edges_false, edges_all)\n",
        "        assert ~ismember(val_edges, train_edges)\n",
        "        assert ~ismember(test_edges, train_edges)\n",
        "        assert ~ismember(val_edges, test_edges)\n",
        "\n",
        "        #get np.ones of elements of the row size of the train_edges\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "\n",
        "        # Re-build adj matrix for the training set\n",
        "        r\"\"\" [ : , 0 ] means (more or less) [ first_row:last_row , column_0 ]. \n",
        "        If you have a 2-dimensional list/matrix/array, this notation will give you all \n",
        "        the values in column 0 (from all rows).\"\"\"\n",
        "\n",
        "        adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "\n",
        "        #add the new adjacency matrix to its transpose\n",
        "        adj_train = adj_train + adj_train.T\n",
        "\n",
        "        #fill all the initialised list\n",
        "        adj_train_l.append(adj_train)\n",
        "        train_edges_l.append(train_edges)\n",
        "        val_edges_l.append(val_edges)\n",
        "        test_edges_l.append(test_edges)\n",
        "        val_edges_false_l.append(val_edges_false)\n",
        "        test_edges_false_l.append(test_edges_false)\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train_l, train_edges_l, val_edges_l, val_edges_false_l, test_edges_l, test_edges_false_l\n",
        "    \n",
        "   \n",
        "def mask_edges_prd(adjs_list):\n",
        "    pos_edges_l , false_edges_l = [], []\n",
        "    edges_list = []\n",
        "    for i in range(0, len(adjs_list)):\n",
        "        # Function to build test set with 10% positive links\n",
        "        # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "        \n",
        "        adj = adjs_list[i]\n",
        "        # Remove diagonal elements\n",
        "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "        adj.eliminate_zeros()\n",
        "        # Check that diag is zero:\n",
        "        assert np.diag(adj.todense()).sum() == 0\n",
        "        \n",
        "        adj_triu = sp.triu(adj)\n",
        "        adj_tuple = sparse_to_tuple(adj_triu)\n",
        "        edges = adj_tuple[0]\n",
        "        edges_all = sparse_to_tuple(adj)[0]\n",
        "        num_false = int(edges.shape[0])\n",
        "        \n",
        "        pos_edges_l.append(edges)\n",
        "        \n",
        "        def ismember(a, b, tol=5):\n",
        "            rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "            return np.any(rows_close)\n",
        "        \n",
        "        edges_false = []\n",
        "        while len(edges_false) < num_false:\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(edges_false)):\n",
        "                    continue\n",
        "            edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        assert ~ismember(edges_false, edges_all)\n",
        "        \n",
        "        false_edges_l.append(edges_false)\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return pos_edges_l, false_edges_l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxYyTQxJK1m4"
      },
      "source": [
        "# loading data\n",
        "\n",
        "path = 'drive/My Drive/Colab Notebooks/VGRNN/data/'\n",
        "# # Enron dataset\n",
        "with open(path+'enron_data/enron_adj_sparse_matrix.pickle', 'rb') as handle:\n",
        "     adj_sparse_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data/enron_adj_dense_matrix.pickle', 'rb') as handle:\n",
        "     adj_dense_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data/enron_edge_attribute_matrix.pickle', 'rb') as handle:\n",
        "     edge_attr_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data/enron_node_attribute_matrix.pickle', 'rb') as handle:\n",
        "     node_attr_matrix = pickle.load(handle)\n",
        "\n",
        "adj_sparse_matrix = adj_sparse_matrix[7:34]                       #80\n",
        "adj_dense_matrix = adj_dense_matrix[7:34]            #80\n",
        "edge_attr_matrix = edge_attr_matrix[7:34]            #80\n",
        "node_attr_matrix = node_attr_matrix[7:34]  \n",
        "\n",
        "\n",
        "#print(adj_sparse_matrix)\n",
        "outs = mask_edges_det(adj_sparse_matrix)\n",
        "\n",
        "#reconstructed adjacency matrix of the training set\n",
        "adj_train_l = outs[0]                 #80\n",
        "\n",
        "#List of training edge set\n",
        "train_edges_l = outs[1]               #80\n",
        "\n",
        "#List of validation edge set\n",
        "val_edges_l = outs[2]                 #80\n",
        "\n",
        "#List of false validation edge set(i.e., never exist)\n",
        "val_edges_false_l = outs[3]           #80\n",
        "\n",
        "#List of test edge set\n",
        "test_edges_l = outs[4]                #80\n",
        "\n",
        "#List of false test edge set \n",
        "test_edges_false_l = outs[5]          #80\n",
        "\n",
        "\n",
        "pos_edges_l, false_edges_l = mask_edges_prd(adj_sparse_matrix)\n",
        "#pos_samples, neg_samples = mask_edges_prd_new(adj_sparse_matrix, adj_dense_matrix)\n",
        "\n",
        "\n",
        "# creating edge list\n",
        "edge_idx_list = []                    #80\n",
        "\n",
        "for i in range(len(train_edges_l)):\n",
        "    edge_idx_list.append(torch.tensor(np.transpose(train_edges_l[i]), dtype=torch.long))\n",
        "\n",
        "#print('Training edges: ', edge_idx_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8EhQwXnK-iv"
      },
      "source": [
        "# layers\n",
        "\n",
        "class E_GCN_Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act=F.relu, improved=True, bias=True, num_channels=10, aggr='sum'):\n",
        "        super(E_GCN_Conv, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels            #[64]\n",
        "        self.out_channels = out_channels          #[32]\n",
        "        self.act = act\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels, num_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels, num_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if (aggr == 'concat'):\n",
        "            self.aggr = 'concat'\n",
        "            self.last_ops = nn.Linear(self.out_channels * self.num_channels, self.out_channels)\n",
        "        elif (aggr == 'sum'):\n",
        "            self.aggr = 'sum'\n",
        "            self.last_ops = nn.Linear(self.out_channels, self.out_channels)\n",
        "        \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "\n",
        "        #print(edge_index.size())\n",
        "        #print(edge_attr.size())\n",
        "\n",
        "        #add or remove node self loop. We remove in our case\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "                \n",
        "        #edge index rows and column representation\n",
        "        row, col = edge_index     #[21]\n",
        "\n",
        "        #normalize the adjacency matrix\n",
        "        #deg = scatter_add(edge_attr, row, dim=0, dim_size=x.size(0))\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        #reshape the row and column vectors\n",
        "        deg_inv_sqrt_row = deg_inv_sqrt[row].view(-1, 1)      #[[1.0000],[1.0000]]\n",
        "        deg_inv_sqrt_col = deg_inv_sqrt[col].view(-1, 1)      #[[0.5774],[0.0000]]\n",
        "\n",
        "        #multiply row and col vectors with edge weights (We replace the adjacencodery matrix with the edge tensor)\n",
        "        norm_edge = deg_inv_sqrt_row * edge_attr * deg_inv_sqrt_col     #size([edge_index[row/col] No., 14])\n",
        "\n",
        "\n",
        "        #Slice and list the normalized vectors based on the nu. of channels\n",
        "        norm = []\n",
        "        for i in range(0, edge_attr.size()[1]):\n",
        "            norm.append(norm_edge[:, i:i+1])\n",
        "\n",
        "        node_state_list = []\n",
        "        #for each edge channels, we perform a weoghted convolution with edge weights as co-efficient\n",
        "        for c in range(self.num_channels):\n",
        "            if self.in_channels > self.out_channels:\n",
        "\n",
        "                #if the weight matrix is not none\n",
        "                if self.weight is not None:\n",
        "                    #matrix product of the node (hidden state) with the weight matrix\n",
        "                    weighted_nodes = torch.matmul(x, self.weight[:, :, c])      #(size[149, 32])\n",
        "                else:\n",
        "                    #otherwise, hidden state remains same\n",
        "                    weighted_nodes = x\n",
        "                \n",
        "                #if vectors are normalized\n",
        "                if norm is not None: \n",
        "                    #multiply each element in the each channels of the norm with weighted hidden state             \n",
        "                    weighted_conv = torch.mul(norm[c], weighted_nodes[row])      #size(21, 32)\n",
        "\n",
        "                    #propagate messages through all edges and update the nodes\n",
        "                    weighted_conv_sum = scatter_add(weighted_conv, col, dim=0, dim_size=x.size(0)) #size(149, 32)\n",
        "                else:\n",
        "                    weighted_conv_sum = scatter_add(weighted_nodes[row], col, dim=0, dim_size=x.size(0))\n",
        "\n",
        "                channel_node_state = weighted_conv_sum\n",
        "\n",
        "            else:\n",
        "                if norm is not None:\n",
        "                    unweighted_conv = torch.mul(norm[c], x[row])\n",
        "                    unweighted_conv_sum = scatter_add(unweighted_conv, col, dim=0, dim_size=x.size(0))\n",
        "                else:\n",
        "                    unweighted_conv_sum = scatter_add(x[row], col, dim=0, dim_size=x.size(0))\n",
        "                \n",
        "                if self.weight is not None:\n",
        "                    channel_node_state = torch.matmul(unweighted_conv_sum.float(), self.weight[:, :, c])\n",
        "            \n",
        "            #add linear bias if True\n",
        "            if self.bias is not None:\n",
        "                channel_node_state = channel_node_state + self.bias[:, c]\n",
        "            \n",
        "            #pass param through a linear activation function\n",
        "            channel_node_state = self.act(channel_node_state)\n",
        "            #append each channel to node state list\n",
        "            node_state_list.append(channel_node_state)        #size(N, 32/16)\n",
        "\n",
        "        #we consider two aggregation method across each channels of the edge weights\n",
        "        #1. Sum aggregation method \n",
        "        if (self.aggr == 'sum'):\n",
        "            node_states = torch.stack(node_state_list, dim=1).sum(1).float()     #[N, 32]\n",
        "        #2. Concat aggregation method               \n",
        "        elif (self.aggr == 'concat'): \n",
        "            node_states = torch.cat(node_state_list, dim=1).float()\n",
        "            \n",
        "        #pass aggregated vectors through a flexible linear transformation layer      \n",
        "        out = self.last_ops(node_states)                        #size(N, 32/16)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, \n",
        "                                   self.out_channels, self.num_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwpqrN33LFIJ"
      },
      "source": [
        "class gru_gcn(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layer, bias=True):\n",
        "        super(gru_gcn, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layer = n_layer\n",
        "        \n",
        "        # gru weights\n",
        "        self.weight_xz = []\n",
        "        self.weight_hz = []\n",
        "        self.weight_xr = []\n",
        "        self.weight_hr = []\n",
        "        self.weight_xh = []\n",
        "        self.weight_hh = []\n",
        "        \n",
        "        for i in range(self.n_layer):\n",
        "            if i==0:\n",
        "                self.weight_xz.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xr.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xh.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "            else:\n",
        "                self.weight_xz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "    \n",
        "    def forward(self, inp, edge_index, edge_tensor, h):\n",
        "        h_out = torch.zeros(h.size())\n",
        "        for i in range(self.n_layer):\n",
        "            if i==0:\n",
        "                \n",
        "                z_g = torch.sigmoid(self.weight_xz[i](inp, edge_index, edge_tensor) + self.weight_hz[i](h[i], edge_index, edge_tensor))\n",
        "                r_g = torch.sigmoid(self.weight_xr[i](inp, edge_index, edge_tensor) + self.weight_hr[i](h[i], edge_index, edge_tensor))\n",
        "                h_tilde_g = torch.tanh(self.weight_xh[i](inp, edge_index, edge_tensor) + self.weight_hh[i](r_g * h[i], edge_index, edge_tensor))\n",
        "                h_out[i] = z_g * h[i][0: inp.size(0)] + (1 - z_g) * h_tilde_g\n",
        "        #         out = self.decoder(h_t.view(1,-1))\n",
        "            else:\n",
        "                z_g = torch.sigmoid(self.weight_xz[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hz[i](h[i], edge_index, edge_tensor))\n",
        "                r_g = torch.sigmoid(self.weight_xr[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hr[i](h[i], edge_index, edge_tensor))\n",
        "                h_tilde_g = torch.tanh(self.weight_xh[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hh[i](r_g * h[i], edge_index, edge_tensor))\n",
        "                h_out[i] = z_g * h[i] + (1 - z_g) * h_tilde_g\n",
        "        #         out = self.decoder(h_t.view(1,-1))\n",
        "        \n",
        "        out = h_out\n",
        "        return out, h_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNec0ycDLPfC"
      },
      "source": [
        "# VGRNN model\n",
        "\n",
        "class VGAE_Edge(nn.Module):\n",
        "    def __init__(self, node_feat_dim, hidden_dim, latent_var_dim, n_layers, eps, conv='GCN', bias=False):\n",
        "        super(VGAE_Edge, self).__init__()\n",
        "        \n",
        "        #input dimension\n",
        "        self.node_feat_dim = node_feat_dim        \n",
        "        self.eps = eps\n",
        "        #hidden_layer dim.\n",
        "        self.hidden_dim = hidden_dim        #32\n",
        "        #latent variable dim.\n",
        "        self.latent_var_dim = latent_var_dim        #10\n",
        "        self.n_layers = n_layers   #1\n",
        "       \n",
        "        if conv == 'GCN':\n",
        "            #flexible sequential neural network linear transformations\n",
        "            self.input_emb = nn.Sequential(nn.Linear(node_feat_dim, hidden_dim), nn.ReLU())\n",
        "            self.output_emb = nn.Sequential(nn.Linear(latent_var_dim, hidden_dim), nn.ReLU())\n",
        "            \n",
        "            #encoder functions\n",
        "            self.encoder = E_GCN_Conv(hidden_dim + hidden_dim, hidden_dim)            \n",
        "            self.encoder_mu = E_GCN_Conv(hidden_dim, latent_var_dim, act=lambda x:x)\n",
        "            self.encoder_sigma = E_GCN_Conv(hidden_dim, latent_var_dim, act=F.softplus)\n",
        "            \n",
        "            #linear linear transformation of the prior functions\n",
        "            self.prior = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
        "            self.prior_mu = nn.Sequential(nn.Linear(hidden_dim, latent_var_dim))\n",
        "            self.prior_sigma = nn.Sequential(nn.Linear(hidden_dim, latent_var_dim), nn.Softplus())\n",
        "            \n",
        "            #recurrent neural networks model function\n",
        "            self.rnn = gru_gcn(hidden_dim + hidden_dim, hidden_dim, n_layers, bias)\n",
        "          \n",
        "          \n",
        "\n",
        "    def forward(self, x, edge_idx_list, edge_attr_matrix, adj_dense_matrix, hidden_in=None):\n",
        "\n",
        "        #assert the length of edge matrix = elngth of the edge indices\n",
        "        assert len(adj_dense_matrix) == len(edge_idx_list)\n",
        "\n",
        "        #initialize params\n",
        "        kld_loss = 0\n",
        "        nll_loss = 0\n",
        "        encoder_mu_list, encoder_sigma_list = [], []\n",
        "        prior_mu_list, prior_sigma_list = [], []\n",
        "        decoded_list, z_list = [], []\n",
        "        \n",
        "        #hidden var will be none in the first set of operations\n",
        "        if hidden_in is None:\n",
        "            #so we create a matrix of zeros as initial representation\n",
        "            h = torch.zeros(self.n_layers, x.size(1), self.hidden_dim)  #size([1, 149, 32])\n",
        "        else:\n",
        "            #hidden var here will be the recurrent vectors\n",
        "            h = hidden_in\n",
        "\n",
        "        for t in range(x.size(0)):       #x.size(0) = 60\n",
        "\n",
        "            #linearly transform x features\n",
        "            input_emb_t = self.input_emb(x[t].float())              #[149, 32]\n",
        "            #edge indices at time t    \n",
        "            edge_idx_list_t = edge_idx_list[t]\n",
        "\n",
        "            #edge tensor matrix at time t => extract on the tensors associated with the edge indices at time t\n",
        "            #Note: there are 14 vectors in each edge attributes. We can reduce to 10 if we choose\n",
        "            #to extract only topics of communication. The model eval works differently for 14 and 10\n",
        "            edge_tensor_t = (edge_attr_matrix[t][edge_idx_list_t[0], edge_idx_list_t[1]])#[:, 0:latent_var_dim]\n",
        "            adj_dense_matrix_t = adj_dense_matrix[t]\n",
        "\n",
        "            \n",
        "            #encoder\n",
        "            #encoders conditioned on priors so features of previous states can be \n",
        "            #recurrently modeled\n",
        "            encoder_t = self.encoder(torch.cat([input_emb_t, h[-1]], 1), edge_idx_list_t, edge_tensor_t)    #[149, 32]\n",
        "            #encoder mean\n",
        "            encoder_mu_t = self.encoder_mu(encoder_t, edge_idx_list_t, edge_tensor_t)                   #[149, 10]\n",
        "            #encoder standar deviation\n",
        "            encoder_sigma_t = self.encoder_sigma(encoder_t, edge_idx_list_t, edge_tensor_t)                     #[149, 10]\n",
        "            \n",
        "            #prior\n",
        "            prior_t = self.prior(h[-1])                           #[149, 32]\n",
        "            prior_mu_t = self.prior_mu(prior_t)               #[149, 10]\n",
        "            prior_sigma_t = self.prior_sigma(prior_t)                 #[149, 10]\n",
        "            \n",
        "            #sampling and reparameterization\n",
        "            z_t = self._reparameterized_sample(encoder_mu_t, encoder_sigma_t)  #[149, 10]\n",
        "            #apply a fully connected layer to z_t\n",
        "            output_emb_t = self.output_emb(z_t)                                  #[149, 32]\n",
        "\n",
        "            #decoder function -> takes the linearly transformed latent variable and egde indices as args\n",
        "            decoder_t = self.dec(output_emb_t, edge_idx_list_t)\n",
        "\n",
        "            #recurrencodere\n",
        "            _, h = self.rnn(torch.cat([input_emb_t, output_emb_t], 1), edge_idx_list_t, edge_tensor_t, h)       #[1, 149, 32]\n",
        "            #print('h: ', h.size())\n",
        "            \n",
        "            num_nodes = adj_dense_matrix_t.size()[0]\n",
        "            encoder_mu_t_slice = encoder_mu_t[0:num_nodes, :]\n",
        "            encoder_sigma_t_slice = encoder_sigma_t[0:num_nodes, :]\n",
        "            prior_mu_t_slice = prior_mu_t[0:num_nodes, :]\n",
        "            prior_sigma_t_slice = prior_sigma_t[0:num_nodes, :]\n",
        "            adj_decoder_t = decoder_t[0:num_nodes, 0:num_nodes]       #Size[149, 149]\n",
        "\n",
        "            #computing losses\n",
        "            kld_loss += self.kl_divergence(encoder_mu_t_slice, encoder_sigma_t_slice, prior_mu_t_slice, prior_sigma_t_slice)\n",
        "            #kld_loss += self.kl_divergence_zu(encoder_mean_t, encoder_std_t)\n",
        "            nll_loss += self.nll_bernoulli(adj_decoder_t, adj_dense_matrix_t)\n",
        "\n",
        "            \n",
        "            encoder_sigma_list.append(encoder_sigma_t_slice)\n",
        "            encoder_mu_list.append(encoder_mu_t_slice)\n",
        "            prior_mu_list.append(prior_mu_t_slice)\n",
        "            prior_sigma_list.append(prior_sigma_t_slice)\n",
        "            z_list.append(z_t)\n",
        "            decoded_list.append(adj_decoder_t)\n",
        "\n",
        "        return kld_loss, nll_loss, encoder_mu_list, prior_mu_list, decoded_list, h\n",
        "    \n",
        "    #decoder function\n",
        "    def dec(self, z, edge_index):\n",
        "        #output = neural network decoder\n",
        "        outputs = Decoder(act=lambda x:x)(z, edge_index)\n",
        "        return outputs\n",
        "    \n",
        "    def reset_parameters(self, stdv=1e-1):\n",
        "        for weight in self.parameters():\n",
        "            weight.data.normal_(0, stdv)\n",
        "     \n",
        "    def _init_weights(self, stdv):\n",
        "        pass\n",
        "    \n",
        "    def _reparameterized_sample(self, mu, sigma):\n",
        "        eps1 = torch.FloatTensor(sigma.size()).normal_()\n",
        "        eps1 = Variable(eps1)\n",
        "        return eps1.mul(sigma).add_(mu)\n",
        "    \n",
        "    def kl_divergence(self, encoder_mu, encoder_sigma, prior_mu, prior_sigma):\n",
        "        mu_size = encoder_mu.size(0)\n",
        "        encoder_sigma_log = torch.log(encoder_sigma + self.eps)\n",
        "        prior_sigma_log = torch.log(prior_sigma + self.eps)\n",
        "        encoder_sigma = encoder_sigma + self.eps\n",
        "        prior_sigma = prior_sigma + self.eps\n",
        "\n",
        "        kld_element = (2 * prior_sigma_log - 2 * encoder_sigma_log + (torch.pow(encoder_sigma, 2) + torch.pow(encoder_mu - prior_mu, 2)) / \n",
        "                      torch.pow(prior_sigma, 2) - 1)\n",
        "        kld_element = kld_element.detach().numpy()      \n",
        "        kld_element = torch.tensor(np.nan_to_num(kld_element, copy=True, nan=0.0))\n",
        "        kld = (0.5 / mu_size) * kld_element.sum(1).mean()\n",
        "        return kld\n",
        "    \n",
        "    def kl_divergence_zu(self, mu, sigma):        \n",
        "        mu_size = mu.size(0)\n",
        "        sigma_log = torch.log(sigma + self.eps)\n",
        "        kld_element =  (1 + 2*sigma_log - (sigma**2) - (mu**2))\n",
        "        kld_element = kld_element.detach().numpy()      \n",
        "        kld_element = torch.tensor(np.nan_to_num(kld_element, copy=True, nan=0.0))        \n",
        "        return (-0.5 / mu_size) * kld_element.sum(1).mean()\n",
        "    \n",
        "    #negative log likelihood bernoulli\n",
        "    def nll_bernoulli(self, logits, target):                     \n",
        "        positive_weight = float(target.size(0) * target.size(0) - target.sum()) / target.sum()   #negative samples/positive samples\n",
        "        #norm_weight = target.size(0) * target.size(0) / float((target.size(0) * target.size(0) - target.sum())*2) \n",
        "        bce = F.binary_cross_entropy_with_logits(logits, target, pos_weight=positive_weight, reduction='mean')\n",
        "        \n",
        "        nll_loss = (-1.0 / target.size(0)) * bce\n",
        "        return - nll_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yDfLMrsLUk5"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, act=torch.sigmoid):\n",
        "        super(Decoder, self).__init__()        \n",
        "\n",
        "        self.act = act\n",
        "    def forward(self, z, edge_index):\n",
        "        z = F.dropout(z, p=0., training=True) \n",
        "\n",
        "        a_hat = torch.transpose(z, dim0=0, dim1=1)\n",
        "        a_hat = self.act(torch.mm(z, a_hat))\n",
        "\n",
        "        a_hat_z0 = z[edge_index[0]]\n",
        "        a_hat_z1 = z[edge_index[1]]\n",
        "\n",
        "        link_prob = (a_hat_z0 * a_hat_z1).sum(dim=1)\n",
        "        adj_prob = self.act(link_prob) if self.act else link_prob\n",
        "        \n",
        "        return a_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sko45PpLa6x"
      },
      "source": [
        "r\"\"\" Calculate and evaluate the Area Under (Receiver Operating Characteristic) Curve \n",
        "     and the Average Precision (AP) \"\"\"\n",
        "\n",
        "# evaluation function\n",
        "def get_eval_scores(edges_pos, edges_neg, adj_dense_matrix, a_embs):\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    auc, ap = [], []\n",
        "    \n",
        "    for i in range(len(edges_pos)):\n",
        "        \n",
        "        # Predict on test set of edges\n",
        "        #explicitly remove the computational graph of the tensor \n",
        "        #(from gradient descent) with detach and change back to numpy\n",
        "        a_emb = a_embs[i].detach().numpy()\n",
        "\n",
        "        #reconstruct the adjacency matrix of the embeddings\n",
        "        adj_emb = np.dot(a_emb, a_emb.T)          #[149, 149]\n",
        "\n",
        "        adj_dense = adj_dense_matrix[i]\n",
        "       \n",
        "        #initialize predicted edge list\n",
        "        pos_adj, pred_pos_adj = [], []\n",
        "\n",
        "        for e in edges_pos[i]:\n",
        "\n",
        "            pred_pos_adj.append(sigmoid(adj_emb[e[0], e[1]]))\n",
        "            pos_adj.append(adj_dense[e[0], e[1]])         \n",
        "\n",
        "        #print('Pos: ', pos_adj)\n",
        "        #print('\\nPred: ', pred_pos_adj)\n",
        "\n",
        "        neg_adj, pred_neg_adj = [], []\n",
        "\n",
        "        for e in edges_neg[i]:\n",
        "\n",
        "            pred_neg_adj.append(sigmoid(adj_emb[e[0], e[1]]))\n",
        "            neg_adj.append(adj_dense[e[0], e[1]])\n",
        "\n",
        "        #stack up the positive and negative predicted features\n",
        "        all_pred_adj = np.hstack([pred_pos_adj, pred_neg_adj])       \n",
        "        all_true_adj = np.hstack([np.ones(len(pred_pos_adj)), np.zeros(len(pred_neg_adj))])\n",
        "        \n",
        "        auc.append(roc_auc_score(all_true_adj, all_pred_adj))\n",
        "        ap.append(average_precision_score(all_true_adj, all_pred_adj))\n",
        "\n",
        "    return auc, ap, neg_adj, pred_neg_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42a_R7tBLiYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2c7639-1728-4380-80a5-a0522b76f39a"
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "hidden_dim = 32\n",
        "latent_var_dim = 16\n",
        "n_layers =  1\n",
        "clip = 10\n",
        "learning_rate = 1e-2\n",
        "timesteps_len = len(train_edges_l)  \n",
        "num_nodes = node_attr_matrix[0].shape[1]\n",
        "node_feat_dim = num_nodes\n",
        "eps = 1e-10\n",
        "conv_type='GCN'\n",
        "\n",
        "# creating input tensors\n",
        "node_attr = torch.stack(node_attr_matrix)   #[80, 149, 6]\n",
        "\n",
        "adj_label_list = []\n",
        "for i in range(len(adj_train_l)):\n",
        "    temp_matrix = adj_train_l[i]\n",
        "    adj_label_list.append(torch.tensor(temp_matrix.toarray().astype(np.float32))) \n",
        "\n",
        "\n",
        "# building model\n",
        "model = VGAE_Edge(node_feat_dim, hidden_dim, latent_var_dim, n_layers, eps, conv=conv_type, bias=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# training\n",
        "timesteps_init = 0\n",
        "timesteps_end = timesteps_len - 1\n",
        "test_init = 0\n",
        "\n",
        "\n",
        "#writer = SummaryWriter('drive/MyDrive/Colab Notebooks/VGRNN/tensorboard_log/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "start_time = time.monotonic()\n",
        "for k in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    kld_loss, nll_loss, _, _, _, hidden_st = model(node_attr[timesteps_init:timesteps_end]\n",
        "                                                , edge_idx_list[timesteps_init:timesteps_end]\n",
        "                                                , edge_attr_matrix[timesteps_init:timesteps_end]\n",
        "                                                , adj_dense_matrix[timesteps_init:timesteps_end]\n",
        "                                                )\n",
        "    \n",
        "    loss = kld_loss + nll_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    if k > test_init:\n",
        "        _, _, encs_, priors_, adj_dec, _ = model(node_attr[timesteps_end:timesteps_len]\n",
        "                                          , edge_idx_list[timesteps_end:timesteps_len]\n",
        "                                          , edge_attr_matrix[timesteps_end:timesteps_len]\n",
        "                                          , adj_label_list[timesteps_end:timesteps_len]\n",
        "                                          , hidden_st)\n",
        "        \n",
        "        \n",
        "        auc, ap, pos, pred = get_eval_scores(pos_edges_l[timesteps_end:timesteps_len]\n",
        "                                            , false_edges_l[timesteps_end:timesteps_len]\n",
        "                                            , adj_dense_matrix[timesteps_end:timesteps_len]\n",
        "                                            , priors_                                                        \n",
        "                                            )\n",
        "\n",
        "\n",
        "    #Note: Prior mean reduces the loss than the decoded variables. \n",
        "    print('********************************************************')\n",
        "    print('epoch: ', k)\n",
        "    print('\\nLOSS => kld_loss: {} | nll_loss: {} | loss: {}'.format( \n",
        "                                                                      round(kld_loss.mean().item(), 4)\n",
        "                                                                    , round(nll_loss.mean().item(), 4)\n",
        "                                                                    , round(loss.mean().item(), 4)\n",
        "                                                                    ))\n",
        "    #writer.add_scalar(\"Loss/train\", loss.mean().item(), k)\n",
        "    if k > test_init:\n",
        "        #writer.add_scalar(\"validation_auc\", np.mean(np.array(auc_val)), k)\n",
        "        #writer.add_scalar(\"validation_ap\", np.mean(np.array(ap_val)), k)\n",
        "        #writer.add_scalar(\"test_auc\", np.mean(np.array(auc_test)), k)\n",
        "        #writer.add_scalar(\"test_ap\", np.mean(np.array(ap_test)), k)\n",
        "\n",
        "        print('\\nADJ. RECONSTRUCTION => auc_mean: {} | ap_mean: {}'.format(round(np.mean(np.array(auc)), 4)\n",
        "                                                                          , round(np.mean(np.array(ap)), 4)\n",
        "                                                                          ))\n",
        "#writer.flush() \n",
        "#writer.close()   \n",
        "end_time = time.monotonic()\n",
        "print('Total Execution Time: {}'.format(timedelta(seconds=end_time - start_time)))\n",
        "\n",
        "#!pip install tensorboard\n",
        "#!tensorboard --logdir='drive/MyDrive/Colab Notebooks/VGRNN/tensorboard_log/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************************************\n",
            "epoch:  0\n",
            "\n",
            "LOSS => kld_loss: 16.3929 | nll_loss: 3.8883 | loss: 20.2812\n",
            "********************************************************\n",
            "epoch:  1\n",
            "\n",
            "LOSS => kld_loss: 9.0241 | nll_loss: 1.5187 | loss: 10.5427\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.7016 | ap_mean: 0.7192\n",
            "********************************************************\n",
            "epoch:  2\n",
            "\n",
            "LOSS => kld_loss: 5.9413 | nll_loss: 0.6781 | loss: 6.6194\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.7012 | ap_mean: 0.719\n",
            "********************************************************\n",
            "epoch:  3\n",
            "\n",
            "LOSS => kld_loss: 4.9395 | nll_loss: 0.3429 | loss: 5.2824\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6976 | ap_mean: 0.7159\n",
            "********************************************************\n",
            "epoch:  4\n",
            "\n",
            "LOSS => kld_loss: 4.6895 | nll_loss: 0.2719 | loss: 4.9614\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6987 | ap_mean: 0.7181\n",
            "********************************************************\n",
            "epoch:  5\n",
            "\n",
            "LOSS => kld_loss: 4.4074 | nll_loss: 0.2641 | loss: 4.6714\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6945 | ap_mean: 0.7137\n",
            "********************************************************\n",
            "epoch:  6\n",
            "\n",
            "LOSS => kld_loss: 5.0925 | nll_loss: 0.2726 | loss: 5.3651\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6925 | ap_mean: 0.7094\n",
            "********************************************************\n",
            "epoch:  7\n",
            "\n",
            "LOSS => kld_loss: 6.6191 | nll_loss: 0.2775 | loss: 6.8966\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6874 | ap_mean: 0.7065\n",
            "********************************************************\n",
            "epoch:  8\n",
            "\n",
            "LOSS => kld_loss: 7.8147 | nll_loss: 0.2814 | loss: 8.0961\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6878 | ap_mean: 0.7083\n",
            "********************************************************\n",
            "epoch:  9\n",
            "\n",
            "LOSS => kld_loss: 9.1544 | nll_loss: 0.2778 | loss: 9.4322\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6814 | ap_mean: 0.7027\n",
            "********************************************************\n",
            "epoch:  10\n",
            "\n",
            "LOSS => kld_loss: 10.417 | nll_loss: 0.2728 | loss: 10.6899\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6735 | ap_mean: 0.6993\n",
            "********************************************************\n",
            "epoch:  11\n",
            "\n",
            "LOSS => kld_loss: 11.1374 | nll_loss: 0.2706 | loss: 11.408\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6736 | ap_mean: 0.6992\n",
            "********************************************************\n",
            "epoch:  12\n",
            "\n",
            "LOSS => kld_loss: 11.825 | nll_loss: 0.26 | loss: 12.085\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6699 | ap_mean: 0.697\n",
            "********************************************************\n",
            "epoch:  13\n",
            "\n",
            "LOSS => kld_loss: 12.3627 | nll_loss: 0.2565 | loss: 12.6193\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.67 | ap_mean: 0.6997\n",
            "********************************************************\n",
            "epoch:  14\n",
            "\n",
            "LOSS => kld_loss: 13.0606 | nll_loss: 0.2501 | loss: 13.3108\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6668 | ap_mean: 0.6987\n",
            "********************************************************\n",
            "epoch:  15\n",
            "\n",
            "LOSS => kld_loss: 13.6346 | nll_loss: 0.2479 | loss: 13.8825\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6647 | ap_mean: 0.696\n",
            "********************************************************\n",
            "epoch:  16\n",
            "\n",
            "LOSS => kld_loss: 14.1817 | nll_loss: 0.2444 | loss: 14.426\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6597 | ap_mean: 0.6932\n",
            "********************************************************\n",
            "epoch:  17\n",
            "\n",
            "LOSS => kld_loss: 14.8454 | nll_loss: 0.2413 | loss: 15.0867\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6603 | ap_mean: 0.6925\n",
            "********************************************************\n",
            "epoch:  18\n",
            "\n",
            "LOSS => kld_loss: 15.6257 | nll_loss: 0.2408 | loss: 15.8665\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6597 | ap_mean: 0.6934\n",
            "********************************************************\n",
            "epoch:  19\n",
            "\n",
            "LOSS => kld_loss: 16.3261 | nll_loss: 0.239 | loss: 16.5651\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6599 | ap_mean: 0.6933\n",
            "********************************************************\n",
            "epoch:  20\n",
            "\n",
            "LOSS => kld_loss: 17.0816 | nll_loss: 0.2385 | loss: 17.3201\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6574 | ap_mean: 0.6915\n",
            "********************************************************\n",
            "epoch:  21\n",
            "\n",
            "LOSS => kld_loss: 17.7885 | nll_loss: 0.2383 | loss: 18.0268\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6576 | ap_mean: 0.6915\n",
            "********************************************************\n",
            "epoch:  22\n",
            "\n",
            "LOSS => kld_loss: 18.5609 | nll_loss: 0.2379 | loss: 18.7988\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6567 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  23\n",
            "\n",
            "LOSS => kld_loss: 19.2998 | nll_loss: 0.2379 | loss: 19.5377\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6586 | ap_mean: 0.6934\n",
            "********************************************************\n",
            "epoch:  24\n",
            "\n",
            "LOSS => kld_loss: 19.9868 | nll_loss: 0.2379 | loss: 20.2247\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6547 | ap_mean: 0.6908\n",
            "********************************************************\n",
            "epoch:  25\n",
            "\n",
            "LOSS => kld_loss: 20.7182 | nll_loss: 0.2377 | loss: 20.9559\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.659 | ap_mean: 0.6948\n",
            "********************************************************\n",
            "epoch:  26\n",
            "\n",
            "LOSS => kld_loss: 20.6039 | nll_loss: 0.2377 | loss: 20.8415\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6561 | ap_mean: 0.6929\n",
            "********************************************************\n",
            "epoch:  27\n",
            "\n",
            "LOSS => kld_loss: 21.1163 | nll_loss: 0.2377 | loss: 21.354\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6554 | ap_mean: 0.693\n",
            "********************************************************\n",
            "epoch:  28\n",
            "\n",
            "LOSS => kld_loss: 21.5971 | nll_loss: 0.2375 | loss: 21.8346\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.655 | ap_mean: 0.6926\n",
            "********************************************************\n",
            "epoch:  29\n",
            "\n",
            "LOSS => kld_loss: 22.0508 | nll_loss: 0.2376 | loss: 22.2884\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6545 | ap_mean: 0.6924\n",
            "********************************************************\n",
            "epoch:  30\n",
            "\n",
            "LOSS => kld_loss: 22.4711 | nll_loss: 0.2376 | loss: 22.7086\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6537 | ap_mean: 0.6924\n",
            "********************************************************\n",
            "epoch:  31\n",
            "\n",
            "LOSS => kld_loss: 22.8943 | nll_loss: 0.2376 | loss: 23.132\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6544 | ap_mean: 0.6927\n",
            "********************************************************\n",
            "epoch:  32\n",
            "\n",
            "LOSS => kld_loss: 23.2414 | nll_loss: 0.2376 | loss: 23.4789\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.655 | ap_mean: 0.6939\n",
            "********************************************************\n",
            "epoch:  33\n",
            "\n",
            "LOSS => kld_loss: 23.6018 | nll_loss: 0.2375 | loss: 23.8393\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6539 | ap_mean: 0.6931\n",
            "********************************************************\n",
            "epoch:  34\n",
            "\n",
            "LOSS => kld_loss: 23.9421 | nll_loss: 0.2375 | loss: 24.1796\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6534 | ap_mean: 0.6932\n",
            "********************************************************\n",
            "epoch:  35\n",
            "\n",
            "LOSS => kld_loss: 24.2122 | nll_loss: 0.2375 | loss: 24.4497\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6536 | ap_mean: 0.6933\n",
            "********************************************************\n",
            "epoch:  36\n",
            "\n",
            "LOSS => kld_loss: 24.4873 | nll_loss: 0.2376 | loss: 24.7249\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6536 | ap_mean: 0.6935\n",
            "********************************************************\n",
            "epoch:  37\n",
            "\n",
            "LOSS => kld_loss: 24.7376 | nll_loss: 0.2376 | loss: 24.9752\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6534 | ap_mean: 0.6934\n",
            "********************************************************\n",
            "epoch:  38\n",
            "\n",
            "LOSS => kld_loss: 24.974 | nll_loss: 0.2375 | loss: 25.2115\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6533 | ap_mean: 0.6937\n",
            "********************************************************\n",
            "epoch:  39\n",
            "\n",
            "LOSS => kld_loss: 25.2009 | nll_loss: 0.2376 | loss: 25.4384\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6528 | ap_mean: 0.6932\n",
            "********************************************************\n",
            "epoch:  40\n",
            "\n",
            "LOSS => kld_loss: 25.4987 | nll_loss: 0.2376 | loss: 25.7362\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6544 | ap_mean: 0.6947\n",
            "********************************************************\n",
            "epoch:  41\n",
            "\n",
            "LOSS => kld_loss: 27.0431 | nll_loss: 0.2375 | loss: 27.2807\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6528 | ap_mean: 0.6934\n",
            "********************************************************\n",
            "epoch:  42\n",
            "\n",
            "LOSS => kld_loss: 27.0619 | nll_loss: 0.2375 | loss: 27.2994\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6525 | ap_mean: 0.6932\n",
            "********************************************************\n",
            "epoch:  43\n",
            "\n",
            "LOSS => kld_loss: 27.1794 | nll_loss: 0.2376 | loss: 27.417\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6521 | ap_mean: 0.6937\n",
            "********************************************************\n",
            "epoch:  44\n",
            "\n",
            "LOSS => kld_loss: 27.2917 | nll_loss: 0.2375 | loss: 27.5292\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.652 | ap_mean: 0.693\n",
            "********************************************************\n",
            "epoch:  45\n",
            "\n",
            "LOSS => kld_loss: 27.3858 | nll_loss: 0.2375 | loss: 27.6233\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6515 | ap_mean: 0.6944\n",
            "********************************************************\n",
            "epoch:  46\n",
            "\n",
            "LOSS => kld_loss: 27.4868 | nll_loss: 0.2376 | loss: 27.7243\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6515 | ap_mean: 0.6917\n",
            "********************************************************\n",
            "epoch:  47\n",
            "\n",
            "LOSS => kld_loss: 27.5776 | nll_loss: 0.2375 | loss: 27.8152\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6512 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  48\n",
            "\n",
            "LOSS => kld_loss: 27.6566 | nll_loss: 0.2376 | loss: 27.8941\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6512 | ap_mean: 0.6915\n",
            "********************************************************\n",
            "epoch:  49\n",
            "\n",
            "LOSS => kld_loss: 27.7223 | nll_loss: 0.2375 | loss: 27.9598\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.651 | ap_mean: 0.6917\n",
            "********************************************************\n",
            "epoch:  50\n",
            "\n",
            "LOSS => kld_loss: 27.7963 | nll_loss: 0.2375 | loss: 28.0338\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6513 | ap_mean: 0.6924\n",
            "********************************************************\n",
            "epoch:  51\n",
            "\n",
            "LOSS => kld_loss: 27.8589 | nll_loss: 0.2375 | loss: 28.0964\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.651 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  52\n",
            "\n",
            "LOSS => kld_loss: 27.9178 | nll_loss: 0.2375 | loss: 28.1554\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  53\n",
            "\n",
            "LOSS => kld_loss: 27.9726 | nll_loss: 0.2376 | loss: 28.2102\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  54\n",
            "\n",
            "LOSS => kld_loss: 28.0177 | nll_loss: 0.2376 | loss: 28.2553\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6513 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  55\n",
            "\n",
            "LOSS => kld_loss: 28.0687 | nll_loss: 0.2375 | loss: 28.3063\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  56\n",
            "\n",
            "LOSS => kld_loss: 28.0988 | nll_loss: 0.2375 | loss: 28.3363\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  57\n",
            "\n",
            "LOSS => kld_loss: 28.1417 | nll_loss: 0.2375 | loss: 28.3792\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  58\n",
            "\n",
            "LOSS => kld_loss: 28.1483 | nll_loss: 0.2375 | loss: 28.3859\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  59\n",
            "\n",
            "LOSS => kld_loss: 28.1816 | nll_loss: 0.2376 | loss: 28.4191\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.651 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  60\n",
            "\n",
            "LOSS => kld_loss: 28.212 | nll_loss: 0.2375 | loss: 28.4495\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.6917\n",
            "********************************************************\n",
            "epoch:  61\n",
            "\n",
            "LOSS => kld_loss: 28.239 | nll_loss: 0.2375 | loss: 28.4765\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6511 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  62\n",
            "\n",
            "LOSS => kld_loss: 28.2636 | nll_loss: 0.2376 | loss: 28.5011\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  63\n",
            "\n",
            "LOSS => kld_loss: 28.2835 | nll_loss: 0.2376 | loss: 28.5211\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  64\n",
            "\n",
            "LOSS => kld_loss: 28.2917 | nll_loss: 0.2376 | loss: 28.5292\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  65\n",
            "\n",
            "LOSS => kld_loss: 28.296 | nll_loss: 0.2376 | loss: 28.5336\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  66\n",
            "\n",
            "LOSS => kld_loss: 28.2954 | nll_loss: 0.2375 | loss: 28.5329\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.651 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  67\n",
            "\n",
            "LOSS => kld_loss: 28.3089 | nll_loss: 0.2375 | loss: 28.5464\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6507 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  68\n",
            "\n",
            "LOSS => kld_loss: 28.3189 | nll_loss: 0.2376 | loss: 28.5565\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6508 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  69\n",
            "\n",
            "LOSS => kld_loss: 28.3277 | nll_loss: 0.2376 | loss: 28.5653\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  70\n",
            "\n",
            "LOSS => kld_loss: 28.3363 | nll_loss: 0.2375 | loss: 28.5738\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6507 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  71\n",
            "\n",
            "LOSS => kld_loss: 28.3334 | nll_loss: 0.2376 | loss: 28.571\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.651 | ap_mean: 0.6924\n",
            "********************************************************\n",
            "epoch:  72\n",
            "\n",
            "LOSS => kld_loss: 28.3382 | nll_loss: 0.2375 | loss: 28.5758\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6917\n",
            "********************************************************\n",
            "epoch:  73\n",
            "\n",
            "LOSS => kld_loss: 28.3419 | nll_loss: 0.2375 | loss: 28.5794\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6504 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  74\n",
            "\n",
            "LOSS => kld_loss: 28.3453 | nll_loss: 0.2375 | loss: 28.5829\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6506 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  75\n",
            "\n",
            "LOSS => kld_loss: 28.3472 | nll_loss: 0.2375 | loss: 28.5847\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  76\n",
            "\n",
            "LOSS => kld_loss: 28.35 | nll_loss: 0.2376 | loss: 28.5876\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6928\n",
            "********************************************************\n",
            "epoch:  77\n",
            "\n",
            "LOSS => kld_loss: 28.3524 | nll_loss: 0.2376 | loss: 28.59\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  78\n",
            "\n",
            "LOSS => kld_loss: 28.3547 | nll_loss: 0.2375 | loss: 28.5922\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  79\n",
            "\n",
            "LOSS => kld_loss: 28.3583 | nll_loss: 0.2376 | loss: 28.5958\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6504 | ap_mean: 0.6918\n",
            "********************************************************\n",
            "epoch:  80\n",
            "\n",
            "LOSS => kld_loss: 28.3596 | nll_loss: 0.2376 | loss: 28.5972\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6505 | ap_mean: 0.6921\n",
            "********************************************************\n",
            "epoch:  81\n",
            "\n",
            "LOSS => kld_loss: 28.3618 | nll_loss: 0.2375 | loss: 28.5993\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6498 | ap_mean: 0.6915\n",
            "********************************************************\n",
            "epoch:  82\n",
            "\n",
            "LOSS => kld_loss: 28.3653 | nll_loss: 0.2376 | loss: 28.6029\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6509 | ap_mean: 0.6923\n",
            "********************************************************\n",
            "epoch:  83\n",
            "\n",
            "LOSS => kld_loss: 28.3619 | nll_loss: 0.2376 | loss: 28.5995\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  84\n",
            "\n",
            "LOSS => kld_loss: 28.3628 | nll_loss: 0.2376 | loss: 28.6004\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6507 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  85\n",
            "\n",
            "LOSS => kld_loss: 28.358 | nll_loss: 0.2375 | loss: 28.5955\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6505 | ap_mean: 0.6917\n",
            "********************************************************\n",
            "epoch:  86\n",
            "\n",
            "LOSS => kld_loss: 28.3558 | nll_loss: 0.2375 | loss: 28.5934\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6501 | ap_mean: 0.6933\n",
            "********************************************************\n",
            "epoch:  87\n",
            "\n",
            "LOSS => kld_loss: 28.3574 | nll_loss: 0.2375 | loss: 28.5949\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6506 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  88\n",
            "\n",
            "LOSS => kld_loss: 28.354 | nll_loss: 0.2375 | loss: 28.5915\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  89\n",
            "\n",
            "LOSS => kld_loss: 28.3419 | nll_loss: 0.2376 | loss: 28.5795\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6505 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  90\n",
            "\n",
            "LOSS => kld_loss: 28.3196 | nll_loss: 0.2375 | loss: 28.5571\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  91\n",
            "\n",
            "LOSS => kld_loss: 28.3107 | nll_loss: 0.2375 | loss: 28.5483\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  92\n",
            "\n",
            "LOSS => kld_loss: 28.3018 | nll_loss: 0.2375 | loss: 28.5393\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6504 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  93\n",
            "\n",
            "LOSS => kld_loss: 28.2921 | nll_loss: 0.2376 | loss: 28.5297\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  94\n",
            "\n",
            "LOSS => kld_loss: 28.2838 | nll_loss: 0.2376 | loss: 28.5213\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  95\n",
            "\n",
            "LOSS => kld_loss: 28.2781 | nll_loss: 0.2375 | loss: 28.5157\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  96\n",
            "\n",
            "LOSS => kld_loss: 28.2666 | nll_loss: 0.2376 | loss: 28.5041\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6503 | ap_mean: 0.6919\n",
            "********************************************************\n",
            "epoch:  97\n",
            "\n",
            "LOSS => kld_loss: 28.26 | nll_loss: 0.2375 | loss: 28.4975\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6505 | ap_mean: 0.6922\n",
            "********************************************************\n",
            "epoch:  98\n",
            "\n",
            "LOSS => kld_loss: 28.2548 | nll_loss: 0.2375 | loss: 28.4923\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6504 | ap_mean: 0.692\n",
            "********************************************************\n",
            "epoch:  99\n",
            "\n",
            "LOSS => kld_loss: 28.2499 | nll_loss: 0.2376 | loss: 28.4874\n",
            "\n",
            "ADJ. RECONSTRUCTION => auc_mean: 0.6504 | ap_mean: 0.692\n",
            "Total Execution Time: 0:01:53.038940\n"
          ]
        }
      ]
    }
  ]
}