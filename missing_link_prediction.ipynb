{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "missing_link_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Y0rT8ldQ8y1R9_0-jA59qOs_1gadMwDe",
      "authorship_tag": "ABX9TyNVor3cXCScF57cvdrpURtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spyderweb-abdul/Deletion-Detection-in-Unstructured-Data/blob/main/missing_link_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRgFQaGhJTZ_"
      },
      "source": [
        "import os, sys\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#nb_path = '/content/libraries'\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/VGRNN/')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n",
        "\n",
        "#os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "#sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB5xXwYNKQeS"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import io\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.ndimage import rotate\n",
        "from torch.distributions.uniform import Uniform\n",
        "from torch.distributions.normal import Normal\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "# from torch_geometric import nn as tgnn\n",
        "from input_data import load_data\n",
        "from preprocessing import preprocess_graph, construct_feed_dict, sparse_to_tuple, mask_test_edges\n",
        "import scipy.sparse as sp\n",
        "from scipy.linalg import block_diag\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import tarfile\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import time\n",
        "\n",
        "#!pip uninstall torch-scatter torch-sparse torch-geometric\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.6.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.6.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "import torch_scatter\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_add\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, degree, segregate_self_loops\n",
        "#from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "import scipy.io as sio\n",
        "\n",
        "import inspect\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "#!pip install sparse\n",
        "import sparse\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ryNuuLkKjAv"
      },
      "source": [
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "# utility functions\n",
        "\n",
        "def uniform(size, tensor):\n",
        "    stdv = 1.0 / math.sqrt(size)\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    stdv = math.sqrt(6.0 / (tensor.size(0) + tensor.size(1)))\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)\n",
        "\n",
        "\n",
        "def ones(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(1)\n",
        "\n",
        "\n",
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "def tuple_to_array(lot):\n",
        "    out = np.array(list(lot[0]))\n",
        "    for i in range(1, len(lot)):\n",
        "        out = np.vstack((out, np.array(list(lot[i]))))\n",
        "    \n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-RKqWpKt3Q"
      },
      "source": [
        "# masking functions\n",
        "\n",
        "def mask_edges_det(adjs_list):\n",
        "\n",
        "    adj_train_l, train_edges_l, val_edges_l = [], [], []\n",
        "    val_edges_false_l, test_edges_l, test_edges_false_l = [], [], []\n",
        "    edges_list = []\n",
        "    for i in range(0, len(adjs_list)):\n",
        "        # Function to build test set with 10% positive links\n",
        "        # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "        \n",
        "        adj = adjs_list[i]\n",
        "        # Remove diagonal elements\n",
        "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "        adj.eliminate_zeros()\n",
        "\n",
        "        # Check that diag is zero:\n",
        "        assert np.diag(adj.todense()).sum() == 0\n",
        "        \n",
        "        #get the upper trianglar portion of the matrix.\n",
        "        adj_triu = sp.triu(adj)\n",
        "\n",
        "        #convert the matrix into a tuple of the format: ((([1, 10]), ([1, 1, 1,..., 1, 1, 1])),...)\n",
        "        adj_tuple = sparse_to_tuple(adj_triu)\n",
        "\n",
        "        #get only the 0 index of the tuple. Returns as list: [[1 10],[1 12],[1 4],[20 25]]\n",
        "        #shape: (n, 2)\n",
        "        edges = adj_tuple[0]\n",
        "\n",
        "        #convert the adj sparse matrix to tuple and return the result of the 0 index of the tuple\n",
        "        edges_all = sparse_to_tuple(adj)[0]\n",
        "\n",
        "        #get the number of test set: row number(n)/10\n",
        "        num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "      \n",
        "        #get the number of the validation set: row number(n)/20\n",
        "        num_val = int(np.floor(edges.shape[0] /20.))\n",
        "    \n",
        "        #list numbers of edge index based on the row axis of the edges\n",
        "        #all_edge_idx = range(edges.shape[0])\n",
        "        all_edge_idx = list(range(edges.shape[0]))\n",
        "\n",
        "        #randomize the result\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        #get validation edge index from the randomized edge list. Extract only numbers equal to num_val\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "\n",
        "        #get test edge index from the randomized edge list.\n",
        "        #Extract only numbers equal to [num_val : (num_val + num_test)]\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "\n",
        "        #get the main test edge set by extracting values fom the edge list indexed by the test_edge_idx list\n",
        "        test_edges = edges[test_edge_idx]\n",
        "        \n",
        "        #get the main validation edge set by extracting values fom the edge list indexed by the test_edge_idx list\n",
        "        val_edges = edges[val_edge_idx]\n",
        "\n",
        "        #delete the stacked test and validation edge set (along the axis=0) from the list of edges. \n",
        "        #This will be the training set\n",
        "        # [[162 165], [162 169], [162 172], [171 174]]\n",
        "\n",
        "        train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "        \n",
        "        #append the list of main edges\n",
        "        edges_list.append(edges)\n",
        "        \n",
        "        def ismember(a, b, tol=5):\n",
        "            #Test whether all array elements along a given axis evaluate to True. (np.all)\n",
        "            rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "            return np.any(rows_close)  #np.any evaluate whether any elements evaluate to True\n",
        "\n",
        "        #get false edge test set\n",
        "        test_edges_false = []\n",
        "        #Do while test_egde_false list length is still less than the tst_edge list\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            #get random integers between 0 (lower) and the row size of the adj (higher)\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "\n",
        "            #if right and left values are equal, go back to the top loop\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            #if the tuple of the 2 values are part of edges_all (returns a bool), back to top\n",
        "            if ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            #if the empty test_edges_false list is not None, check the conditions\n",
        "            if test_edges_false:\n",
        "                #if the tuple of the 2 values are part of test_edges_false list, back to top\n",
        "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                    continue\n",
        "            #append result to the test_edges_false list\n",
        "            test_edges_false.append([idx_i, idx_j])  #result sample: [[19, 2], [177, 163], [15, 119], [3, 155],...] \n",
        "\n",
        "        \n",
        "        #get false validation edge set    \n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], val_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], val_edges):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        r\"\"\" The assert keyword lets you test if a condition in your code returns True, \n",
        "        if not, the program will raise an AssertionError.\n",
        "\n",
        "        #we assert the truthfulness of these conditions. \n",
        "        #check to confirm that the values (arg: 1) are bitwise NOT (tilde)\n",
        "        #in the set of values (arg: 2) in the other list.\"\"\"\n",
        "\n",
        "        assert ~ismember(test_edges_false, edges_all)\n",
        "        assert ~ismember(val_edges_false, edges_all)\n",
        "        assert ~ismember(val_edges, train_edges)\n",
        "        assert ~ismember(test_edges, train_edges)\n",
        "        assert ~ismember(val_edges, test_edges)\n",
        "\n",
        "        #get np.ones of elements of the row size of the train_edges\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "\n",
        "        # Re-build adj matrix for the training set\n",
        "        r\"\"\" [ : , 0 ] means (more or less) [ first_row:last_row , column_0 ]. \n",
        "        If you have a 2-dimensional list/matrix/array, this notation will give you all \n",
        "        the values in column 0 (from all rows).\"\"\"\n",
        "\n",
        "        adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "\n",
        "        #add the new adjacency matrix to its transpose\n",
        "        adj_train = adj_train + adj_train.T\n",
        "\n",
        "        #fill all the initialised list\n",
        "        adj_train_l.append(adj_train)\n",
        "        train_edges_l.append(train_edges)\n",
        "        val_edges_l.append(val_edges)\n",
        "        test_edges_l.append(test_edges)\n",
        "        val_edges_false_l.append(val_edges_false)\n",
        "        test_edges_false_l.append(test_edges_false)\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train_l, train_edges_l, val_edges_l, val_edges_false_l, test_edges_l, test_edges_false_l\n",
        "    \n",
        "   \n",
        "def mask_edges_prd(adjs_list):\n",
        "    pos_edges_l , false_edges_l = [], []\n",
        "    edges_list = []\n",
        "    for i in range(0, len(adjs_list)):\n",
        "        # Function to build test set with 10% positive links\n",
        "        # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "        \n",
        "        adj = adjs_list[i]\n",
        "        # Remove diagonal elements\n",
        "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "        adj.eliminate_zeros()\n",
        "        # Check that diag is zero:\n",
        "        assert np.diag(adj.todense()).sum() == 0\n",
        "        \n",
        "        adj_triu = sp.triu(adj)\n",
        "        adj_tuple = sparse_to_tuple(adj_triu)\n",
        "        edges = adj_tuple[0]\n",
        "        edges_all = sparse_to_tuple(adj)[0]\n",
        "        num_false = int(edges.shape[0])\n",
        "        \n",
        "        pos_edges_l.append(edges)\n",
        "        \n",
        "        def ismember(a, b, tol=5):\n",
        "            rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "            return np.any(rows_close)\n",
        "        \n",
        "        edges_false = []\n",
        "        while len(edges_false) < num_false:\n",
        "            idx_i = np.random.randint(0, adj.shape[0])\n",
        "            idx_j = np.random.randint(0, adj.shape[0])\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(edges_false)):\n",
        "                    continue\n",
        "            edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        assert ~ismember(edges_false, edges_all)\n",
        "        \n",
        "        false_edges_l.append(edges_false)\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return pos_edges_l, false_edges_l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxYyTQxJK1m4"
      },
      "source": [
        "# loading data\n",
        "\n",
        "path = 'drive/My Drive/Colab Notebooks/VGRNN/data/'\n",
        "# # Enron dataset\n",
        "#with open(path+'enron_data/enron_adj_sparse_matrix.pickle', 'rb') as handle:\n",
        "     #adj_sparse_matrix = pickle.load(handle)\n",
        "\n",
        "#with open(path+'enron_data/enron_adj_dense_matrix.pickle', 'rb') as handle:\n",
        "     #adj_dense_matrix = pickle.load(handle)\n",
        "\n",
        "#with open(path+'enron_data/enron_edge_attribute_matrix.pickle', 'rb') as handle:\n",
        "     #edge_attr_matrix = pickle.load(handle)\n",
        "\n",
        "#with open(path+'enron_data/enron_node_attribute_matrix.pickle', 'rb') as handle:\n",
        "     #node_attr_matrix = pickle.load(handle)\n",
        "\n",
        "#adj_sparse_matrix = adj_sparse_matrix[7:34]                       #80\n",
        "#adj_dense_matrix = adj_dense_matrix[7:34]            #80\n",
        "#edge_attr_matrix = edge_attr_matrix[7:34]            #80\n",
        "#node_attr_matrix = node_attr_matrix[7:34]  \n",
        "\n",
        "with open(path+'enron_data_random/enron_adj_sparse_80.pickle', 'rb') as handle:\n",
        "     adj_sparse_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data_random/enron_adj_dense_80.pickle', 'rb') as handle:\n",
        "     adj_dense_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data_random/enron_edge_attribute_80.pickle', 'rb') as handle:\n",
        "     edge_attr_matrix = pickle.load(handle)\n",
        "\n",
        "with open(path+'enron_data_random/enron_node_attribute_80.pickle', 'rb') as handle:\n",
        "     node_attr_matrix = pickle.load(handle)\n",
        "\n",
        "#print(adj_sparse_matrix)\n",
        "outs = mask_edges_det(adj_sparse_matrix)\n",
        "\n",
        "#reconstructed adjacency matrix of the training set\n",
        "adj_train_l = outs[0]                 #80\n",
        "\n",
        "#List of training edge set\n",
        "train_edges_l = outs[1]               #80\n",
        "\n",
        "#List of validation edge set\n",
        "val_edges_l = outs[2]                 #80\n",
        "\n",
        "#List of false validation edge set(i.e., never exist)\n",
        "val_edges_false_l = outs[3]           #80\n",
        "\n",
        "#List of test edge set\n",
        "test_edges_l = outs[4]                #80\n",
        "\n",
        "#List of false test edge set \n",
        "test_edges_false_l = outs[5]          #80\n",
        "\n",
        "\n",
        "pos_edges_l, false_edges_l = mask_edges_prd(adj_sparse_matrix)\n",
        "#pos_samples, neg_samples = mask_edges_prd_new(adj_sparse_matrix, adj_dense_matrix)\n",
        "\n",
        "\n",
        "# creating edge list\n",
        "edge_idx_list = []                    #80\n",
        "\n",
        "for i in range(len(train_edges_l)):\n",
        "    edge_idx_list.append(torch.tensor(np.transpose(train_edges_l[i]), dtype=torch.long))\n",
        "\n",
        "#print('Training edges: ', edge_idx_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8EhQwXnK-iv"
      },
      "source": [
        "# layers\n",
        "\n",
        "class E_GCN_Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act=F.relu, improved=True, bias=True, num_channels=10, aggr='sum'):\n",
        "        super(E_GCN_Conv, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels            #[64]\n",
        "        self.out_channels = out_channels          #[32]\n",
        "        self.act = act\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels, num_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels, num_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if (aggr == 'concat'):\n",
        "            self.aggr = 'concat'\n",
        "            self.last_ops = nn.Linear(self.out_channels * self.num_channels, self.out_channels)\n",
        "        elif (aggr == 'sum'):\n",
        "            self.aggr = 'sum'\n",
        "            self.last_ops = nn.Linear(self.out_channels, self.out_channels)\n",
        "        \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "\n",
        "        #print(edge_index.size())\n",
        "        #print(edge_attr.size())\n",
        "\n",
        "        #add or remove node self loop. We remove in our case\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "                \n",
        "        #edge index rows and column representation\n",
        "        row, col = edge_index     #[21]\n",
        "\n",
        "        #normalize the adjacency matrix\n",
        "        #deg = scatter_add(edge_attr, row, dim=0, dim_size=x.size(0))\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        #reshape the row and column vectors\n",
        "        deg_inv_sqrt_row = deg_inv_sqrt[row].view(-1, 1)      #[[1.0000],[1.0000]]\n",
        "        deg_inv_sqrt_col = deg_inv_sqrt[col].view(-1, 1)      #[[0.5774],[0.0000]]\n",
        "\n",
        "        #multiply row and col vectors with edge weights (We replace the adjacencodery matrix with the edge tensor)\n",
        "        norm_edge = deg_inv_sqrt_row * edge_attr * deg_inv_sqrt_col     #size([edge_index[row/col] No., 14])\n",
        "\n",
        "\n",
        "        #Slice and list the normalized vectors based on the nu. of channels\n",
        "        norm = []\n",
        "        for i in range(0, edge_attr.size()[1]):\n",
        "            norm.append(norm_edge[:, i:i+1])\n",
        "\n",
        "        node_state_list = []\n",
        "        #for each edge channels, we perform a weoghted convolution with edge weights as co-efficient\n",
        "        for c in range(self.num_channels):\n",
        "            if self.in_channels > self.out_channels:\n",
        "\n",
        "                #if the weight matrix is not none\n",
        "                if self.weight is not None:\n",
        "                    #matrix product of the node (hidden state) with the weight matrix\n",
        "                    weighted_nodes = torch.matmul(x, self.weight[:, :, c])      #(size[149, 32])\n",
        "                else:\n",
        "                    #otherwise, hidden state remains same\n",
        "                    weighted_nodes = x\n",
        "                \n",
        "                #if vectors are normalized\n",
        "                if norm is not None: \n",
        "                    #multiply each element in the each channels of the norm with weighted hidden state             \n",
        "                    weighted_conv = torch.mul(norm[c], weighted_nodes[row])      #size(21, 32)\n",
        "\n",
        "                    #propagate messages through all edges and update the nodes\n",
        "                    weighted_conv_sum = scatter_add(weighted_conv, col, dim=0, dim_size=x.size(0)) #size(149, 32)\n",
        "                else:\n",
        "                    weighted_conv_sum = scatter_add(weighted_nodes[row], col, dim=0, dim_size=x.size(0))\n",
        "\n",
        "                channel_node_state = weighted_conv_sum\n",
        "\n",
        "            else:\n",
        "                if norm is not None:\n",
        "                    unweighted_conv = torch.mul(norm[c], x[row])\n",
        "                    unweighted_conv_sum = scatter_add(unweighted_conv, col, dim=0, dim_size=x.size(0))\n",
        "                else:\n",
        "                    unweighted_conv_sum = scatter_add(x[row], col, dim=0, dim_size=x.size(0))\n",
        "                \n",
        "                if self.weight is not None:\n",
        "                    channel_node_state = torch.matmul(unweighted_conv_sum.float(), self.weight[:, :, c])\n",
        "            \n",
        "            #add linear bias if True\n",
        "            if self.bias is not None:\n",
        "                channel_node_state = channel_node_state + self.bias[:, c]\n",
        "            \n",
        "            #pass param through a linear activation function\n",
        "            channel_node_state = self.act(channel_node_state)\n",
        "            #append each channel to node state list\n",
        "            node_state_list.append(channel_node_state)        #size(N, 32/16)\n",
        "\n",
        "        #we consider two aggregation method across each channels of the edge weights\n",
        "        #1. Sum aggregation method \n",
        "        if (self.aggr == 'sum'):\n",
        "            node_states = torch.stack(node_state_list, dim=1).sum(1).float()     #[N, 32]\n",
        "        #2. Concat aggregation method               \n",
        "        elif (self.aggr == 'concat'): \n",
        "            node_states = torch.cat(node_state_list, dim=1).float()\n",
        "            \n",
        "        #pass aggregated vectors through a flexible linear transformation layer      \n",
        "        out = self.last_ops(node_states)                        #size(N, 32/16)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, \n",
        "                                   self.out_channels, self.num_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwpqrN33LFIJ"
      },
      "source": [
        "class gru_gcn(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layer, bias=True):\n",
        "        super(gru_gcn, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layer = n_layer\n",
        "        \n",
        "        # gru weights\n",
        "        self.weight_xz = []\n",
        "        self.weight_hz = []\n",
        "        self.weight_xr = []\n",
        "        self.weight_hr = []\n",
        "        self.weight_xh = []\n",
        "        self.weight_hh = []\n",
        "        \n",
        "        for i in range(self.n_layer):\n",
        "            if i==0:\n",
        "                self.weight_xz.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xr.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xh.append(E_GCN_Conv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "            else:\n",
        "                self.weight_xz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hz.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hr.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_xh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "                self.weight_hh.append(E_GCN_Conv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
        "    \n",
        "    def forward(self, inp, edge_index, edge_tensor, h):\n",
        "        h_out = torch.zeros(h.size())\n",
        "        for i in range(self.n_layer):\n",
        "            if i==0:\n",
        "                \n",
        "                z_g = torch.sigmoid(self.weight_xz[i](inp, edge_index, edge_tensor) + self.weight_hz[i](h[i], edge_index, edge_tensor))\n",
        "                r_g = torch.sigmoid(self.weight_xr[i](inp, edge_index, edge_tensor) + self.weight_hr[i](h[i], edge_index, edge_tensor))\n",
        "                h_tilde_g = torch.tanh(self.weight_xh[i](inp, edge_index, edge_tensor) + self.weight_hh[i](r_g * h[i], edge_index, edge_tensor))\n",
        "                h_out[i] = z_g * h[i][0: inp.size(0)] + (1 - z_g) * h_tilde_g\n",
        "        #         out = self.decoder(h_t.view(1,-1))\n",
        "            else:\n",
        "                z_g = torch.sigmoid(self.weight_xz[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hz[i](h[i], edge_index, edge_tensor))\n",
        "                r_g = torch.sigmoid(self.weight_xr[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hr[i](h[i], edge_index, edge_tensor))\n",
        "                h_tilde_g = torch.tanh(self.weight_xh[i](h_out[i-1], edge_index, edge_tensor) + self.weight_hh[i](r_g * h[i], edge_index, edge_tensor))\n",
        "                h_out[i] = z_g * h[i] + (1 - z_g) * h_tilde_g\n",
        "        #         out = self.decoder(h_t.view(1,-1))\n",
        "        \n",
        "        out = h_out\n",
        "        return out, h_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNec0ycDLPfC"
      },
      "source": [
        "# VGRNN model\n",
        "\n",
        "class VGAE_Edge(nn.Module):\n",
        "    def __init__(self, node_feat_dim, hidden_dim, latent_var_dim, n_layers, edge_feat_dim, eps, conv='GCN', bias=False):\n",
        "        super(VGAE_Edge, self).__init__()\n",
        "        \n",
        "        #input dimension\n",
        "        self.node_feat_dim = node_feat_dim        \n",
        "        self.eps = eps\n",
        "        #hidden_layer dim.\n",
        "        self.hidden_dim = hidden_dim        #32\n",
        "        #latent variable dim.\n",
        "        self.latent_var_dim = latent_var_dim        #10\n",
        "        self.n_layers = n_layers   #1\n",
        "        self.edge_feat_dim = edge_feat_dim\n",
        "       \n",
        "        if conv == 'GCN':\n",
        "            #flexible sequential neural network linear transformations\n",
        "            self.input_emb = nn.Sequential(nn.Linear(node_feat_dim, hidden_dim), nn.ReLU())\n",
        "            self.output_emb = nn.Sequential(nn.Linear(latent_var_dim, hidden_dim), nn.ReLU())\n",
        "            self.edge_feat_emb = nn.Sequential(nn.Linear(edge_feat_dim, hidden_dim), nn.ReLU())\n",
        "            \n",
        "            #encoder functions\n",
        "            self.encoder = E_GCN_Conv(hidden_dim + hidden_dim, hidden_dim)            \n",
        "            self.encoder_mu = E_GCN_Conv(hidden_dim, latent_var_dim, act=lambda x:x)\n",
        "            self.encoder_sigma = E_GCN_Conv(hidden_dim, latent_var_dim, act=F.softplus)\n",
        "            \n",
        "            #linear linear transformation of the prior functions\n",
        "            self.prior = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
        "            self.prior_mu = nn.Sequential(nn.Linear(hidden_dim, latent_var_dim))\n",
        "            self.prior_sigma = nn.Sequential(nn.Linear(hidden_dim, latent_var_dim), nn.Softplus())\n",
        "            \n",
        "            #recurrent neural networks model function\n",
        "            self.rnn = gru_gcn(hidden_dim + hidden_dim, hidden_dim, n_layers, bias)\n",
        "          \n",
        "          \n",
        "\n",
        "    def forward(self, x, edge_idx_list, edge_attr_matrix, adj_dense_matrix, hidden_in=None):\n",
        "\n",
        "        #assert the length of edge matrix = elngth of the edge indices\n",
        "        assert len(adj_dense_matrix) == len(edge_idx_list)\n",
        "\n",
        "        #initialize params\n",
        "        kld_loss = 0\n",
        "        nll_loss = 0\n",
        "        encoder_mu_list, encoder_sigma_list = [], []\n",
        "        prior_mu_list, prior_sigma_list = [], []\n",
        "        decoded_list, z_list = [], []\n",
        "        \n",
        "        #hidden var will be none in the first set of operations\n",
        "        if hidden_in is None:\n",
        "            #so we create a matrix of zeros as initial representation\n",
        "            h = torch.zeros(self.n_layers, x.size(1), self.hidden_dim)  #size([1, 149, 32])\n",
        "        else:\n",
        "            #hidden var here will be the recurrent vectors\n",
        "            h = hidden_in\n",
        "\n",
        "        for t in range(x.size(0)):       #x.size(0) = 60\n",
        "\n",
        "            #linearly transform x features\n",
        "            input_emb_t = self.input_emb(x[t].float())              #[149, 32]\n",
        "            #edge indices at time t    \n",
        "            edge_idx_list_t = edge_idx_list[t]\n",
        "\n",
        "            #edge tensor matrix at time t => extract on the tensors associated with the edge indices at time t\n",
        "            #Note: there are 14 vectors in each edge attributes. We can reduce to 10 if we choose\n",
        "            #to extract only topics of communication. The model eval works differently for 14 and 10\n",
        "            edge_tensor_t = (edge_attr_matrix[t][edge_idx_list_t[0], edge_idx_list_t[1]])#[:, 0:latent_var_dim]\n",
        "            adj_dense_matrix_t = adj_dense_matrix[t]\n",
        "            edge_inp_emb_t = self.edge_feat_emb(edge_tensor_t.float())\n",
        "            \n",
        "            #encoder\n",
        "            #encoders conditioned on priors so features of previous states can be \n",
        "            #recurrently modeled\n",
        "            #encoder_t = self.encoder(torch.cat([input_emb_t, h[-1]], 1), edge_idx_list_t, edge_tensor_t)    #[149, 32]\n",
        "            encoder_t = self.encoder(torch.cat([input_emb_t, h[-1]], 1), edge_idx_list_t, edge_inp_emb_t)    #[149, 32]\n",
        "            #encoder mean\n",
        "            #encoder_mu_t = self.encoder_mu(encoder_t, edge_idx_list_t, edge_tensor_t)                   #[149, 16]\n",
        "            encoder_mu_t = self.encoder_mu(encoder_t, edge_idx_list_t, edge_inp_emb_t)                   #[149, 16]\n",
        "            #encoder standard deviation\n",
        "            #encoder_sigma_t = self.encoder_sigma(encoder_t, edge_idx_list_t, edge_tensor_t)                     #[149, 16]\n",
        "            encoder_sigma_t = self.encoder_sigma(encoder_t, edge_idx_list_t, edge_inp_emb_t)                     #[149, 16]     \n",
        "            \n",
        "            #prior\n",
        "            prior_t = self.prior(h[-1])                           #[149, 32]\n",
        "            prior_mu_t = self.prior_mu(prior_t)               #[149, 10]\n",
        "            prior_sigma_t = self.prior_sigma(prior_t)                 #[149, 10]\n",
        "            \n",
        "            #sampling and reparameterization\n",
        "            z_t = self._reparameterized_sample(encoder_mu_t, encoder_sigma_t)  #[149, 10]\n",
        "            #apply a fully connected layer to z_t\n",
        "            output_emb_t = self.output_emb(z_t)                                  #[149, 32]\n",
        "\n",
        "            #decoder function -> takes the linearly transformed latent variable and egde indices as args\n",
        "            decoder_t = self.dec(output_emb_t, edge_idx_list_t)\n",
        "\n",
        "            #recurrencodere\n",
        "            #_, h = self.rnn(torch.cat([input_emb_t, output_emb_t], 1), edge_idx_list_t, edge_tensor_t, h)       #[1, 149, 32]\n",
        "            _, h = self.rnn(torch.cat([input_emb_t, output_emb_t], 1), edge_idx_list_t, edge_inp_emb_t, h)       #[1, 149, 32]\n",
        "            #print('h: ', h.size())\n",
        "            \n",
        "            num_nodes = adj_dense_matrix_t.size()[0]\n",
        "            encoder_mu_t_slice = encoder_mu_t[0:num_nodes, :]\n",
        "            encoder_sigma_t_slice = encoder_sigma_t[0:num_nodes, :]\n",
        "            prior_mu_t_slice = prior_mu_t[0:num_nodes, :]\n",
        "            prior_sigma_t_slice = prior_sigma_t[0:num_nodes, :]\n",
        "            adj_decoder_t = decoder_t[0:num_nodes, 0:num_nodes]       #Size[149, 149]\n",
        "\n",
        "            #computing losses\n",
        "            kld_loss += self.kl_divergence(encoder_mu_t_slice, encoder_sigma_t_slice, prior_mu_t_slice, prior_sigma_t_slice)\n",
        "            #kld_loss += self.kl_divergence_zu(encoder_mean_t, encoder_std_t)\n",
        "            nll_loss += self.nll_bernoulli(adj_decoder_t, adj_dense_matrix_t)\n",
        "\n",
        "            \n",
        "            encoder_sigma_list.append(encoder_sigma_t_slice)\n",
        "            encoder_mu_list.append(encoder_mu_t_slice)\n",
        "            prior_mu_list.append(prior_mu_t_slice)\n",
        "            prior_sigma_list.append(prior_sigma_t_slice)\n",
        "            z_list.append(z_t)\n",
        "            decoded_list.append(adj_decoder_t)\n",
        "\n",
        "        return kld_loss, nll_loss, encoder_mu_list, prior_mu_list, decoded_list, h\n",
        "    \n",
        "    #decoder function\n",
        "    def dec(self, z, edge_index):\n",
        "        #output = neural network decoder\n",
        "        outputs = Decoder(act=lambda x:x)(z, edge_index)\n",
        "        return outputs\n",
        "    \n",
        "    def reset_parameters(self, stdv=1e-1):\n",
        "        for weight in self.parameters():\n",
        "            weight.data.normal_(0, stdv)\n",
        "     \n",
        "    def _init_weights(self, stdv):\n",
        "        pass\n",
        "    \n",
        "    def _reparameterized_sample(self, mu, sigma):\n",
        "        eps1 = torch.FloatTensor(sigma.size()).normal_()\n",
        "        eps1 = Variable(eps1)\n",
        "        return eps1.mul(sigma).add_(mu)\n",
        "    \n",
        "    def kl_divergence(self, encoder_mu, encoder_sigma, prior_mu, prior_sigma):\n",
        "        mu_size = encoder_mu.size(0)\n",
        "        encoder_sigma_log = torch.log(encoder_sigma + self.eps)\n",
        "        prior_sigma_log = torch.log(prior_sigma + self.eps)\n",
        "        encoder_sigma = encoder_sigma + self.eps\n",
        "        prior_sigma = prior_sigma + self.eps\n",
        "\n",
        "        kld_element = (2 * prior_sigma_log - 2 * encoder_sigma_log + (torch.pow(encoder_sigma, 2) + torch.pow(encoder_mu - prior_mu, 2)) / \n",
        "                      torch.pow(prior_sigma, 2) - 1)\n",
        "        kld_element = kld_element.detach().numpy()      \n",
        "        kld_element = torch.tensor(np.nan_to_num(kld_element, copy=True, nan=0.0))\n",
        "        kld = (0.5 / mu_size) * kld_element.sum(1).mean()\n",
        "        return kld\n",
        "    \n",
        "    def kl_divergence_zu(self, mu, sigma):        \n",
        "        mu_size = mu.size(0)\n",
        "        sigma_log = torch.log(sigma + self.eps)\n",
        "        kld_element =  (1 + 2*sigma_log - (sigma**2) - (mu**2))\n",
        "        kld_element = kld_element.detach().numpy()      \n",
        "        kld_element = torch.tensor(np.nan_to_num(kld_element, copy=True, nan=0.0))        \n",
        "        return (-0.5 / mu_size) * kld_element.sum(1).mean()\n",
        "    \n",
        "    #negative log likelihood bernoulli\n",
        "    def nll_bernoulli(self, logits, target):                     \n",
        "        positive_weight = float(target.size(0) * target.size(0) - target.sum()) / target.sum()   #negative samples/positive samples\n",
        "        #norm_weight = target.size(0) * target.size(0) / float((target.size(0) * target.size(0) - target.sum())*2) \n",
        "        bce = F.binary_cross_entropy_with_logits(logits, target, pos_weight=positive_weight, reduction='mean')\n",
        "        \n",
        "        nll_loss = (-1.0 / target.size(0)) * bce\n",
        "        return - nll_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yDfLMrsLUk5"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, act=torch.sigmoid):\n",
        "        super(Decoder, self).__init__()        \n",
        "\n",
        "        self.act = act\n",
        "    def forward(self, z, edge_index):\n",
        "        z = F.dropout(z, p=0., training=True) \n",
        "\n",
        "        a_hat = torch.transpose(z, dim0=0, dim1=1)\n",
        "        a_hat = self.act(torch.mm(z, a_hat))\n",
        "\n",
        "        a_hat_z0 = z[edge_index[0]]\n",
        "        a_hat_z1 = z[edge_index[1]]\n",
        "\n",
        "        link_prob = (a_hat_z0 * a_hat_z1).sum(dim=1)\n",
        "        #print(link_prob)\n",
        "        #if link_prob > 0.5:\n",
        "            #link_prob = 1\n",
        "        #else:\n",
        "            #link_prob = 0\n",
        "        adj_prob = self.act(link_prob) if self.act else link_prob\n",
        "        \n",
        "        return a_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sko45PpLa6x"
      },
      "source": [
        "r\"\"\" Calculate and evaluate the Area Under (Receiver Operating Characteristic) Curve \n",
        "     and the Average Precision (AP) \"\"\"\n",
        "\n",
        "# evaluation function\n",
        "def get_eval_scores(edges_pos, edges_neg, adj_dense_matrix, a_embs):\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    auc, ap, cm = [], [], []\n",
        "    \n",
        "    for i in range(len(edges_pos)):\n",
        "        \n",
        "        # Predict on test set of edges\n",
        "        #explicitly remove the computational graph of the tensor \n",
        "        #(from gradient descent) with detach and change back to numpy\n",
        "        a_emb = a_embs[i].detach().numpy()\n",
        "\n",
        "        #reconstruct the adjacency matrix of the embeddings\n",
        "        adj_emb = np.dot(a_emb, a_emb.T)          #[149, 149]\n",
        "\n",
        "        adj_dense = adj_dense_matrix[i]\n",
        "       \n",
        "        #initialize predicted edge list\n",
        "        pos_adj, pred_pos_adj = [], []\n",
        "\n",
        "        for e in edges_pos[i]:\n",
        "\n",
        "            pred_pos_adj.append(sigmoid(adj_emb[e[0], e[1]]))\n",
        "            pos_adj.append(adj_dense[e[0], e[1]])         \n",
        "\n",
        "        #print('Pos: ', pos_adj)\n",
        "        #print('\\nPred: ', pred_pos_adj)\n",
        "\n",
        "        neg_adj, pred_neg_adj = [], []\n",
        "\n",
        "        for e in edges_neg[i]:\n",
        "\n",
        "            pred_neg_adj.append(sigmoid(adj_emb[e[0], e[1]]))\n",
        "            neg_adj.append(adj_dense[e[0], e[1]])\n",
        "\n",
        "        #stack up the positive and negative predicted features\n",
        "        all_pred_adj = np.hstack([pred_pos_adj, pred_neg_adj])       \n",
        "        all_true_adj = np.hstack([np.ones(len(pred_pos_adj)), np.zeros(len(pred_neg_adj))])\n",
        "        \n",
        "        auc.append(roc_auc_score(all_true_adj, all_pred_adj))\n",
        "        ap.append(average_precision_score(all_true_adj, all_pred_adj))\n",
        "        cm.append(confusion_matrix(all_true_adj, all_pred_adj))\n",
        "\n",
        "    return auc, ap, cm, neg_adj, pred_neg_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42a_R7tBLiYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8fcba76-5dfe-4044-8d6d-ccedb5995465"
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "hidden_dim = 32\n",
        "latent_var_dim = 16\n",
        "n_layers =  1\n",
        "clip = 10\n",
        "learning_rate = 1e-2\n",
        "timesteps_len = len(train_edges_l)  \n",
        "num_nodes = node_attr_matrix[0].shape[1]\n",
        "node_feat_dim = num_nodes\n",
        "edge_feat_dim = 10\n",
        "eps = 1e-10\n",
        "conv_type='GCN'\n",
        "\n",
        "# creating input tensors\n",
        "node_attr = torch.stack(node_attr_matrix)   #[80, 149, 6]\n",
        "\n",
        "adj_label_list = []\n",
        "for i in range(len(adj_train_l)):\n",
        "    temp_matrix = adj_train_l[i]\n",
        "    adj_label_list.append(torch.tensor(temp_matrix.toarray().astype(np.float32))) \n",
        "\n",
        "\n",
        "# building model\n",
        "model = VGAE_Edge(node_feat_dim, hidden_dim, latent_var_dim, n_layers, edge_feat_dim, eps, conv=conv_type, bias=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# training\n",
        "timesteps_init = 0\n",
        "timesteps_end = timesteps_len - 1\n",
        "test_init = 0\n",
        "\n",
        "\n",
        "#writer = SummaryWriter('drive/MyDrive/Colab Notebooks/VGRNN/tensorboard_log/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "start_time = time.monotonic()\n",
        "for k in range(10):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    kld_loss, nll_loss, _, _, _, hidden_st = model(node_attr[timesteps_init:timesteps_end]\n",
        "                                                , edge_idx_list[timesteps_init:timesteps_end]\n",
        "                                                , edge_attr_matrix[timesteps_init:timesteps_end]\n",
        "                                                , adj_dense_matrix[timesteps_init:timesteps_end]\n",
        "                                                )\n",
        "    \n",
        "    loss = kld_loss + nll_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    if k > test_init:\n",
        "        _, _, encs_, priors_, adj_dec, _ = model(node_attr[timesteps_end:timesteps_len]\n",
        "                                          , edge_idx_list[timesteps_end:timesteps_len]\n",
        "                                          , edge_attr_matrix[timesteps_end:timesteps_len]\n",
        "                                          , adj_label_list[timesteps_end:timesteps_len]\n",
        "                                          , hidden_st)\n",
        "        \n",
        "        \n",
        "        auc, ap, cm, pos, pred = get_eval_scores(pos_edges_l[timesteps_end:timesteps_len]\n",
        "                                            , false_edges_l[timesteps_end:timesteps_len]\n",
        "                                            , adj_dense_matrix[timesteps_end:timesteps_len]\n",
        "                                            , priors_                                                        \n",
        "                                            )\n",
        "\n",
        "\n",
        "    #Note: Prior mean reduces the loss than the decoded variables. \n",
        "    print('********************************************************')\n",
        "    print('epoch: ', k)\n",
        "    print('\\nLOSS => kld_loss: {} | nll_loss: {} | loss: {}'.format( \n",
        "                                                                      round(kld_loss.mean().item(), 4)\n",
        "                                                                    , round(nll_loss.mean().item(), 4)\n",
        "                                                                    , round(loss.mean().item(), 4)\n",
        "                                                                    ))\n",
        "    #writer.add_scalar(\"Loss/train\", loss.mean().item(), k)\n",
        "    if k > test_init:\n",
        "        #writer.add_scalar(\"validation_auc\", np.mean(np.array(auc_val)), k)\n",
        "        #writer.add_scalar(\"validation_ap\", np.mean(np.array(ap_val)), k)\n",
        "        #writer.add_scalar(\"test_auc\", np.mean(np.array(auc_test)), k)\n",
        "        #writer.add_scalar(\"test_ap\", np.mean(np.array(ap_test)), k)\n",
        "\n",
        "        print('\\nADJ. RECONSTRUCTION => auc_mean: {} | ap_mean: {}'.format(round(np.mean(np.array(auc)), 4)\n",
        "                                                                          , round(np.mean(np.array(ap)), 4)\n",
        "                                                                          ))\n",
        "        tn, fp, fn, tp = confusion_matrix(cm[:]).ravel()\n",
        "        print(tn, fp, fn, tp)\n",
        "#writer.flush() \n",
        "#writer.close()   \n",
        "end_time = time.monotonic()\n",
        "print('Total Execution Time: {}'.format(timedelta(seconds=end_time - start_time)))\n",
        "\n",
        "#!pip install tensorboard\n",
        "#!tensorboard --logdir='drive/MyDrive/Colab Notebooks/VGRNN/tensorboard_log/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.0027, 36.2609, 42.4557, 19.0485,  3.8072, 16.9858, 15.5519, 22.2142,\n",
            "        31.5789, 29.7550, 14.9002, 36.3025, 15.1256, 37.0751, 38.1362, 22.0434,\n",
            "         6.9859, 52.7062, 25.0443, 40.7669, 13.9568, 12.0655,  3.7122, 13.2313,\n",
            "        22.3002, 14.0465, 20.0497, 19.2521,  3.8314, 89.0035],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([11.4961,  6.3161,  3.3120, 29.1245, 14.1560, 38.0626, 27.9804,  5.7012,\n",
            "        28.9111, 24.3043, 37.2953, 47.9587, 19.8292, 15.3455, 34.9501, 35.9382,\n",
            "         8.7268, 13.1163, 14.6001,  8.2191, 20.4916, 38.1783, 16.1105,  6.2751,\n",
            "        32.6489, 15.2449,  0.9654, 12.7373,  4.2299, 12.9910,  9.3444, 88.2227,\n",
            "        91.5743, 23.8572,  6.0919, 24.4736, 11.5601, 11.6067, 17.8473, 18.2881],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([12.7079, 38.9590, 21.3786, 63.4286, 18.2682,  5.8209, 24.5617,  5.3238,\n",
            "        15.1518, 20.8608, 29.5302, 36.6152, 24.0085, 45.9432, 35.3370, 12.1404,\n",
            "        22.8312,  0.9861,  6.9817,  2.5879, 26.8656, 14.1384,  3.6093, 11.5508,\n",
            "         8.5690,  8.7980, 15.1922, 23.1850,  7.2748, 18.4196, 15.7775, 14.5469,\n",
            "         9.2727, 42.4764, 60.4928,  9.4019, 34.9283, 38.7903,  4.7852,  3.6652,\n",
            "        16.6421, 44.8661], grad_fn=<SumBackward1>)\n",
            "tensor([69.3010, 43.1442,  9.8650,  5.3922, 14.6347,  8.1649, 15.2237,  3.1709,\n",
            "        33.3107, 34.0599, 16.1435, 60.5968, 67.8859,  4.5449,  4.8496, 50.0056,\n",
            "        16.8783, 32.3256, 20.9104, 18.0582, 22.5044, 37.3773, 37.9714, 20.5670,\n",
            "        32.6213, 14.3389, 17.9333, 64.1725, 28.6266, 18.9265, 27.0506, 31.7197,\n",
            "        34.9315, 20.8042,  3.8109, 13.3890,  9.8513, 16.9282, 11.5875, 24.6021,\n",
            "        60.8554, 32.3491, 18.9119, 18.5982,  9.7014, 41.1813, 50.9567, 20.6592],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 2.9364, 14.2058, 34.5909, 48.8509, 33.9014,  1.6916,  7.2804,  7.9457,\n",
            "        28.9759,  7.6529,  6.4490,  3.1040,  5.7413,  7.8317, 13.2146, 33.3381,\n",
            "        62.1378, 40.0025, 14.5076,  8.8964, 30.9992, 10.5012, 28.8862, 67.8049,\n",
            "        26.8572, 26.6555,  8.5664, 15.0330, 33.4556,  6.4732, 13.9289,  6.4860,\n",
            "        46.6980, 15.2182, 28.8898, 23.1918, 15.7524, 18.4166, 10.5084, 25.0500,\n",
            "        18.6578, 25.2974, 18.0439, 43.9878, 13.2798, 24.4774, 38.9187],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([19.8189, 40.4024,  6.7647, 22.7250, 27.6902,  4.6659, 14.4131, 10.0614,\n",
            "        44.8703, 54.2182, 17.8081, 51.4821, 36.2566, 16.6364,  3.0494,  8.2497,\n",
            "        11.2078,  5.2711, 28.1861, 44.5411, 10.8905, 29.0768,  8.7877, 12.6656,\n",
            "         3.8462, 14.5900, 15.6956, 36.4898, 42.8266, 61.2183, 11.5082, 18.4536,\n",
            "        17.7632, 29.0408, 38.1090, 18.5973, 43.1585, 14.8488,  3.8858,  8.3174,\n",
            "        19.3573, 17.3697, 34.2400, 24.2499,  4.6819, 29.1680, 12.0628,  1.9248,\n",
            "        55.5065,  4.5479,  4.6472, 11.9557,  0.9706, 42.6705, 13.1084],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([29.0781, 18.4653, 29.9190,  4.3212,  8.4243, 42.2322, 22.7488, 40.2967,\n",
            "        49.2914, 29.8665, 64.4913, 47.0104, 21.8498, 22.3072,  5.8981, 20.7425,\n",
            "        14.2748,  9.3731, 44.1902, 31.7011, 52.1754,  2.1079,  9.5967, 20.4600,\n",
            "        28.8611, 21.7631,  9.6797, 30.8994, 22.9705, 26.7943, 29.3401, 74.3179,\n",
            "         5.0464, 36.6889, 60.5200, 21.4001,  7.0032, 13.5537, 47.6850, 43.5611,\n",
            "        19.8982,  6.2026,  3.7138, 28.2301,  6.7304, 55.0229,  9.4840, 14.9532,\n",
            "        43.4834, 29.6666, 44.0046, 14.2586, 55.6414,  2.2150, 25.0532, 46.7623,\n",
            "         0.1465, 34.2715, 17.1565, 14.8788, 57.0943, 52.5003,  6.0967, 33.3563,\n",
            "        15.6305,  8.0457, 86.3441, 59.0038, 42.6657, 26.8577],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 23.8770,  23.0685,  26.0385,  10.6878,  35.8689,   2.6223,  12.1991,\n",
            "         27.5798,   2.2686,  64.5348,  85.1532,  16.3492,   6.6938,   4.8870,\n",
            "          2.7769,  16.5689,   1.0428,   5.2338,  13.7126,  10.7299,  26.4369,\n",
            "         33.3287,   1.9692,   8.4775,  55.7745,  14.0986,  27.6730,  93.4810,\n",
            "         16.5633,  73.6547,  29.6819,  42.0450,  47.5515,  18.7811, 105.6514,\n",
            "         46.1007,  66.6580,  10.6506,  19.8314,  33.2388,  43.4252,  27.3069,\n",
            "         16.0503,  96.4317,  56.4674,  53.6929,  31.8968,   5.8676,  30.5108,\n",
            "         20.4211,  22.1122,  70.3509,  15.0878,  18.9242,  13.7700,   5.5582,\n",
            "         48.2506,   3.5430,  17.1014,  34.2145,  10.1070,  29.7051,  84.3179,\n",
            "         41.6875, 104.8626,  23.3544,  80.6309,  32.7155, 225.6479,   3.2959,\n",
            "          9.3250,   9.0036,  50.2508,  22.3992,  85.8214,  34.7362,  14.9392,\n",
            "         47.5598,  27.8199,  51.9922,  27.4415,  72.0778,   9.3813,  67.1047,\n",
            "         14.3242,  59.1651,  14.7183,   6.0213], grad_fn=<SumBackward1>)\n",
            "tensor([64.6183, 22.6747, 31.3964, 43.1052, 16.6814,  7.6432, 34.3821,  1.1808,\n",
            "        71.1356, 29.0019, 19.5191,  3.8489,  9.9630, 18.1120, 23.6669,  5.8540,\n",
            "        40.0006, 35.3511, 11.2824, 31.2762, 57.8778, 20.2748, 68.1976,  9.6408,\n",
            "        17.2529,  9.3790,  8.1793, 13.5326, 32.3739, 84.3985, 29.6022, 63.1843,\n",
            "        34.3468,  9.6576, 20.5152,  4.3997,  9.9244, 15.8114, 19.0844, 40.1839,\n",
            "        21.4833, 24.5343, 18.6849, 39.3568, 20.7077,  3.9356, 41.8622, 23.4121,\n",
            "        25.5816,  2.5314, 25.4429, 15.6694, 15.3577, 36.1423, 36.1313, 41.5762,\n",
            "        35.3692, 12.0957, 53.2113,  7.1840,  7.9241, 19.6136, 11.6966, 46.0191,\n",
            "        30.0518, 40.8071, 25.6449, 16.9198, 27.6240,  9.6717, 68.6777,  5.5337,\n",
            "        37.6299, 35.2044, 14.3545, 22.9974, 98.5458, 15.9823, 42.3294, 27.1008,\n",
            "         2.7433, 23.0240, 12.7969, 34.3816, 24.9376, 14.4472, 37.1671, 45.5372,\n",
            "         7.3956, 57.8825,  8.5732, 13.1174,  7.3649, 15.8810, 16.5900, 15.3255,\n",
            "         2.2916, 10.2658,  4.8697, 28.6897, 30.1140,  7.2919, 27.8640, 15.3444,\n",
            "        48.9169, 35.1980, 42.3088, 22.5599, 56.9515, 40.0351, 18.4872, 18.4117,\n",
            "         4.6989, 45.8024,  6.3437, 39.2907,  8.0964,  6.5641, 27.8276, 15.1710,\n",
            "         7.4583, 17.7853, 30.7841,  7.1891, 31.1642, 12.6220, 18.6716, 39.7458,\n",
            "         5.3866, 13.0896, 14.0499, 39.2618, 80.8939], grad_fn=<SumBackward1>)\n",
            "tensor([ 17.7118,  14.1604,   4.3271,  99.5124,  33.5406,  14.7869,   4.4238,\n",
            "          7.9268,  45.5238,  19.4036,  42.6712,  39.9572,  42.6832,  22.0973,\n",
            "          8.3715, 122.8809,  53.8344,   3.3703,  28.9074,  15.2980,   9.1492,\n",
            "         14.2728,  24.5805,  24.8535,   5.8767,   7.9655,  10.9674,  13.6811,\n",
            "         37.3196, 108.0454,  10.9561,  39.8298,  51.0395,  11.8551,   6.2639,\n",
            "         23.4692,  19.0463,   6.9483,  15.8395,  14.2737,  29.4660,  28.2594,\n",
            "         21.7965,  32.6002,  25.0473,  11.5294,  12.8447,  14.6148,   1.9462,\n",
            "          3.5823,  42.7057,  50.2098,  36.9790,   9.3632,  47.7728,  11.7332,\n",
            "          2.2917,   4.4623,   4.8802,   5.6696,   8.8868,   4.8935,   5.7861,\n",
            "          6.6667,  22.9260,   6.2265,  10.5042,  96.2239,  74.2088,  45.9712,\n",
            "         89.8822,  72.2104,  42.6228,  11.3543,  54.6459,  41.8128,  33.2493,\n",
            "          3.7188,  47.9701,   5.0594,  10.8092,  17.6014,  16.8436,   8.3756,\n",
            "        151.2003,  44.4974,   6.4086,   9.9779,  41.8862,  73.8741,   7.7928,\n",
            "         55.1318,  78.7847,  12.7568,  10.7622,  32.2512,  37.2244,  45.6884,\n",
            "        101.8888,  18.6112,   6.4769,  24.2573,  88.3956,  14.6622],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 48.9253,  13.5393,   9.7257,  18.4007,  62.1095,   7.5266,   0.4367,\n",
            "         30.9955,   7.2568,   8.5519,  22.2933,  17.0550,   6.1178,  18.4334,\n",
            "         58.1999,  31.1359,  39.5796,  27.5562,   5.6010, 112.5582,   2.8629,\n",
            "         37.9686,  35.2450,   6.1950,  30.4325,   8.5625,   2.8924,  12.2690,\n",
            "         69.2314,  18.0336,  16.4839,  30.6596,  49.7520,  61.5341,  17.8930,\n",
            "         22.8567,  37.2575,   5.0507,  29.1111,  68.9257,  47.9174,  29.0901,\n",
            "          7.0875,  37.0986,  57.9891,  12.5239,   7.3330,  29.3928,   9.4159,\n",
            "         35.5430,  33.1313,   1.2523,   5.6476,   6.0830,  13.5175,   9.7004,\n",
            "          2.1516,   7.1928,  52.2444,  16.5548,   6.1138,  75.4569,  16.8083,\n",
            "          7.9776,  50.3142,  56.8584,  24.9141,   5.2819,  11.6875,   1.8057,\n",
            "          8.1804,  43.7465,  28.2593,   5.1450,  11.5707,  49.2531,  51.3396,\n",
            "         11.1266,  45.0874,  85.0756,  21.8115,  48.4863, 403.4291, 197.9868,\n",
            "         94.6027, 131.8396,  40.7584,  16.3072,  56.4084,   0.4046,  14.9438,\n",
            "         27.3987, 163.5254,  19.0263,  29.1138,   7.2113,  42.0599,   2.3997,\n",
            "        232.5983,   9.4886,   9.3119,  12.0806,  16.6665,  18.4120,  73.7206,\n",
            "          5.3723,  10.2010,  23.1282,   5.4172,  11.6518,  38.4512,  11.1652,\n",
            "         44.0988,  23.5012,  22.0300,  33.8727,  25.2714,  66.5987,  20.3978,\n",
            "         32.4914, 117.3186,   3.1265,  71.4068,  79.7904,   0.6307,   2.7608,\n",
            "         19.8781], grad_fn=<SumBackward1>)\n",
            "tensor([ 58.7603, 124.4503,  45.7046,  53.4394,  40.0911,  35.6722,  18.2443,\n",
            "         28.6917,  58.3565,  29.1993,  37.0735,  27.3447,  25.4168,  46.8168,\n",
            "         45.1604,   3.5117,   1.2316,   2.6257,   8.2355,  30.9162,   3.5897,\n",
            "         18.9184,  23.0241,  38.4701,  93.7076,  54.4364,   8.3495,   3.1717,\n",
            "          5.4488,   6.3159,  15.5288,   6.9886,   3.8296,  35.0175,   5.9578,\n",
            "         29.3367,  18.3558,  12.5627,   8.4502,   3.7312,  56.4454,  46.1772,\n",
            "         36.1832,   4.7044,   4.3423,  36.2957,  39.4731,  20.1203,  12.1094,\n",
            "         49.8392,  29.1870,  56.1243,  59.1088,  54.4062,  30.8665,  18.6821,\n",
            "         47.7359,  42.9048,  66.3721,   0.3945,   8.0989,  62.5990,   5.9632,\n",
            "         21.0445,  21.9505,  23.8035,   7.5721,  51.9166,   8.3311,  24.2107,\n",
            "         76.9146,  37.8299,  36.0555,  27.2096,  24.7542,  12.4794,   9.1269,\n",
            "         18.7642,   9.3168,  10.4240,   3.5786,  41.9353,  20.9906,  18.3696,\n",
            "         25.9794,  17.7696,   2.5201,   4.4372,   8.6649,   3.0479,  24.3923,\n",
            "          1.7104,  62.6776,  27.7673,  75.4366,   8.8858,  35.8102,  31.3222,\n",
            "        310.7347, 106.0066,  21.4276,  40.8093,  26.5767,  33.1902,  13.9211,\n",
            "         21.3394,  28.0729,   2.9173,  18.9378,   4.5883,   8.8177,  28.5970,\n",
            "         26.3818,  34.9872,  47.1939, 108.1127,  98.8162,  51.8927, 105.9250,\n",
            "         53.8849,  66.6407,  10.1440, 116.3649,  27.6981,  38.5584,  64.0104,\n",
            "         15.9738,  93.7545,  21.7489,   7.4032,  85.2170,  54.4389,  26.8569,\n",
            "         25.7911,  49.4010,  28.6396], grad_fn=<SumBackward1>)\n",
            "tensor([ 18.7052,   8.7169,  43.0424,  97.3963,  25.1359,  13.7402,  17.7658,\n",
            "         35.1916,  65.3418,  15.4317,   3.6978,  27.8743,  18.1991,  23.0895,\n",
            "         66.9420,  47.1047, 162.2464,  26.4700,  77.9230,  35.9216,  65.6830,\n",
            "         96.0680, 129.9734,  54.2186,  56.4650,  19.0297,  15.6742,  14.7499,\n",
            "        107.0055,  66.6518,  18.9805,  57.0025,  16.9211,  23.0911,  23.3374,\n",
            "         10.4598,  32.4991,  23.1361, 111.7457,  44.7159,  38.3353,   3.0622,\n",
            "         72.9425,  12.2996,  47.0476,  23.4950,  33.5122,   6.6981,  43.7327,\n",
            "        114.3234,  69.3282,  30.6104,  17.6352,  13.6107,  60.2036,  26.3059,\n",
            "         19.6629,  53.1054,  36.5584,  39.0483,  91.9856,  36.2135,  74.0567,\n",
            "         58.8875,  43.2876,  85.6649,  14.9612,  49.5355, 174.0861, 125.1144,\n",
            "          4.5437, 110.2118, 125.2658,  50.6764,  52.9199,  71.7625,  57.7149,\n",
            "         49.2282,  59.1014,  14.7962, 102.5282,  82.1754, 137.2489, 405.2754,\n",
            "        193.5505, 193.7336, 186.1272,   0.0000,  29.8009,   4.8102,   3.7632,\n",
            "         25.3482,  30.3431,  35.8597,  55.7090,  11.4599,  18.7638,  50.9596,\n",
            "         13.4652,  19.6313,  84.8180,  36.7009,  47.0737,  28.9713,   7.5176,\n",
            "         63.1410, 330.5979,  67.5301, 177.1161,  95.1449,   8.1240,  46.0322,\n",
            "         21.9974,  10.9993,  29.2964,  76.6146, 158.9869, 337.4262,  19.3560,\n",
            "         83.5713,  39.5735,  64.8061, 156.6246, 307.5942,  93.0875, 577.9652,\n",
            "        118.9875, 193.2104, 104.7782, 255.7366, 141.5146,   9.4174,  23.8919,\n",
            "         32.4792,  32.6747,   5.1284,  77.6259, 270.3221,  89.9890,  20.8997,\n",
            "        446.2753,  62.3447], grad_fn=<SumBackward1>)\n",
            "tensor([ 30.2305,  14.3573,  69.8361,  82.3907,   1.7409,   2.0894,  26.1675,\n",
            "         28.7638, 103.2363, 122.9680,   6.6439,  20.3167,  60.7834,  46.7023,\n",
            "         22.9859,  26.8681,  16.2417,   5.8268,  98.4927,   1.6864,   4.3093,\n",
            "          8.3995,  62.1730,   6.0868,  13.1295,   4.2440,  38.5126,  20.0032,\n",
            "         12.8755,  24.7188,  13.1614,  34.2765,  14.5246,  28.5924,  58.2510,\n",
            "         40.1856,  27.3224,   4.3922,   4.0642,   6.9580,   5.3946,  63.9198,\n",
            "         57.7393,  37.8936,  50.3547,  61.6051,  14.5152,   8.4681,  22.1073,\n",
            "          6.8347,  49.1418,  20.7704,   4.1859,   5.2203,   8.3586,  10.0604,\n",
            "         25.4013,  26.9918,   6.3638,   4.9355,  50.8859,  62.4915,  22.4912,\n",
            "         16.6857,  37.1206,   5.2930,  22.7503,  78.3365,  10.4695,  19.4383,\n",
            "         16.8818,  23.5702,  52.3418,  67.2319,  30.9298,  23.3488,   4.4230,\n",
            "         21.5230,  42.2788,  12.5556,  25.6474,  93.0661,  50.3674,  78.0679,\n",
            "         20.7344,  56.3036,  25.2724,  42.8500,  58.7879,  46.8010,  11.6364,\n",
            "         18.1165,  24.9673,  17.6081,  42.6452,  82.0838,   6.4044,   6.1905,\n",
            "         33.0691, 117.4723,   4.4608, 104.9047,   5.3574,  71.2866,  30.0653,\n",
            "         14.0750,   8.8072,  19.0292,  33.9367,  91.4616,   5.1029,  15.8110,\n",
            "         43.0686,  20.4985,  83.3743,  41.5627,  55.6778,  10.8386,  12.6223,\n",
            "          5.0599,  10.9272,   3.6393,  21.6306,  26.8048,  28.8199,  27.3282,\n",
            "         33.9009,  70.4056,  88.5670,  17.4567,  32.8877,  36.5641,  92.9026,\n",
            "         19.7906,  27.2238, 324.9384], grad_fn=<SumBackward1>)\n",
            "tensor([ 15.0999,  28.4098,   8.6377,  57.5512,  12.6559,  10.2554,  68.2573,\n",
            "          5.7357,  28.1674,  23.0564,  22.0792,   5.9257,  56.3762,  17.8949,\n",
            "          1.5300,   2.4074,   9.6735,  76.0650,  26.6116,  35.7274,  26.5938,\n",
            "         13.9050,   5.9997,  17.3965,  35.4303,  45.1180,  16.6270,   3.0006,\n",
            "          9.5743,  18.8465,   8.5856,  15.9454,   5.2991,  34.3806,  41.2478,\n",
            "         16.1950,  19.0326,  43.8839,  83.6358,   4.1178,  19.7613,  35.4444,\n",
            "         14.5839,  15.8153,  43.1739,  13.6431,   1.9898,  21.8437,  11.6615,\n",
            "          3.1012,   0.5590,   5.3160,  28.9509,   3.1926,  32.5672,  24.0209,\n",
            "         25.1749,  44.0270,   6.7903,   7.5555,  24.8231,  19.0402,  22.1446,\n",
            "         90.3480,   5.3320,  85.0556,  19.1722,   6.2238,   4.7044,   6.6531,\n",
            "         35.8441,  43.4613,  31.8290,  21.0766,  58.1189,  12.7170,  27.7649,\n",
            "         38.8669,  46.7251,  38.1966,  35.5334,  35.1185,  57.0875,  57.3802,\n",
            "          8.7214,  60.6096,  94.1962,  48.7846,   0.3863,   4.2009,  11.2735,\n",
            "         28.8799,  10.0679,  10.3333,  12.3523,  45.4779,  66.0032,  38.3767,\n",
            "         66.5822,  23.4650,   4.5470,  21.6194,  27.1132,  24.6289,   4.1613,\n",
            "         26.6451,  51.7593,   1.6533,  68.5290,  89.8353,  41.1517,  92.3137,\n",
            "         31.5591,  27.6637,   4.2791,  92.8661, 149.3225,  51.0766,  61.4440,\n",
            "         45.9864,  16.8911, 112.6674,  73.3735,  26.9560,  36.4174,  23.9245,\n",
            "         59.2914,   8.5353,  58.2958,  66.4795,  40.6468,  58.4317, 159.4033],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 41.5421,  18.1220,  17.5289,  54.0373,  11.8970,   9.3890,  19.1597,\n",
            "          8.5218,  33.5657,   3.8631,   7.5342,  29.8380,   9.9890,  23.3014,\n",
            "         49.5744,  64.9839,  19.3537,  19.1948,  29.0996,   8.2430,   7.6425,\n",
            "         24.0669,   6.2302,  26.6984,  14.2223,   1.3952,  29.7491,  72.6621,\n",
            "          7.6502,  21.7686,   9.4997,   6.9848,  29.4187,  40.1030,  46.1283,\n",
            "         10.5525,   1.1874,  42.3515,  15.6365,   6.8833,  43.0872,  15.4769,\n",
            "         29.2514,  58.0747,  25.6040,  18.7440,  26.7283,  17.4263,  22.8607,\n",
            "         32.0602,  31.9140,  21.4187,  10.1801,  22.9892,  26.1262,  27.0475,\n",
            "          6.6943,  15.5823,  25.0496,  29.3432,  16.2571,  60.7641,  12.4098,\n",
            "         32.4310, 111.9401,  13.2843,   7.0337,  18.0918,  14.7594,   3.2120,\n",
            "         29.8316,  21.5602,  18.0359,  36.8797,   5.9127,  11.7221,  49.5079,\n",
            "         52.5972,  36.2754,   9.0902,  24.5973,  26.4767,   3.4627,  20.7253,\n",
            "         80.0358,   7.8365,  37.7157,  42.5510,  49.8988,   4.2353,  30.9443,\n",
            "         10.8034,  29.4406,   3.4748,  21.5551,  11.4686,   9.6759,  12.1957,\n",
            "          6.9193,  12.2088,   4.2816,   2.6120,   4.4625,  28.8464,  23.9939,\n",
            "         42.6845,  15.2571,  37.8890,  13.1600,  13.1254,  32.9543,  13.5318,\n",
            "         40.2132,  35.2116,   5.0166,  48.2865,  18.5890,  19.8404,  15.6646,\n",
            "         30.6335,  93.4706,  67.5262,  38.4061, 121.0331,  14.9302,  40.9639,\n",
            "         59.9801,  15.0872,  49.6684,  13.9014,  27.3552,  28.7235,  31.8160,\n",
            "         33.6298,  37.6446,  33.5760,  19.8213,  59.3027,   9.9880,  41.5054,\n",
            "         10.9202,  46.2966,  40.2968,  13.0498,  17.6817,  13.3834,  15.5878,\n",
            "         43.2433,  39.6611,  43.4197,   1.2031,  15.3213,  24.8298,  30.6648,\n",
            "        108.2676,  68.2117, 144.7409, 102.0794, 131.0102],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([  3.2125,  12.4333,  17.3818,   8.4054,   4.8244,   8.0738,  10.7797,\n",
            "          3.2585,   9.9430,  28.1407,   8.5259,   9.8453,  40.1736,  77.4606,\n",
            "          2.9846,  69.7242,  36.2058,  55.2726,  11.2444,  47.5588,  66.4254,\n",
            "         10.6741,  21.1800,  21.1934,   4.1133,  31.8389,  29.2376,  29.7326,\n",
            "         68.1151,  21.2045,  15.7619,  15.4544,  19.1449,  11.4208,  23.3514,\n",
            "          3.0443,   3.0795,   4.7646,  16.0260,  16.6290,  29.3573,  88.6513,\n",
            "         25.4421,  36.1063,  36.4330,  27.6420, 103.7176,  12.6810,  18.8282,\n",
            "         64.8648,   3.4735,  11.2482,   0.2756,  20.5541,   6.0094,  24.2349,\n",
            "         12.2943,  35.8832,  64.7664,  10.0387,   9.4001,  23.9716,  19.5615,\n",
            "         32.7936,   6.4752,  10.1366,  28.4417,  43.9442,  28.3854,  59.7555,\n",
            "         35.9598,  28.1740,  38.0279,  22.5485,   8.7686,  21.3206,  33.9851,\n",
            "         30.8399,  15.7940,  16.3446,  26.7423,   9.0053,  92.7853,  35.7859,\n",
            "        148.9304,  21.2764,   4.7780,   4.4569,  16.5833,  16.5338,  26.8129,\n",
            "          8.4466,   7.6087,  24.2002,  57.8414,  95.3656,   6.5600,  41.9274,\n",
            "          0.2196,   5.9041,  28.8327,   0.8608,  14.5542,  21.9385,  18.4749,\n",
            "         10.1262,  28.1519,  36.5048,  23.0207,  35.1938,  14.8291,  37.8223,\n",
            "          9.4723,  43.3533,  35.6982,   9.9052,  34.5941,  23.7061,  41.2414,\n",
            "          9.0045,  85.4597,  44.4304,  37.8082, 145.1880,   7.5216,   7.8294,\n",
            "         34.8342,  15.9327,  11.8824,  66.2428,  22.3440,   9.0772,  43.0416,\n",
            "         18.3429,  63.1950,  26.2937, 149.9649,  20.7148,   7.1869,   1.0665,\n",
            "         54.2037,  14.6365,  15.9440,  19.0167,  13.6141, 133.4683, 146.7417,\n",
            "         45.6104,  48.3763,  24.6006,  33.7202,  11.6337,  58.6483,   6.0343,\n",
            "         21.2111,  77.9016,  24.6206,  64.5761,  97.8490,  82.3734,  61.7338,\n",
            "         20.1980,  42.8944,  23.4109, 149.2451,   9.1237,  26.1284,  37.4694,\n",
            "         59.5812, 142.3890,  34.1748,  71.9697,  15.6873,  38.1945,  19.4968,\n",
            "         23.9943,  16.6772,  26.5651,  58.5605,  79.1927, 117.5709,  44.9922,\n",
            "         29.4128,  36.9030, 214.9280], grad_fn=<SumBackward1>)\n",
            "tensor([6.4524e-01, 4.5324e+01, 1.6451e+01, 7.4580e+00, 1.0153e+01, 1.7472e+01,\n",
            "        1.2010e+01, 1.8997e+01, 1.9690e+01, 1.8250e+01, 2.8816e+01, 6.0437e+01,\n",
            "        1.5472e+01, 4.5330e+01, 2.0813e+01, 1.9638e+01, 2.0811e+01, 3.0416e+01,\n",
            "        3.9747e+01, 2.7623e+01, 2.3504e+01, 2.0445e+01, 3.5147e+01, 3.6431e+01,\n",
            "        3.0255e+01, 1.3416e+01, 1.7598e+01, 1.0495e+02, 3.6594e+01, 8.6605e+01,\n",
            "        6.8542e+01, 1.2128e+01, 2.3243e+01, 1.3838e+01, 4.1980e+01, 6.1565e+01,\n",
            "        5.1647e+01, 5.2521e+01, 6.1864e+01, 3.3976e+01, 4.6173e+00, 1.6741e+01,\n",
            "        3.4115e+01, 1.8475e+01, 4.5769e+01, 3.0538e+00, 1.4811e+01, 1.2093e+02,\n",
            "        9.3700e+01, 1.4207e+01, 2.1047e+01, 5.3232e+01, 1.9416e+01, 3.7376e+01,\n",
            "        3.0907e+01, 6.9597e+00, 1.7115e+01, 1.9780e+01, 1.7716e+01, 1.4675e+01,\n",
            "        3.0951e+01, 6.2842e+01, 4.2649e+01, 2.7747e+01, 4.3655e+01, 5.6949e+01,\n",
            "        5.2034e+01, 8.6519e+01, 3.1732e+01, 4.3065e+01, 1.4468e+01, 1.4136e+01,\n",
            "        2.4117e+01, 1.0944e-01, 7.0624e+01, 1.0146e+01, 6.4703e+00, 2.0996e+01,\n",
            "        4.2809e+00, 2.5882e+01, 3.7018e+01, 3.8687e+01, 5.2719e+00, 4.7662e+01,\n",
            "        2.7266e+00, 3.4289e+00, 7.8399e+01, 7.2880e+01, 5.5294e+01, 4.6397e+01,\n",
            "        8.4729e+00, 5.5258e+01, 4.2508e+01, 1.0048e+01, 2.6891e+01, 3.6681e+01,\n",
            "        4.4044e+01, 3.5745e+01, 2.5021e+01, 4.8791e+00, 9.7993e+00, 3.9357e+01,\n",
            "        1.7486e+01, 5.3673e+00, 1.3990e+01, 2.0453e+01, 1.7602e+01, 2.1077e+01,\n",
            "        2.4563e+01, 2.0256e+01, 8.5252e+00, 1.2463e+01, 4.2596e+01, 3.9429e+01,\n",
            "        2.7482e+01, 4.3900e+00, 7.2611e+01, 3.2603e+01, 7.5515e+01, 2.7865e+00,\n",
            "        1.4830e+01, 1.2013e+02, 6.4688e+01, 4.4444e+01, 3.2416e+01, 3.1835e+01,\n",
            "        3.8911e+01, 2.5164e+01, 2.9523e+01, 3.1177e+01, 2.7014e+01, 1.0161e+02,\n",
            "        5.1452e+01, 1.0356e+02, 8.5435e+01, 2.2699e+01, 2.7523e+01, 8.6260e+00,\n",
            "        2.8838e+01, 3.0428e+01, 1.8323e+01, 5.3997e+00, 1.7480e+01, 4.3539e+00,\n",
            "        3.1011e+00, 5.6911e+01, 9.5285e+00, 1.2316e+01, 1.6238e+01, 1.7861e+01,\n",
            "        6.8834e+01, 6.8716e+00, 6.9875e+01, 3.4586e+01, 8.2300e+01, 8.6858e+01,\n",
            "        1.2696e+02, 3.1796e+01, 1.0049e+02, 2.5048e+01, 1.7262e+01, 8.1067e+00,\n",
            "        4.8302e+01, 6.9384e+00, 3.8499e+01, 1.4592e+01, 9.8833e+00, 1.6986e+01,\n",
            "        2.1348e+01, 1.7742e+01, 8.5867e+00, 2.0962e+01, 4.4009e+01, 1.8466e+01,\n",
            "        1.2399e+01, 9.4450e+01, 3.1164e+01, 3.9459e+01, 5.8564e+01, 7.5027e+01,\n",
            "        4.8299e+01, 2.5431e+01, 4.1694e+01, 4.6546e+01, 8.8361e+00, 4.5246e+01,\n",
            "        5.4511e+01, 1.5647e+02], grad_fn=<SumBackward1>)\n",
            "tensor([ 15.5853,  33.2593,  38.3709,  24.0885,  17.1597,  66.2721,  21.9083,\n",
            "          2.6785,   6.1288,  34.7336,  13.1515,  10.7055,   2.6201,  16.1066,\n",
            "         22.7031,   8.3374,   1.5090,  32.2885,   0.5013,  12.3889,  33.8766,\n",
            "         48.6070,   5.5293,  35.2051,   3.2431,  58.3343,   7.2145,  34.7829,\n",
            "         31.0544,  13.5998,  53.4220,  14.1206,  30.5855,  12.0829,  23.9885,\n",
            "         44.1324,  46.2261,  25.8188,   2.0650,  10.9692,  15.2526,  13.4641,\n",
            "         34.9142,  21.3209,  16.0069,   7.7807,  41.9070,  71.0033,  19.8118,\n",
            "         24.4920,  39.8647,   6.6972,  37.0938,  37.9161,  13.4226,  22.4803,\n",
            "          5.8801,   8.4232,  11.7435,  25.5151,   1.3883,  39.9444,  22.4827,\n",
            "         39.1417,  18.2974,  15.3139,  23.0865,  18.4340,  10.4697,   7.7195,\n",
            "         16.3320,  12.0941,   8.0700,  18.3209,  26.5955,   7.8452,   1.1085,\n",
            "         32.8197,   1.0240,   4.6105,  26.2556,   8.6661,  34.3797,   0.9323,\n",
            "         52.0181,  22.5991,  10.0385,   2.9266,   4.4721,  11.4022,  45.4858,\n",
            "         83.6834,   9.4589,  29.1242,   2.3224,   5.5934,   8.8542,   5.0541,\n",
            "         39.1146,  14.6537,   3.8576,  28.1128,  14.7275,   8.3615,  35.7301,\n",
            "         13.1686,  19.5399,  38.8620,  51.1052,  23.7754,  13.1968,  41.1144,\n",
            "         28.7156,  13.7446,   7.0256,   1.4713,  45.0501,  17.7961,  32.1685,\n",
            "         54.6887,  62.4794, 202.5503], grad_fn=<SumBackward1>)\n",
            "tensor([7.8116e+00, 4.5530e+01, 2.7503e+00, 8.6405e-01, 4.0400e+00, 8.2336e+00,\n",
            "        3.3260e+01, 5.5993e+00, 1.0335e+01, 8.3037e+00, 1.5574e+01, 2.0901e+01,\n",
            "        1.1868e+00, 5.5696e+00, 2.1209e+01, 2.6768e+01, 1.8315e+01, 3.6749e+01,\n",
            "        3.2644e+01, 1.7569e+01, 1.0240e+00, 5.2445e+00, 7.9949e+00, 2.8669e+01,\n",
            "        1.3997e+01, 1.1879e+01, 2.8564e+01, 5.3018e+01, 4.4309e+01, 2.6041e+01,\n",
            "        5.4835e+01, 2.1730e+01, 7.8769e+01, 3.5402e+00, 2.3486e+01, 4.5558e+01,\n",
            "        1.3004e+01, 3.1633e+01, 1.2718e+00, 2.0581e+01, 3.7307e+01, 1.4620e+01,\n",
            "        2.2861e+01, 7.1430e+00, 1.0717e+01, 2.0131e+01, 3.6900e+01, 1.9280e-01,\n",
            "        2.3226e+01, 5.6933e+01, 7.3256e+01, 2.7932e+01, 5.2896e+01, 1.9632e+01,\n",
            "        2.6803e+01, 4.2402e+00, 6.9769e+00, 3.3120e+00, 1.0662e+00, 6.0784e+00,\n",
            "        1.7463e+01, 3.7541e+01, 2.1210e+01, 2.6071e+01, 9.1700e+00, 5.0830e+01,\n",
            "        8.8077e+00, 1.2502e+01, 2.2977e+01, 3.1032e+00, 1.1085e+01, 2.4254e+01,\n",
            "        1.0932e+01, 3.9233e+01, 3.1396e+01, 3.7115e+01, 1.3507e+00, 7.0967e+00,\n",
            "        5.4852e+00, 2.3883e+01, 9.6638e+01, 3.9721e+01, 1.0204e+02, 2.0699e+02,\n",
            "        4.2845e+01, 1.1414e+02, 5.7664e+01, 2.3040e+01, 4.8988e+01, 2.2176e+02,\n",
            "        4.6538e+00, 2.5868e+00, 1.4310e+01, 3.5370e+01, 1.5304e+02, 1.5760e+01,\n",
            "        8.4069e+01, 4.1351e+01, 1.9248e+02, 1.4136e+01, 4.9408e+00, 3.3588e+01,\n",
            "        3.1343e+01, 3.8445e+01, 3.0059e+01, 3.3005e+01, 7.0833e+01, 3.7184e+01,\n",
            "        7.5191e+00, 2.1750e+01, 2.2739e+01, 4.6766e+01, 5.8912e+01, 2.4374e+01,\n",
            "        1.1492e+02, 2.3527e+01, 6.2337e+01, 6.7565e+01, 1.1045e+02, 1.5981e+02,\n",
            "        2.1429e+02, 9.0861e+01, 1.8067e+01, 3.9034e+01, 1.2052e+01, 6.7484e+01,\n",
            "        6.5270e+00, 1.4479e+01, 5.6413e+00, 4.0384e+00, 5.1195e+01, 2.1520e+01,\n",
            "        2.6279e+01, 7.5275e+00, 8.4589e+01, 3.8421e+02, 3.2424e+01, 2.4742e+01,\n",
            "        2.5516e+01, 8.8828e+01, 2.0135e+01, 3.1713e+01],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 19.0657,   5.2081,  16.8641,   8.0390,  18.4478,   9.2585,  69.9915,\n",
            "         15.4250,  29.0020,  91.6236, 106.7201,  28.3759, 146.2648,  86.1165,\n",
            "         24.0388,  32.3728,   9.5279,   8.1282,  21.4496,  16.6928,  18.8296,\n",
            "         23.9909,  42.2002,  16.0651,  44.3918,  10.4131,  23.5519,  74.6223,\n",
            "         64.6557,  29.0919,  32.4941,  63.2153,  56.0298,  43.5015,  60.7922,\n",
            "         26.1461,  13.5199,  90.4374,  35.2356,  32.7357,  18.5388, 104.1439,\n",
            "         99.7819,  24.4538,   9.3057,  27.4396,  31.9412,  49.1575,  15.7176,\n",
            "         13.7282,  68.8870,   6.1103,  31.2845,  23.3010,  13.2149,  17.1240,\n",
            "         51.5970,  63.4343,  23.1731,  23.9320,  27.5388,  42.4368,  62.4703,\n",
            "         21.5754,   0.0000,  14.0529,  20.3138,  12.3937,   1.3172,  75.9041,\n",
            "         58.5240,  23.8177,  50.3528,  24.2718,  31.5196,  31.4715,  28.8026,\n",
            "         17.1878,  36.1740, 116.0037,  63.3231, 115.3105,  21.2620,   4.6203,\n",
            "         54.8432,   8.2560,  30.5430,  10.0729,   9.2399,  26.0927,  11.3015,\n",
            "         27.8077,   3.6961,  16.6760,   3.9171,  44.8764,  34.5950,  62.4156,\n",
            "         37.3456, 155.7794,   2.6396,  76.2048,  21.4449,  36.9593, 123.0029,\n",
            "         45.0546, 127.7817,  64.9147,  65.0543,   9.0046, 130.1277,  27.6792,\n",
            "        282.9663, 197.1025, 266.6055,   8.3835,  10.3140, 347.1753,  58.2877,\n",
            "         27.6559,  63.7918,  29.5318,  18.2291,  10.9144,   0.8603,  19.3393,\n",
            "         17.3646,  83.2778,  35.0633,   5.8640,  20.0453,   8.2710,  25.2752,\n",
            "          8.6459,  13.0633,  60.7567,  26.3258,  12.1742,  39.8749,   2.4522,\n",
            "         30.8081,  55.5759,  36.1200,  18.4432,  36.3005,  19.7498,  20.3315,\n",
            "         54.6650,  78.5037, 133.0894, 364.5193,   7.3879,  25.7065,   8.7428,\n",
            "         14.0204,   8.6090,  34.5170,  50.3128,  55.2144,   5.6335,  66.0197,\n",
            "         27.0635,  11.2211,   5.9308,   5.5378,   0.0000,  28.6154,  30.1199,\n",
            "         23.5557,  19.3100, 100.7696,  97.4300,  46.3036,  60.5698,  63.5053,\n",
            "         46.3251, 132.5135,  36.7912, 219.3034,   8.9245,  93.7532,   9.8376],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 44.8906, 109.8160,  46.1294,  90.2045,   0.7144,  31.9164,  16.3269,\n",
            "         28.7886,  28.1344,  28.6597,  35.7868,  34.6168,  31.2887,   4.2553,\n",
            "          6.1826,  31.2939,  13.5123,   5.7498,  28.9609,  20.4670,  15.7478,\n",
            "        124.1340,  26.2420,  57.3790,  63.3099,  15.0307,  12.1740,  44.9584,\n",
            "         40.7878,  30.6985,  18.6269,   0.4910,  11.4232,   9.0904,  15.7629,\n",
            "         14.5147,   4.2346,  13.2732,   6.4646,  14.0058,   5.7796,  12.5151,\n",
            "         33.0065,  46.0088,  15.5686,  41.1136,  32.2294,  27.2018,  91.5395,\n",
            "         46.2867,  34.0071,  12.4817,  23.0602,  13.3260,  53.4284,  57.8095,\n",
            "         85.2853,  41.3814,  15.5949,   6.5578,  11.2324,  43.1283,   8.6100,\n",
            "         26.1657,  45.0600,   7.5910,  18.3986,  12.3890,  33.7382,  12.1875,\n",
            "         56.5623,   1.8350,  35.9908,  10.4169,  30.9569,  28.6665,  53.4343,\n",
            "         39.3129,   6.6732,  40.8253,  23.6355,  79.4747,   6.5686,  42.1090,\n",
            "         25.7432,  22.5265,  40.5295,   8.5350,   0.8657,   1.9885,  13.6415,\n",
            "          4.5612,  24.3758,  49.6212,  58.7794,  22.1984,  75.1877,  53.4066,\n",
            "         41.4030,  18.5290,  39.7000,  41.6053,  31.4139,  57.0463,  22.3986,\n",
            "          7.4031,  94.9659,   2.6810,  26.1989,  15.6493,  32.6585,  71.3038,\n",
            "          9.0027,  59.1671,  32.2920,  45.8976,   8.8633,  15.5086,   4.9736,\n",
            "         26.2318,  24.8544,  21.1694,  58.9168,  14.7472,  26.3312,  30.3686,\n",
            "         35.6459,  32.3149,  38.9657,  57.2558,  22.3536,  39.5961,  65.9794,\n",
            "         19.0868,  48.0590,  71.5801,  10.5788,   0.7752,  45.5134,  51.5878,\n",
            "         30.5571,   1.9419,   8.8951,   7.6925,   7.4111,  14.0612,   6.5354,\n",
            "         40.5966,  16.5575,  11.2305,   5.5190,   7.6914,  25.7648,   6.8333,\n",
            "         19.6968,  37.7618,  26.2853,  47.4449,  59.1295,  10.3891,  59.8940,\n",
            "         33.3882,  38.0466,  25.6025,   6.3675,  44.1412,  28.8691,  59.7846,\n",
            "         31.1840,  53.4364,  44.1174,  34.3624,  12.2361,  70.7391,   9.5841,\n",
            "         30.9722,  88.1314,  31.2348,   9.9223,  16.8947,  72.8748,   4.2704,\n",
            "         25.3127,  24.1083,  53.3830,  11.9807,  21.1356,  25.3051,  16.8633,\n",
            "         19.1502,  13.7121,   4.1173,  68.6887,  21.2183,  84.8256,  28.3959,\n",
            "         34.3138,  94.4052,  42.1005,  18.2502, 109.9026,  62.5225,  50.2852,\n",
            "        164.4335,  73.0804], grad_fn=<SumBackward1>)\n",
            "tensor([  2.1110,   1.7782,   7.5634,  12.1702,  50.6781,  11.3393,  12.3071,\n",
            "         36.3903,  12.6924,  29.8009,  68.2736,  15.1136,  14.1167,  11.9159,\n",
            "         13.3826,  15.9799,  20.1086,  36.7673,  31.3031,  40.9889,  17.3783,\n",
            "         12.6681,  14.5899,  54.2557,  50.0444,  22.2476,  42.9459,  22.2522,\n",
            "         17.9647,  24.2623,  30.4767,  51.4079,  21.3692,  16.0986,  94.2555,\n",
            "         14.9625,  50.6015,  30.0835,  11.1610,  22.6450,   9.5891,   7.2971,\n",
            "         20.0467,   9.2913,  10.0585,   6.6076,  25.2621,  80.4598,  56.2959,\n",
            "         35.7575,  28.4555,  17.9495,  11.2369,  22.0126,  22.1722,  33.8683,\n",
            "         26.1976,   7.2238,  35.7477,   7.3683,  85.0909,  14.1225,   6.7235,\n",
            "         34.7596,  40.9043,  59.1053,   6.2695,  27.5773,  73.4573,  33.8591,\n",
            "         28.8374,  39.5474,  10.4580,  22.4277,  46.2706,  10.6952,  29.8330,\n",
            "         16.4897,   5.8579,  25.0062,  33.7593,  27.5764,   7.4700,  34.4560,\n",
            "          2.1479,  31.0960,  33.6175,  39.0763,  25.3628,  43.9024,  10.1187,\n",
            "         15.7950,   2.7970,  11.0735,   7.4692,  28.1061,  10.4142,   7.4446,\n",
            "         14.6063,  13.2647,  18.5223,  30.2238,   8.1352,  26.0398,  22.8933,\n",
            "         39.7160,   9.1604,   1.8731, 117.0885,  17.2989,  43.8519,  26.1567,\n",
            "         33.5889,  10.0449,  11.9134,  25.8756,   3.6585,  18.8611,  14.4605,\n",
            "         18.3140,  35.3455,  23.4886,  17.3762,  10.7000,  18.1270,  15.0675,\n",
            "          7.6624,  18.5773,  27.2826,  19.2398,  43.2722,  20.8454,  38.9433,\n",
            "         26.4923,  16.8542,  45.4514,  18.3534,  16.8193,  22.8397,  34.8843,\n",
            "         23.7251, 109.7792,  32.2093,  44.6096,  64.2295,  17.2610,  16.6562,\n",
            "         15.1274,  36.1245,  21.3620,  18.0471,  38.3941,  16.1442, 109.3690,\n",
            "         96.9082,  65.8153,  39.4686,   2.1168,  15.1711,  32.9167,  14.6600,\n",
            "         24.9041,  49.0414,  34.7941,  47.1581,  28.5177,  50.4129,  35.9441,\n",
            "         65.0246,  52.8913,  31.1559,  64.3336,  51.5998,  29.9344,  16.8982,\n",
            "         11.0419,  24.9784,   8.7268,  68.8353,  46.8049,  13.9447,  53.6977,\n",
            "         47.7514,  22.1951,  23.5094,  64.7248,   8.3968,  24.6563,  14.0407,\n",
            "         32.1866,  30.7695,  45.0159,  34.2781,  16.9622,  58.4083,  34.0974,\n",
            "         31.8515,   9.4836,   0.9878,  18.7628,  19.0753,  32.6416,  34.1762,\n",
            "         40.0944, 254.9502,  75.0671, 174.4019, 246.7469,  95.7286,  68.6337,\n",
            "        138.5836, 107.4222,  91.9939,  64.2259, 177.2416,  26.8801,  94.1981,\n",
            "         93.5215,  22.2516,  26.7574,  76.7448,  66.1992,   1.7490,  31.3689,\n",
            "         26.1194,  14.7068,  26.1262,  14.1301,   4.0424,  59.9171,  17.4957,\n",
            "         15.1152,   4.4785,   8.2211,  13.7630,  85.4375, 124.9511,  61.2491,\n",
            "         14.3624,  10.9822,  23.5133,  78.2400, 209.3912, 154.1408,  63.0890,\n",
            "          6.1609,  20.6987,  77.9912,  36.2916,  10.8717,   5.8901,  78.7459,\n",
            "         18.3722,  59.1155,  25.7270,  79.4901, 234.1948,  99.2979, 163.1636,\n",
            "         42.3276, 104.5506, 136.0757, 137.9047, 121.1399,  15.1130],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 24.0215,  42.6501,  58.4037,  21.6490,  24.8619,  37.8775,   7.7280,\n",
            "         51.2788,  21.4035,  23.6873,  28.1495,   8.9573,  27.6738,  15.3626,\n",
            "         22.7863,  13.3226,  11.4956,  21.3110,   5.0871,  27.6340,  40.5082,\n",
            "          7.3699,  64.0057,  12.8179,  29.8130,  16.6279,  38.4649,  17.9426,\n",
            "         34.0032, 158.8246,  37.5503, 188.9914,  24.3222,  10.8675,  25.7334,\n",
            "         57.6361,  60.3385,  56.9110,  73.4392,  46.7466,  84.5807,   2.0666,\n",
            "         55.4844,  14.2171,  46.8754,   6.6592,  15.4930,  27.2689,  18.7388,\n",
            "         78.5493,  21.4431,  72.4137,  87.6763,  43.9414,  32.7716,  19.3563,\n",
            "          9.2776,  10.6890,   4.5682,  25.7764,  16.2220,   7.4993,   4.7144,\n",
            "         33.6963,  14.6398,  33.0986,  10.1910,  10.9398,  10.9663, 197.1723,\n",
            "        357.4540, 234.8714,   7.2396,  29.3351,   4.7648,  37.5715,  14.8929,\n",
            "         38.7427,   4.4452,  45.7272,  37.6535,  40.1124,  60.2660,  48.2584,\n",
            "          5.3725,   3.3678,   5.6192,  10.6889,  11.1133,  17.6957,  31.5962,\n",
            "         22.6470,  10.7069,   1.7253,   2.0604,  18.2860,  28.6223,  10.3086,\n",
            "         14.9973,  23.4342,  10.8219,   9.1107,   2.0236,  39.2606,   8.6619,\n",
            "          3.6896,   3.7229,   1.2303,  44.5790,  15.7601,  23.4098,  29.8080,\n",
            "         35.7715,  22.0254,  17.5503,  12.3597,  12.8408,  10.4459,   4.7793,\n",
            "         75.2885,  26.0298, 157.8003,  11.9822,  25.0484,  16.4999,  26.1836,\n",
            "         16.4842,  40.7054,  30.1384, 113.8068, 128.9622,  29.4782,  55.8022,\n",
            "         86.3202,  43.0725,  50.3685,  26.9745,   2.9908,  54.0368, 178.7203,\n",
            "         50.8027,  69.6473,  12.7989,   3.8103, 110.3378,  37.1169,  58.7866,\n",
            "         43.4132, 173.5089,  57.0218, 144.4130, 160.4948,  28.7267, 154.6146,\n",
            "         53.1249,  16.2656,  16.5333,  44.4187,  25.9336,  29.9756,  39.9973,\n",
            "         31.3887,  31.0481, 118.7779,  98.8044,  53.1697,  34.2411,  55.1705,\n",
            "         48.2397,  77.8683, 115.8828,  85.5957,  15.0949,  37.1162,  59.9646,\n",
            "         38.2541, 112.2648,  42.5112,  16.7527, 175.5121, 157.2735,  47.8114,\n",
            "         50.1924,  61.5469,   8.5778,  44.6293,  42.4258,  53.4347,   6.1025,\n",
            "         58.6245,  59.0086, 141.8314,  64.9751,  10.7486,  13.7368,  21.1049,\n",
            "         56.1095,   7.8099,  53.6324,  16.4287,  42.1980,  46.0433,  27.0112,\n",
            "        137.8915, 255.9921,  92.7522,  12.8355,  28.5428,  28.0308,  72.5256,\n",
            "        106.1710,  45.4463,  65.9228, 123.8713,  85.2476,   9.6615,   6.7444,\n",
            "         20.3594,   3.8137,  26.2776,   7.6030,  66.7441,  16.1565],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 72.1049,  37.9029,  11.4035,  36.8976,  72.7985,  16.2504,  62.5015,\n",
            "          2.9066,  10.4745,  24.2107,  15.2749,  65.3317,  83.8303,  60.5898,\n",
            "         75.8705,  48.3994, 126.4561,  89.4726,  27.0961,  79.9358,  15.3651,\n",
            "         10.3835,  31.9520,  11.9375,   6.2182,   6.9624,  21.8744,  16.3576,\n",
            "          1.2631,   0.5604,  32.6806,  18.6922,   4.4019,   6.0598,  25.5479,\n",
            "         26.9575,  12.2318,  26.3146,   5.8509,  39.7430,  63.2217,  54.6281,\n",
            "          9.0010,  71.8043,  67.8946,  47.9483,  35.2845,  32.0990,  35.7293,\n",
            "         15.9162,  25.3188,  20.4818,  11.7025,  15.6788,  18.0083,   7.9644,\n",
            "          3.5471,  22.5098,  56.0929,   2.5444,  10.2022,  12.5103,   0.7175,\n",
            "         11.6956,  11.6485,   4.0537,   6.9020,   2.9430,  25.6621,   5.5114,\n",
            "         16.6415,  11.0780,  24.7934,  34.5281,  23.8354,  60.7908,  96.7959,\n",
            "          9.2066,   7.5349,  21.0322,  42.4853,  34.7340,  48.0337,  71.2849,\n",
            "         21.3322,  90.6638,  25.0349,   9.4170,  20.3086,  29.9799,  34.3647,\n",
            "          3.9841,  34.6172,  44.2948,  50.8271,  18.1849,  16.2025,   2.8612,\n",
            "         13.0457,  60.5663,   4.1329,  50.8101,  12.1145,  24.2140,  61.8447,\n",
            "         41.9168,  68.9223,  43.8170,  22.6405,  39.3622,  50.2219,  41.2424,\n",
            "         47.4921,  64.4882,  20.2932,  54.0902,  23.1269,  19.3105,  87.6724,\n",
            "         11.0267,  59.2109,  19.9734,  60.2014,  33.7457,   7.4210,   1.7992,\n",
            "         21.0049,  19.0341,  32.1572,  13.4080,   4.3333,  26.7180,  31.8696,\n",
            "         28.6517,  31.4477,   9.8278,  15.3955,   5.8216,   1.5637,  26.0987,\n",
            "         19.5598,  71.1694, 135.3690,  58.6614,  49.4560,  49.6762,  22.1425,\n",
            "         51.1452,  27.8206,  27.6240,  33.5583,  74.6584,  71.8534,  83.0636,\n",
            "         79.2193, 274.8569,  84.8990, 162.2942], grad_fn=<SumBackward1>)\n",
            "tensor([  2.0296,   4.6630,  18.1095,  16.1890,   1.0336,   7.6649,  12.4268,\n",
            "         15.0276,   9.0758,  13.2493,   3.7781,   0.6655,  22.2931,   1.5175,\n",
            "          2.7498,   8.3148,  10.7634,  34.7035,  19.9505,  39.4903,  18.5425,\n",
            "         13.5266,  26.2323,   4.0482,  18.9522,  27.6985,  10.0579,   4.4404,\n",
            "         21.4133,  19.9176,  14.8363,   7.9087,  26.7458,  40.6235,  53.4032,\n",
            "         77.6012,   4.5807,  94.2881,  11.7262,  46.0231,  30.5912, 150.2557,\n",
            "         94.5594,   3.7122,   9.3767,  52.9580,  51.0016,  12.6770,   1.1277,\n",
            "          2.9288,  13.5665,  31.8476,  17.9350,  59.3082, 113.1934,  12.5873,\n",
            "         41.4929,  24.3157,  57.5400,  45.4242,  29.6495,  37.7091,  19.9618,\n",
            "         24.2520,   4.4293,  17.5768,  46.1817,   3.5551,  30.0644,   2.5905,\n",
            "         45.0087,  29.5860,  23.7974,  29.8684,  25.4338,  12.9148,  11.2331,\n",
            "          4.4238,   2.3908,  44.9011,  60.8635,  17.6796, 112.2360,   6.8098,\n",
            "         12.3956,  14.0312,  25.3981,  46.0538,  16.0406,  22.8949,   8.4260,\n",
            "          6.9474,  40.6032,   8.6188,  14.3158,  64.8447,   0.6271,   5.1190,\n",
            "          6.9155,  30.7391,  41.0046,  16.5478,  14.4765,  10.5325,   8.5650,\n",
            "         15.4691,  11.8242,   5.4714, 118.3987,  26.8180,  17.4240,  18.6574,\n",
            "         39.9000,  18.7205, 144.2945,  19.2818,  20.9937,  71.0045,  48.3482,\n",
            "          0.3352,  15.0146,  23.6793,  32.3990,   3.0262,  22.0359,  16.7137,\n",
            "         37.4430,  73.4905,  15.0201,  21.1820,  34.8244,  10.6755,  19.4100,\n",
            "         48.0298,   5.4554,  39.3709,  57.4945,  35.4511,  38.2380,  16.9107,\n",
            "         44.0608], grad_fn=<SumBackward1>)\n",
            "********************************************************\n",
            "epoch:  0\n",
            "\n",
            "LOSS => kld_loss: 16.1204 | nll_loss: 4.7218 | loss: 20.8421\n",
            "tensor([ 0.6451,  4.1281, 18.7130,  0.5763, 11.5827, 19.0378,  5.1134, 18.9545,\n",
            "         8.0246,  9.0466,  3.0926,  5.6698, 26.8739, 52.3001, 13.8744,  5.7934,\n",
            "        23.0940,  7.6962, 19.2704, 17.0443,  2.3964, 12.4829,  1.1334,  2.3810,\n",
            "         9.8101, 11.5560, 38.8395,  1.4893,  4.3146,  1.5085],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([25.1454, 14.7808,  9.5596, 26.3436,  8.4117, 11.4976, 12.3684,  9.2673,\n",
            "         8.9535, 15.1866,  4.2600,  2.2386, 13.4552, 15.2400, 20.6233, 75.5986,\n",
            "         2.8720,  2.4872, 29.3660, 51.7458, 48.5353, 17.5445,  0.6273,  8.7578,\n",
            "         5.8888, 25.4818, 21.8603,  6.4385,  1.5633, 10.2500,  6.1823, 15.2544,\n",
            "         7.8371, 35.3276,  8.0618,  5.7092,  3.1217,  1.1447,  2.2636,  7.1233],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([14.1884, 36.4416, 19.7498, 22.5647, 42.4589, 28.8543,  2.0789,  6.4863,\n",
            "         9.4900,  6.9501,  0.6253,  3.4919,  7.2488, 18.1605,  7.7113,  3.5856,\n",
            "         3.7653, 16.8903,  8.9640,  8.4198,  1.8004,  0.4114, 21.6719,  6.0442,\n",
            "         1.4510,  6.3275,  5.8965, 11.8935,  7.2237,  3.0364,  5.3035,  0.7103,\n",
            "         2.9404, 10.5869, 30.0026,  2.8999, 15.9498, 15.5184, 16.1903,  8.2747,\n",
            "         8.2770,  8.7790], grad_fn=<SumBackward1>)\n",
            "tensor([ 8.6479, 17.8031, 17.6091,  3.7926,  7.5918,  2.0603,  4.6675,  8.0844,\n",
            "        12.8752,  3.8625,  2.3126, 26.5006,  8.8475, 24.0637,  4.8185, 15.9171,\n",
            "         1.3572, 18.0916,  2.3376,  7.3158,  3.9302,  6.1062, 11.3853,  5.6677,\n",
            "         5.7814,  1.9903,  4.1112, 14.2025,  5.9898,  1.0758, 36.1718, 21.6324,\n",
            "         6.4270,  6.6734, 10.5947, 25.2131, 13.5111,  6.3726, 14.7929, 11.0836,\n",
            "        11.8855, 15.2067,  8.8855,  0.4494,  6.3355, 11.3469,  8.1934,  5.6866],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([16.7730, 13.2112, 13.8333,  7.7721, 13.9895,  1.1447,  6.6221, 13.8076,\n",
            "         0.1625, 14.1109, 11.7214,  7.3643,  4.3246,  9.7085, 12.0458, 12.8807,\n",
            "         6.4133,  3.7108, 14.5688, 21.0007,  5.1175, 13.4747,  2.0932, 12.0981,\n",
            "        10.0662,  2.4779,  8.1356, 10.3763, 12.4583,  6.3956,  4.6352, 11.1590,\n",
            "        58.6436, 12.4399,  3.0031, 21.6345,  8.8448, 34.1361,  5.2555, 14.3655,\n",
            "         5.5131,  1.9806,  6.1956, 14.4551,  3.3618,  6.3431, 20.0632],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([8.0315e+00, 3.9338e+00, 5.3785e+00, 1.0525e+01, 1.2584e+01, 1.0232e+01,\n",
            "        3.0469e+01, 1.6879e+01, 2.0088e+01, 6.2603e+00, 5.4880e+01, 8.4093e+00,\n",
            "        2.7543e+01, 3.4769e-01, 1.4352e+01, 5.6824e+00, 9.8155e+00, 3.9792e+00,\n",
            "        1.0163e+01, 3.1849e+01, 2.2206e+01, 8.9141e+00, 1.1165e-03, 2.2232e+00,\n",
            "        3.7004e+00, 3.4389e+00, 7.4800e+00, 1.9789e+01, 1.7112e+01, 2.1251e+00,\n",
            "        5.6773e-01, 1.9664e+01, 2.8245e+00, 1.6186e+00, 1.7430e+00, 1.1860e+01,\n",
            "        1.8911e+00, 2.1668e+01, 5.8528e+00, 4.3717e+00, 2.2158e+00, 8.2702e+00,\n",
            "        0.0000e+00, 9.4319e+00, 6.4341e+00, 1.1959e+01, 3.8877e+00, 5.2034e+00,\n",
            "        9.7654e+00, 1.0275e+01, 2.4580e+00, 3.1258e+00, 2.6956e+01, 1.2882e+01,\n",
            "        1.2342e+01], grad_fn=<SumBackward1>)\n",
            "tensor([  8.8128,  29.3005,  20.3117,   8.1249,  15.6612,   2.7183,  15.6711,\n",
            "         20.9912,  13.0157,  25.7584,   8.9853,  11.1596,  37.8711,   6.1341,\n",
            "          5.1486,   9.1245,   6.5752,   6.3622,  29.1071,   1.7834,  24.1921,\n",
            "         21.8713,  10.3720,  19.6518,   4.0947,   3.1165,   1.6900,   0.1662,\n",
            "          8.0583,   9.5974,   5.4723,   9.2562,   1.8509,  10.4428,  29.4746,\n",
            "          7.1242,   6.7407,  12.5248,   6.2696,   5.8741,  15.7660,  11.2539,\n",
            "          0.3086,  11.2150,   4.2127,   2.8794,   7.0796,   8.8430,  18.5466,\n",
            "         67.9932,   7.8945,  52.6429,  40.6306,   9.2917,  17.4175,   4.0297,\n",
            "         19.7281,   3.5522,   2.3961,  16.7636,  15.1773,   5.9554,  27.0621,\n",
            "         42.7809,  10.0157,  11.4482,  11.6575, 124.9730,  74.0982,   1.5763],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 5.9408, 15.1226,  7.5941,  2.1113, 43.7010, 29.5993, 21.6722, 26.3811,\n",
            "         5.1410,  5.4648,  1.4408,  1.5300, 10.3394,  4.6131, 24.3870, 23.6963,\n",
            "         6.5437,  3.7301,  7.3105, 14.2720,  7.7668, 16.9384,  2.7186,  7.7597,\n",
            "         0.8927, 12.5941,  7.4299, 14.1008,  2.0934,  8.2962, 14.2823, 31.1400,\n",
            "         4.4555,  7.8759, 20.4132,  5.0170, 11.2007,  7.1743, 25.8358,  3.9030,\n",
            "        13.0433, 16.3202, 14.5468,  6.6448,  4.1218,  5.3430,  1.6492,  9.1551,\n",
            "         0.1075,  1.5376,  4.3762,  2.0488, 47.3418, 48.0491,  1.0028,  0.2825,\n",
            "         0.2682,  4.3371,  6.7954,  4.1194,  6.6656, 33.8366,  2.3395,  4.3664,\n",
            "         0.3927, 15.3061, 21.1218, 25.4028, 23.5295, 18.4618, 20.9839,  3.1964,\n",
            "         6.9212, 13.8008,  4.1333,  3.9079, 10.7180,  7.3196, 22.2671, 15.5407,\n",
            "         1.9639, 67.6898,  5.2974,  2.2825,  9.0872,  4.1814, 11.7109,  6.1751],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([8.6371e+00, 1.4574e+00, 6.8155e+00, 3.1858e+00, 1.3077e+01, 9.2627e+00,\n",
            "        1.7781e+00, 1.3691e+01, 1.2229e+01, 4.6014e+00, 5.0987e-01, 2.2711e+00,\n",
            "        7.0217e-01, 1.0407e+01, 1.3111e+01, 6.9865e+00, 7.9343e+00, 3.0986e+00,\n",
            "        4.5693e+00, 6.7820e-01, 1.2912e+00, 7.2107e-01, 3.9144e+00, 2.6766e+00,\n",
            "        1.5052e+00, 3.0382e+00, 7.0555e+01, 2.3073e+01, 9.7714e+00, 6.0872e+00,\n",
            "        1.8808e+00, 1.2570e+01, 1.1452e+01, 1.1727e+01, 1.1498e+00, 1.1738e+01,\n",
            "        2.3411e+00, 1.6382e+00, 1.1251e+01, 2.4733e+01, 2.2575e+00, 3.0201e+01,\n",
            "        8.4683e+00, 3.1454e+01, 3.1565e+00, 1.5625e+01, 7.7029e+00, 1.2566e+01,\n",
            "        1.4466e+01, 9.6697e+00, 6.2866e+00, 2.9657e+00, 5.9969e+00, 6.3533e+00,\n",
            "        5.8191e-01, 3.1354e+00, 7.4399e+00, 8.7494e+00, 2.7110e+00, 2.9298e+00,\n",
            "        1.1780e+01, 1.4124e+00, 2.7265e+00, 9.5728e+00, 1.2686e+01, 2.4213e+00,\n",
            "        9.7583e+00, 1.4447e+01, 1.2194e+01, 2.9457e+00, 4.3787e+00, 6.5532e+00,\n",
            "        1.1711e+01, 1.6777e+01, 1.1766e+01, 6.7086e+00, 1.1388e+01, 4.6593e+00,\n",
            "        9.1555e+00, 5.5409e+00, 3.0914e-02, 1.3716e+00, 1.4886e+00, 6.7859e-01,\n",
            "        3.1564e+00, 1.7025e+01, 8.6809e+00, 1.9326e+01, 3.0792e+01, 1.5120e+01,\n",
            "        8.8451e+00, 1.0791e+01, 1.1617e+01, 1.3285e+01, 5.6317e+00, 9.3539e+00,\n",
            "        6.8861e+00, 1.2935e+01, 9.3506e+00, 1.3688e+00, 8.9453e+00, 1.3100e+01,\n",
            "        2.1378e+01, 1.6283e+01, 2.0025e+01, 1.0684e+01, 1.0478e+01, 1.0257e+01,\n",
            "        1.1774e+01, 6.0334e+00, 4.1876e+00, 5.0963e+00, 2.1305e+01, 5.5996e+00,\n",
            "        2.9745e+00, 3.2410e+00, 7.8582e+00, 2.4654e+00, 1.0368e+01, 8.2688e+00,\n",
            "        1.0054e+01, 1.2891e+01, 1.0967e+01, 2.1762e+00, 9.4951e-01, 2.2036e+00,\n",
            "        4.4589e+00, 8.6074e+00, 1.7956e+00, 5.2596e+00, 7.7594e+00, 2.9473e+01,\n",
            "        6.2615e+00], grad_fn=<SumBackward1>)\n",
            "tensor([1.0801e+01, 1.0538e+01, 1.0470e+01, 8.4281e+00, 7.1363e+00, 1.9674e+01,\n",
            "        2.0324e+00, 9.7524e+00, 9.0237e+00, 2.0693e+01, 1.7525e+01, 2.0782e+01,\n",
            "        9.2993e-01, 1.0808e+01, 1.1940e+00, 1.4950e+01, 1.7994e+01, 1.7645e+00,\n",
            "        3.0503e-02, 4.3577e+00, 7.3246e+00, 8.0027e+00, 8.8824e+00, 4.5948e+00,\n",
            "        2.0150e+00, 1.3361e+01, 1.0060e+01, 1.2448e+00, 3.3322e+00, 2.5791e+00,\n",
            "        5.6659e+00, 1.8753e+01, 1.1987e+01, 7.6941e+00, 1.1503e+02, 8.1405e+00,\n",
            "        7.4603e+00, 6.0360e+00, 7.8066e+00, 1.6822e+00, 5.9004e+00, 1.2285e+01,\n",
            "        1.0010e+01, 2.0495e+01, 1.4673e+01, 1.6776e+01, 1.4457e+01, 2.2403e+00,\n",
            "        8.4117e+00, 3.5765e+00, 1.7148e+00, 3.7831e+00, 5.6251e+00, 1.1839e+01,\n",
            "        1.6672e+01, 6.6215e+00, 8.9441e+00, 5.2147e+00, 3.7387e+00, 8.1936e+00,\n",
            "        3.3741e+00, 1.1559e+01, 2.3545e+00, 1.1893e+00, 1.1517e+01, 2.9393e+00,\n",
            "        1.1029e+01, 1.1061e+01, 1.1550e+01, 1.3266e+01, 1.1730e+01, 4.1604e+00,\n",
            "        1.7824e+01, 2.8501e+01, 7.2018e+00, 2.3920e+01, 5.5933e+00, 2.8628e+01,\n",
            "        8.7903e+00, 1.1557e+01, 0.0000e+00, 1.0539e+01, 7.7025e-01, 3.0084e+00,\n",
            "        4.4323e+01, 4.7758e+00, 1.5612e+00, 1.7250e+01, 1.5143e+01, 1.4854e+01,\n",
            "        1.6331e+00, 2.6746e+01, 3.4054e+01, 5.1250e+00, 9.2817e+00, 1.1613e+01,\n",
            "        1.1991e+01, 6.5137e+01, 7.0696e+01, 3.1487e+00, 1.1136e+01, 3.7730e+00,\n",
            "        5.7874e+01, 9.4086e+00], grad_fn=<SumBackward1>)\n",
            "tensor([1.1254e+01, 1.1424e+01, 8.0873e+00, 1.5687e+01, 1.5008e+01, 1.9466e-01,\n",
            "        5.2270e+00, 1.5033e+00, 9.4788e+00, 5.5149e-01, 6.7323e-01, 2.6914e+01,\n",
            "        1.4635e+01, 4.4899e+00, 4.6362e+00, 4.6784e-01, 2.5838e+01, 4.6130e-01,\n",
            "        9.9959e+00, 6.8137e+01, 1.4985e+01, 8.7687e+00, 3.9967e+00, 2.5638e+00,\n",
            "        5.0367e+00, 3.4889e+00, 6.6889e-01, 7.2380e+00, 1.2295e+01, 1.5265e+00,\n",
            "        8.7207e+00, 9.7292e+00, 2.5791e+01, 2.8484e+01, 1.1505e+01, 1.3320e+01,\n",
            "        1.3046e+01, 3.5957e+00, 2.6034e+01, 1.1844e+01, 1.2030e+01, 1.4617e+01,\n",
            "        1.4119e+01, 1.2755e+01, 7.3401e+00, 1.6978e+01, 5.5115e+00, 1.8872e+01,\n",
            "        1.4397e+01, 1.1980e+01, 1.8859e+01, 2.3388e+01, 1.8866e+01, 3.5685e+00,\n",
            "        1.6507e+01, 1.2180e+01, 1.0622e+01, 2.2242e+01, 1.1779e+00, 1.4565e+01,\n",
            "        1.6417e+01, 2.4796e+00, 2.4172e+01, 5.7718e+00, 1.3854e+00, 9.7767e+00,\n",
            "        5.8560e+00, 1.1037e-01, 2.8005e+00, 2.6656e+00, 4.6829e-01, 1.4132e+00,\n",
            "        1.5244e+00, 3.1702e+01, 2.5153e+01, 5.4679e+00, 3.1848e+00, 6.6228e+00,\n",
            "        2.4291e+01, 1.7250e+01, 4.3492e+00, 1.5487e+01, 1.4683e+02, 1.1165e+02,\n",
            "        5.5516e+01, 1.2659e+02, 2.7805e+00, 9.0196e+00, 2.7227e+01, 8.1267e+00,\n",
            "        3.5476e+01, 7.9459e+00, 7.8667e+01, 9.3257e+00, 1.4370e+01, 2.4644e+01,\n",
            "        2.5390e+01, 3.1215e+00, 2.6806e+01, 4.4205e+00, 5.4273e+00, 1.1748e+01,\n",
            "        9.5703e+00, 1.2362e+00, 6.5412e+00, 1.4541e+01, 1.4329e+01, 4.3311e-01,\n",
            "        2.7018e+01, 2.5256e+01, 4.1417e+01, 1.5830e+01, 1.0279e+00, 5.1142e+00,\n",
            "        4.9926e-01, 1.5252e+01, 6.9277e+00, 3.8402e+01, 2.7065e+01, 2.3257e+00,\n",
            "        3.5589e+01, 2.9842e+00, 2.0220e+01, 2.8496e+01, 1.6796e+01, 4.0844e+01,\n",
            "        1.4387e+00], grad_fn=<SumBackward1>)\n",
            "tensor([ 38.0998,  14.5359,   7.1633,   7.3485,  13.2894,  16.3773,   4.6134,\n",
            "         11.6861,  15.3156,   2.9357,   7.7158,   5.3416,  16.2649,   6.2358,\n",
            "         11.8879,  12.4135,   1.9740,  10.3102,  24.4710,  16.7723,   5.3687,\n",
            "         30.7332,  15.3024,   7.8053,   8.2656,  34.5173,  10.5272,   2.0594,\n",
            "          5.9254,   5.1440,  12.5330,   9.0279,  10.8773,  26.2965,  26.2833,\n",
            "         48.0071,  16.2301,   2.5772,  39.5348,  12.1073,   7.9856,  11.0220,\n",
            "          9.2361,  15.0980,   7.5139,  11.4419,  25.1990,  11.4507,   1.7808,\n",
            "          1.6898,   3.3985,   9.6078,  10.5639,  11.8072,  25.7949,   9.8906,\n",
            "         10.3863,  12.6799,   4.8838,   7.1763,   3.6870,  15.0008,  32.1548,\n",
            "          4.0384,  16.0997,   2.8125,  13.0984,  10.0780,  23.4131,   5.7224,\n",
            "          6.9848,  11.2741,  17.6914,  35.5956,  14.9057,  11.5378,   7.3559,\n",
            "          4.1532,  15.8304,   6.3824,   3.0784,   4.6263,   0.4238,   5.5309,\n",
            "          5.8144,  25.1582,  12.1810,   6.9689,   1.7363,   0.4752,   2.2356,\n",
            "          2.7624,  11.7113,   3.4023,   1.8959,  16.0435,   2.3900,  30.2485,\n",
            "        112.6489,  44.3076,  32.4479,   9.8134,   5.6312,   6.3353,   8.9451,\n",
            "          7.9386,   8.8908,  12.6645,   8.1092,  24.2645,   3.3538,   2.8385,\n",
            "         81.8750,  20.7429,  85.8481,  97.9323,  25.2034,   6.3855, 211.3336,\n",
            "         11.6845,  18.5551,   5.5639, 239.7199,  10.2966,   7.0819,  48.6687,\n",
            "         10.7647,  38.4535,  10.2200,   3.2717,  65.0832,   4.4501,   4.1987,\n",
            "          1.1025,   2.7638,  19.2095], grad_fn=<SumBackward1>)\n",
            "tensor([6.1087e+00, 6.4434e+00, 7.5733e+00, 7.3099e+00, 1.3608e+00, 7.7020e+00,\n",
            "        1.3624e+01, 8.7707e+00, 7.3789e+00, 2.9146e+01, 1.0100e+01, 2.3248e+01,\n",
            "        4.2541e+00, 5.0307e+00, 1.7878e+01, 1.0582e+00, 1.5820e+01, 6.7816e+00,\n",
            "        8.1067e+00, 2.8420e+00, 9.0673e+00, 1.3805e+00, 6.9837e+00, 2.2390e+01,\n",
            "        1.5908e+00, 2.0469e+01, 6.4518e+00, 1.0044e+01, 1.3055e+01, 6.7338e+00,\n",
            "        1.6529e+01, 2.8158e+01, 3.7874e+00, 2.0178e+00, 5.3115e-01, 4.1406e-01,\n",
            "        1.9063e+00, 1.1840e+01, 3.1457e+01, 1.9285e+01, 3.8834e+00, 1.2991e+01,\n",
            "        9.3116e+00, 1.0957e+01, 2.2399e+00, 5.1417e+00, 1.4697e+01, 5.2302e+00,\n",
            "        1.7473e+01, 3.4660e+01, 4.9017e+01, 4.5594e+00, 2.6291e+00, 3.6758e+00,\n",
            "        1.0789e+01, 3.8535e+01, 1.0757e+01, 1.4216e+01, 4.7333e+00, 8.5167e-01,\n",
            "        9.1723e+00, 5.4956e+00, 1.4997e+00, 2.8264e+00, 1.1444e+01, 7.0805e+00,\n",
            "        8.5245e+00, 4.6797e+01, 7.7836e+01, 6.0091e+01, 7.9794e+00, 1.4243e+01,\n",
            "        2.0475e+01, 1.0757e+01, 5.6408e+00, 1.1449e+01, 9.7123e+00, 8.3029e+00,\n",
            "        1.1588e+01, 1.8009e+01, 2.0415e+02, 8.6987e+01, 1.1774e+02, 3.4312e+02,\n",
            "        8.5569e+01, 5.2273e+02, 3.7563e+02, 3.0225e+00, 3.1046e+00, 3.0802e+00,\n",
            "        1.3447e+00, 1.3055e+00, 2.2824e+01, 2.3264e+01, 4.8673e+01, 1.4392e+01,\n",
            "        1.0295e+01, 7.5596e+00, 7.1775e+00, 1.5288e+01, 2.8653e+01, 1.6890e+01,\n",
            "        5.9537e+01, 4.8967e+01, 1.5334e+01, 1.5838e+02, 2.3102e+02, 2.7443e+02,\n",
            "        2.7269e+02, 1.9525e+02, 9.7173e+00, 3.4012e+01, 1.1018e+01, 1.1602e+00,\n",
            "        2.1795e+01, 2.0331e+01, 1.5167e+02, 1.3920e+02, 5.9115e+01, 1.9035e+01,\n",
            "        8.6855e+01, 8.3638e+00, 1.4878e+02, 2.2184e+02, 4.7367e+01, 3.3218e+02,\n",
            "        4.5370e+01, 5.0919e+02, 1.1638e+02, 1.2716e+02, 9.6689e+01, 5.3547e+00,\n",
            "        3.6106e+01, 9.0444e+00, 7.2425e-01, 7.5794e+00, 2.3799e+02, 2.3520e+02,\n",
            "        9.8314e+01, 1.1058e+01, 4.6236e+02, 6.3458e+01],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([3.7746e+00, 4.5250e+00, 4.6418e+00, 2.4734e+01, 6.1302e+00, 9.5375e+00,\n",
            "        3.5162e+00, 8.7468e+00, 5.9607e+00, 1.2567e+01, 3.8048e+00, 5.8579e+01,\n",
            "        1.6710e+01, 1.4954e+01, 2.8847e+01, 1.1061e+01, 6.3345e+01, 1.5544e+01,\n",
            "        1.3719e+01, 3.3509e+00, 3.8182e+00, 7.2422e+00, 1.2209e+01, 1.0064e+01,\n",
            "        2.8451e+00, 4.7396e+00, 6.3198e+00, 1.0011e+01, 7.0988e+01, 1.8074e+01,\n",
            "        1.6135e+01, 1.5325e+01, 2.5694e+01, 3.4961e+01, 4.0004e-01, 2.6081e+01,\n",
            "        9.1562e+00, 0.0000e+00, 1.5773e+00, 2.1537e+00, 2.0511e+00, 5.9212e+00,\n",
            "        6.1334e-01, 6.1454e-01, 2.7404e+00, 1.0347e+01, 1.3502e+01, 3.3641e+01,\n",
            "        1.5705e+01, 1.1313e+01, 2.4453e+00, 2.7103e+01, 1.6094e+01, 1.4288e+01,\n",
            "        6.9691e+00, 1.8160e+01, 1.0526e+00, 7.8332e+01, 2.0607e+01, 1.1338e+01,\n",
            "        1.4546e+01, 1.2106e+01, 9.9572e+00, 6.0219e+00, 1.1768e+01, 5.4846e+00,\n",
            "        2.0072e+00, 1.7897e+01, 1.1309e+01, 6.0597e+00, 5.3022e+00, 5.7057e+00,\n",
            "        1.7819e+01, 3.4168e+00, 8.2826e+00, 9.3415e+00, 2.9914e-01, 5.0883e+00,\n",
            "        5.1712e+00, 7.4677e+00, 3.5078e+01, 6.1285e+01, 5.1054e+01, 2.8570e+00,\n",
            "        3.3390e+01, 5.7011e+01, 3.0990e+01, 5.4711e+00, 8.8106e+00, 2.5738e+00,\n",
            "        4.9594e+00, 7.5328e+00, 4.9505e+00, 7.1367e+00, 5.7458e+01, 5.1546e+01,\n",
            "        2.7274e+00, 6.1259e+00, 1.6309e+01, 3.1126e+01, 1.5602e+01, 7.2508e+01,\n",
            "        9.8470e+00, 2.0154e-01, 1.9128e+01, 1.4084e+01, 1.3093e+01, 1.8319e+00,\n",
            "        8.1501e+00, 4.1215e+01, 9.8347e+00, 3.0914e+01, 3.2944e+01, 8.0355e+01,\n",
            "        1.6154e+01, 4.9093e+00, 2.5854e+01, 2.4953e+01, 1.0301e+01, 1.6880e+01,\n",
            "        6.3697e+01, 3.0580e+01, 9.4764e+00, 1.2069e+01, 4.3048e+01, 1.1623e+01,\n",
            "        5.1982e+00, 6.6987e+00, 1.7354e+00, 5.6081e+01, 5.5859e+00, 2.9601e+01,\n",
            "        1.6287e+01, 2.0056e+01, 5.7396e+01, 3.3860e+02],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([  4.0374,  17.7388,   8.6333,   0.7530,   3.9878,   7.7944,   6.7308,\n",
            "         12.8596,   0.4909,  21.2437,  11.5807,   6.8666,  10.7387,   2.4198,\n",
            "          5.8233,  18.5706,  13.1137,   9.6261,   7.8222,   3.1302,   0.1910,\n",
            "         20.8074,   7.8661,   4.1988,  14.5312,   8.1373,   3.6758,   0.5698,\n",
            "          5.9850,  11.7644,   1.8795,  21.7978,   1.9006,  13.0938,  10.6481,\n",
            "          5.7091,   8.7152,  13.3104,   1.2547,   0.0000,   3.0790,   1.4068,\n",
            "         10.2091,   5.1044,   2.7159,  14.2668,  12.3725,  20.1391,   5.5145,\n",
            "         16.6198,   3.1397,  11.0386,   1.5688,   4.5721,   2.0236,  36.7143,\n",
            "          9.8268,   4.0144,  10.0465,   7.9399,   3.0506,  11.7754,   7.0641,\n",
            "          5.7073,  14.7865,  17.5461,  19.9080,  10.1228,  23.0842,   0.0000,\n",
            "         33.0419,   4.4264,   6.0639,  12.8464,   9.7038,   2.7656,  10.5356,\n",
            "         19.7977,   7.0699,  18.1150,  10.9028,  19.5226,   6.5945,   0.3310,\n",
            "          7.6099,   6.9836,  35.6972,  12.2307,   5.9823,   8.8050,   7.4340,\n",
            "          8.2798,   6.7714,   3.0493,  46.0191,  11.3200,   7.9118,  11.2374,\n",
            "          2.5350,   4.2958,   5.8549,  20.0827,   5.0769,   5.5627,  19.7491,\n",
            "         15.0618, 105.1495,  13.6635,  26.1205,  36.2926,  42.0010,  22.6188,\n",
            "         50.0634,  11.6068,  16.3925, 167.8344, 153.2225,  78.3519,   5.3489,\n",
            "        139.2686,  12.7079,   9.2826,  14.9802,  20.4745,  76.7972,  67.3852,\n",
            "         37.6870,  34.3537,  11.2339,  30.6128,  50.8381,  38.3349,  79.1200],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([ 1.5036, 15.6943,  7.4710,  2.7349,  8.7667,  9.8702,  9.1398, 24.4322,\n",
            "         4.9296,  0.9170,  2.7795, 11.2580,  3.8178,  6.7965,  1.6980,  3.5082,\n",
            "         3.9231,  1.5019,  4.4418, 23.0877,  2.4250,  4.6236,  2.4888,  2.5457,\n",
            "         5.2640,  7.4937,  8.9239,  9.3967,  4.0388, 22.2285,  7.5428,  7.2156,\n",
            "         8.1673,  6.9946,  3.7139,  4.7536,  1.2745,  6.0646, 16.4083, 14.1136,\n",
            "        21.3291, 11.1828, 10.8548,  5.2993, 16.9643,  1.0986, 11.6599, 27.5246,\n",
            "         0.3109,  3.7067,  8.5639,  7.5851,  8.3658,  7.9621, 37.6167,  0.8123,\n",
            "         2.7402,  1.2788,  7.0547, 66.2985,  5.0935, 13.7044,  2.6134, 27.8231,\n",
            "         4.0253, 10.3982,  4.2600, 10.0117,  3.7056,  4.1351, 19.1239, 28.0124,\n",
            "        13.2721, 24.1182, 29.8226, 22.7891, 28.5041, 58.2865, 18.5411,  2.6990,\n",
            "         6.4587,  7.2255, 11.0995,  0.0000,  7.5126,  9.9606,  4.8740,  5.1531,\n",
            "         7.5694, 24.7047,  4.6956,  1.9361,  0.5174,  4.9186,  2.4087,  7.0348,\n",
            "         3.5560,  4.3777,  5.3785,  3.7990, 11.6874,  0.4996, 11.4006,  4.7411,\n",
            "        31.8010, 14.1375,  6.0860,  3.4492,  6.8301,  2.9126,  2.9377,  6.2996,\n",
            "        25.2992, 19.3565, 12.3820,  5.0173,  5.7136, 19.0746, 31.3501, 25.9536,\n",
            "        14.9059, 35.5855, 16.0418,  9.9090,  5.1512,  5.7322, 10.0914,  4.9200,\n",
            "        11.6651,  7.8327,  3.9606,  8.4777,  3.5675,  5.1408,  0.1691, 14.5510,\n",
            "         2.5382, 21.0658,  1.4171, 45.5410, 23.4722, 22.5689, 16.9800, 23.7909,\n",
            "        11.2436,  7.4827,  1.7363,  3.0262, 12.1986, 15.9129, 11.4477,  8.8205,\n",
            "        13.0643, 23.6385,  8.7308, 26.1314, 21.8413, 25.0249,  6.2218],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([1.0272e+01, 7.1441e+00, 5.3077e-01, 3.0736e-01, 2.2485e+01, 4.1111e+00,\n",
            "        8.3593e+00, 1.6016e+01, 9.0827e+00, 3.6106e-01, 1.7142e+01, 7.9286e+00,\n",
            "        6.3669e+00, 1.7550e+00, 3.1916e+00, 3.5082e+00, 4.5949e+00, 5.0477e+00,\n",
            "        2.3274e+00, 6.4185e+00, 1.7847e+01, 7.9444e+00, 1.6664e+01, 2.3228e+01,\n",
            "        2.5805e+01, 1.6966e+01, 7.5851e-02, 9.5852e+00, 9.8342e+00, 1.0087e+01,\n",
            "        1.2363e+01, 9.8563e+00, 3.5291e+00, 2.6912e+01, 8.0840e+00, 3.2386e+00,\n",
            "        8.5103e+00, 7.5089e-01, 6.5977e+00, 4.5675e+00, 4.7186e+00, 1.4103e+01,\n",
            "        9.3318e+00, 1.5241e+01, 5.4476e+00, 1.3263e+01, 2.0041e+01, 3.9445e+01,\n",
            "        1.1942e+01, 2.3553e+00, 8.4584e+00, 1.0291e+01, 6.2864e+00, 3.0925e+00,\n",
            "        1.1025e+01, 4.0767e+00, 5.2347e+00, 5.7043e+00, 1.3226e+01, 4.3280e+00,\n",
            "        4.7151e+00, 1.4014e+01, 1.1593e+00, 4.0861e+01, 1.4635e+01, 2.0948e+01,\n",
            "        3.2196e+01, 1.6047e-01, 2.2974e+00, 1.7782e+00, 3.0833e+00, 2.4045e+00,\n",
            "        4.5102e+00, 1.3346e+00, 2.1912e+00, 8.7475e+00, 1.2508e+00, 4.1703e-01,\n",
            "        1.6902e+01, 5.5220e+00, 1.4498e-01, 9.1196e+00, 7.9135e+00, 1.1917e+01,\n",
            "        9.4439e+00, 9.9233e+00, 8.9240e+00, 2.3087e+01, 5.0806e+01, 5.4321e+00,\n",
            "        2.7003e+01, 9.0795e+00, 6.4172e+00, 1.5008e+00, 1.1210e+01, 2.0389e+01,\n",
            "        6.1562e+00, 9.5410e+00, 7.9128e+00, 3.2419e+01, 9.2935e+00, 5.6577e+00,\n",
            "        2.0873e+01, 1.0656e+01, 9.9009e+00, 8.3425e+00, 2.2656e+01, 1.3979e+01,\n",
            "        3.4631e+00, 1.9634e+00, 6.1323e+00, 5.2577e+00, 5.1029e+00, 2.7083e+01,\n",
            "        1.3600e+01, 4.1333e+01, 3.3874e+00, 1.3253e+01, 1.1781e+01, 1.0263e+01,\n",
            "        1.5257e+01, 1.0854e+01, 1.3337e+01, 7.5727e+00, 2.0669e+00, 7.2980e+00,\n",
            "        2.1036e+01, 1.8221e+01, 1.7884e+01, 2.6234e+00, 2.4419e+01, 1.9388e+00,\n",
            "        9.9439e+00, 1.3337e+00, 1.8209e+01, 3.2442e+01, 1.1728e+02, 2.2125e-02,\n",
            "        3.3599e-01, 1.6976e+01, 2.2506e+00, 2.9370e+00, 8.8523e+00, 4.3595e+00,\n",
            "        8.7907e+00, 1.0361e+01, 2.8983e+01, 9.0925e+00, 4.4439e+00, 8.6688e+00,\n",
            "        2.1438e+01, 3.8670e+00, 1.0325e+01, 9.6277e+00, 1.6850e+01, 1.3567e+01,\n",
            "        8.6814e+00, 2.2476e+01, 2.6609e+01, 1.0060e+02, 7.0047e+01, 3.6097e+01,\n",
            "        1.1507e+01, 1.9593e+01, 6.4310e+01, 8.0585e+00, 5.9694e-03, 8.6115e+00,\n",
            "        1.1268e+01, 6.2062e+01, 1.6831e+01, 3.9261e+01, 5.5696e+00, 1.7615e+01,\n",
            "        8.5153e+00, 2.0162e+01, 1.0729e+01, 1.8161e+01, 7.2339e+01, 3.8046e+01,\n",
            "        2.4105e+02, 2.7329e+01, 3.0177e+01, 2.7317e+01, 1.0977e+02],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([4.3985e+00, 1.3830e+01, 5.5456e+00, 1.4284e+00, 2.8614e+00, 2.4398e+00,\n",
            "        3.0311e+01, 2.5382e+00, 2.7451e+01, 3.9518e+00, 1.8156e+00, 6.0431e+00,\n",
            "        9.1970e+00, 1.9737e+01, 5.7662e+00, 1.1067e+01, 2.2220e+01, 1.1507e+01,\n",
            "        5.1472e+00, 6.8227e+00, 3.4269e+00, 1.2005e+01, 2.2020e+00, 7.1017e+00,\n",
            "        3.2073e+00, 1.6642e+01, 5.8299e+00, 2.6913e+00, 6.9133e+00, 1.5772e+01,\n",
            "        4.0274e+01, 7.0884e+00, 8.7380e+00, 3.4923e+00, 1.5012e+00, 3.9820e+00,\n",
            "        9.6144e-01, 9.1630e+00, 1.1726e+01, 2.3758e+00, 4.3019e+00, 4.9996e+00,\n",
            "        8.3779e+00, 4.1963e+00, 6.1164e+00, 1.5107e+00, 1.7290e+01, 1.1877e+01,\n",
            "        4.6130e+00, 4.7757e+00, 1.6705e+01, 4.6683e+01, 1.6681e-01, 3.4871e-01,\n",
            "        4.4887e+00, 1.6157e+01, 9.0680e+00, 5.5586e+00, 1.0029e+01, 4.5276e+00,\n",
            "        1.9434e+00, 1.0627e+00, 2.0515e+00, 2.6187e+00, 7.8227e+00, 8.8385e+00,\n",
            "        1.5882e+01, 2.0747e+00, 1.4861e+01, 1.3299e+01, 8.6370e+00, 1.4328e+01,\n",
            "        3.3477e+01, 2.0297e+01, 2.6990e+01, 2.4331e+00, 2.4669e+00, 1.4126e+01,\n",
            "        2.4216e+00, 2.3468e+00, 8.9236e+00, 1.2771e+01, 1.4010e+01, 3.7309e+00,\n",
            "        6.6789e+00, 1.4923e+01, 1.4408e+01, 2.5991e+01, 1.7229e+01, 1.3677e+00,\n",
            "        3.2241e+00, 6.8030e+00, 1.2065e+01, 3.6702e+01, 2.1844e+01, 2.3990e+01,\n",
            "        3.5973e+00, 8.7933e+00, 6.2932e+00, 8.0420e-01, 2.8210e+00, 7.3421e+00,\n",
            "        2.3270e+00, 1.8172e+00, 7.2999e+00, 2.1466e+00, 1.1482e+01, 9.4212e+00,\n",
            "        2.3358e+01, 9.6349e+00, 3.4704e+01, 8.1906e+00, 2.9970e+00, 1.0400e+00,\n",
            "        1.2115e+01, 4.9164e+00, 7.0122e+00, 7.0344e+00, 9.3375e+00, 3.2194e+01,\n",
            "        4.9711e+00, 5.4566e+01, 5.2367e+00, 7.6586e-01, 7.2455e+00, 9.4670e+00,\n",
            "        3.9977e+01, 4.2398e+00, 2.2406e+01, 1.6777e+01, 1.7604e+01, 6.2657e+00,\n",
            "        7.9472e+00, 1.6808e+01, 2.2219e+01, 4.5143e-01, 2.1356e+01, 5.1650e+01,\n",
            "        9.3181e+00, 3.9629e+01, 1.9132e+01, 1.5079e+00, 9.2314e+00, 3.6927e+01,\n",
            "        5.5374e+01, 1.0821e+01, 2.9622e+01, 1.3353e+01, 6.8402e+00, 1.0917e+01,\n",
            "        1.1985e+01, 8.0077e+00, 4.8246e+00, 4.4431e+01, 6.9866e+01, 5.6104e+01,\n",
            "        6.9299e+01, 1.9687e+01, 4.2028e+01, 2.4160e+01, 6.8519e+00, 1.0281e+01,\n",
            "        5.3266e+00, 2.9027e+00, 2.3359e+00, 1.8082e+00, 4.0177e-01, 6.7844e-02,\n",
            "        1.3014e+00, 1.4233e+00, 5.2657e+00, 3.8003e+00, 2.0368e+01, 1.5583e+01,\n",
            "        1.5161e+01, 1.7280e+01, 1.0172e+01, 2.5008e+01, 1.8665e+01, 1.1872e+01,\n",
            "        1.1833e+01, 2.8508e+01, 2.2253e+01, 7.0150e+00, 4.1489e+00, 3.8051e+01,\n",
            "        3.6373e+01, 1.2416e+02], grad_fn=<SumBackward1>)\n",
            "tensor([ 9.2657,  3.8324,  8.8224, 52.9267,  9.0882, 27.5571,  6.5257,  6.9680,\n",
            "        10.3264,  0.7653,  4.7132, 10.1404, 11.2931,  7.0134, 15.8875, 16.1385,\n",
            "         8.0703,  4.8619, 17.7062, 12.9078,  5.9936, 10.0881,  1.4583,  4.6491,\n",
            "        18.4092, 12.1847,  1.8311, 26.7348, 21.7403,  6.8314, 11.0763, 22.6108,\n",
            "        22.7700,  3.5154,  2.0181,  3.9176,  4.2979,  2.9711,  5.2416,  7.0373,\n",
            "        15.0035,  3.0134,  9.1667, 10.1612,  9.6657, 10.4676, 15.5397,  7.8416,\n",
            "         3.4541,  2.2573, 10.3994,  3.6710, 11.6327,  9.5248, 29.4471, 18.6606,\n",
            "        11.6290,  6.3584, 12.0337, 17.1399,  2.0596,  8.2270, 21.4003,  9.0732,\n",
            "         9.2105, 16.4025, 15.0942,  6.1439, 14.9529, 13.6901, 11.9688, 16.2489,\n",
            "         2.4528,  0.0835,  5.6906, 19.7484, 58.9771,  5.0945,  0.0000, 13.4476,\n",
            "        16.1769, 13.1700, 11.5946, 10.2802,  4.5314, 17.7055,  6.5991, 14.7349,\n",
            "         9.6372, 10.6574,  2.7321, 19.1190, 11.6686,  4.5543, 16.5811,  6.4708,\n",
            "         2.7478, 24.8735,  5.7544, 10.7915,  6.5787, 10.5573,  5.2596, 37.0768,\n",
            "        13.4543, 18.8911,  8.3799,  6.6273,  5.8465,  1.1051, 27.4485, 24.2371,\n",
            "         1.0308,  9.8424,  0.4556, 12.3337, 12.7570,  5.6951, 27.7936, 14.9830,\n",
            "        29.5390, 55.2717], grad_fn=<SumBackward1>)\n",
            "tensor([2.1891e+00, 3.6183e+00, 5.3024e+01, 7.1484e+01, 7.0434e+00, 3.6258e+00,\n",
            "        8.2202e+00, 2.3327e+01, 1.1059e+01, 4.2303e+00, 3.5889e+00, 5.9906e+00,\n",
            "        3.4197e+00, 8.2806e+00, 8.4919e+00, 6.4596e+00, 2.3550e+00, 1.4347e+01,\n",
            "        4.8263e+00, 3.4383e+00, 7.9737e+00, 2.1165e+00, 9.2492e+00, 1.6424e+01,\n",
            "        6.4065e+00, 2.2929e+01, 1.0742e+01, 1.2700e+01, 1.2125e+01, 1.3124e+01,\n",
            "        7.4291e+00, 1.6160e+01, 3.4008e+00, 2.2443e+01, 2.2501e+00, 6.7053e+00,\n",
            "        6.0427e+00, 1.5218e+01, 1.9530e+01, 6.8017e+00, 6.3505e+00, 5.1897e+01,\n",
            "        1.8465e+01, 9.0629e+00, 6.9636e+00, 8.3745e+00, 1.2029e+01, 4.4391e+00,\n",
            "        1.6464e+01, 6.3352e+00, 1.7995e+00, 7.0466e+00, 1.5241e+01, 1.0876e+00,\n",
            "        1.9117e+01, 2.2460e+01, 1.9953e+00, 2.4878e-02, 6.0763e+00, 5.3659e+01,\n",
            "        7.9720e+00, 1.0277e+02, 2.8429e+01, 1.4494e+01, 5.4967e+00, 3.8723e+01,\n",
            "        5.2130e+00, 1.5838e+01, 1.0805e+01, 1.0339e+01, 4.0386e+01, 5.7978e+00,\n",
            "        6.5685e+01, 1.9363e+01, 4.4161e+00, 2.1563e+00, 6.5381e+00, 3.1793e+01,\n",
            "        5.5517e+01, 2.6599e+01, 4.4353e+01, 4.4078e+00, 2.7357e+00, 1.2113e+01,\n",
            "        7.4058e+00, 1.7732e+01, 1.0815e+01, 7.8019e+00, 1.7645e+01, 7.3717e+01,\n",
            "        5.2157e+00, 2.1090e+00, 4.0796e+00, 1.7714e+00, 8.5481e+00, 3.1597e+01,\n",
            "        4.8894e+01, 4.4947e+01, 3.9661e+01, 1.1114e+01, 1.4095e+01, 9.9625e+00,\n",
            "        9.2660e+00, 2.5527e+01, 1.0655e+01, 6.1371e+00, 2.8943e+00, 8.3183e+00,\n",
            "        9.6103e-01, 1.0982e+01, 6.9485e+00, 2.3550e+01, 4.4459e+00, 1.3882e+01,\n",
            "        2.3004e+01, 2.6959e+01, 5.2289e-01, 4.3969e+01, 2.9980e+01, 5.2956e+01,\n",
            "        3.1943e+01, 3.6896e+01, 8.7499e+00, 7.2727e+00, 1.7759e+01, 6.3832e+01,\n",
            "        1.2961e+00, 1.0666e+01, 9.7421e+00, 2.0127e+01, 8.4339e+00, 1.3181e+01,\n",
            "        4.4801e+01, 2.0953e+01, 2.1391e+01, 3.2377e+02, 1.3092e+01, 7.0776e+00,\n",
            "        1.9341e+00, 4.6756e+01, 2.1308e+00, 2.4473e+00],\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([2.9354e+00, 1.7324e+00, 5.1394e+00, 1.0023e+00, 6.0585e+00, 1.6424e+01,\n",
            "        1.3327e+01, 2.6016e+01, 5.9212e+00, 9.7860e+00, 3.6243e+01, 7.4878e+00,\n",
            "        2.9101e+01, 5.4075e+00, 3.0091e+00, 9.3200e+00, 1.2373e+01, 1.6390e+01,\n",
            "        1.7956e+01, 2.4401e+01, 3.7495e+00, 5.2493e+00, 1.1390e+00, 1.7704e+01,\n",
            "        1.2670e+01, 1.3729e+00, 2.4011e+01, 1.8823e+01, 5.4215e+00, 7.8733e+00,\n",
            "        5.3598e+00, 2.2282e+00, 7.4692e+00, 1.5779e-01, 1.1574e+01, 2.1071e+01,\n",
            "        2.0973e+00, 8.2914e+01, 5.0479e+00, 6.1052e+01, 4.1081e+01, 3.4673e+01,\n",
            "        3.9380e+01, 1.8268e+01, 4.6146e+00, 3.5719e+00, 9.7750e+00, 1.6896e+01,\n",
            "        3.1545e+00, 5.4823e+00, 4.8932e+00, 8.6738e+00, 2.5661e+00, 7.4535e+00,\n",
            "        2.6145e+00, 1.8081e+01, 4.5891e+00, 6.1248e-01, 3.5701e+01, 7.8006e+01,\n",
            "        2.7949e+01, 4.4131e+00, 5.1573e+01, 2.4144e+01, 1.2128e+01, 1.5482e+01,\n",
            "        4.8188e+00, 1.5611e+01, 5.2705e+00, 7.8762e+00, 9.7122e+00, 1.5166e+01,\n",
            "        2.3540e+01, 3.7064e+01, 6.3025e+00, 5.2715e+00, 7.5401e+00, 1.8449e+01,\n",
            "        6.1618e+00, 1.7485e+01, 1.2488e+01, 1.7881e+01, 1.3644e+01, 7.2984e+01,\n",
            "        9.0476e+00, 1.8636e+01, 7.1406e+01, 9.9193e+00, 2.7583e+00, 2.0278e+01,\n",
            "        2.4150e+00, 6.7589e+01, 1.9456e+01, 5.0555e+00, 2.5179e+01, 1.1157e+01,\n",
            "        5.8627e+00, 1.1050e+01, 1.4124e+00, 6.8487e+00, 5.9821e+00, 2.2406e+02,\n",
            "        9.2332e+01, 4.2244e+01, 2.2234e+02, 2.6087e+01, 1.3787e+02, 5.0848e+01,\n",
            "        9.0428e+01, 1.5443e+01, 6.3692e+02, 1.8768e+02, 5.4726e+02, 3.4112e+02,\n",
            "        5.6814e+02, 5.3417e-01, 7.2791e-01, 6.2125e+02, 3.5962e+02, 3.9184e+01,\n",
            "        2.0612e+02, 9.4899e+01, 6.8364e+00, 2.2131e+00, 2.5740e+01, 1.7791e+01,\n",
            "        9.8674e+00, 9.1866e+01, 1.5819e+01, 1.7109e+01, 2.0730e+01, 7.4168e+00,\n",
            "        0.0000e+00, 3.4262e+01, 1.9301e+00, 1.1429e+01, 1.6553e+01, 1.2027e+01,\n",
            "        1.0589e+01, 1.1244e+01, 1.5416e+01, 2.3983e+01, 9.5683e+00, 9.7012e+00,\n",
            "        4.3880e+00, 9.6842e+00, 1.5503e+01, 1.9845e+01, 2.8001e+01, 1.0907e+02,\n",
            "        3.2418e+02, 9.5846e+00, 1.0136e+01, 1.4513e+01, 3.3220e+00, 1.5553e+01,\n",
            "        2.1347e+01, 5.4290e+01, 1.4151e+02, 6.1128e+00, 1.5099e+01, 1.7689e+01,\n",
            "        2.2636e+00, 1.0501e+01, 3.3300e+00, 6.8781e+00, 3.3859e+01, 4.2082e+01,\n",
            "        1.6114e+01, 1.9207e+01, 2.6911e+01, 1.0300e+01, 1.7143e+01, 5.7160e+01,\n",
            "        4.9088e+01, 4.4240e+00, 4.8677e+01, 1.3423e+01, 7.1461e+01, 3.3963e+01,\n",
            "        6.0812e+01, 4.6277e+00], grad_fn=<SumBackward1>)\n",
            "tensor([3.9049e+00, 1.8913e+00, 1.1661e+01, 8.1531e-01, 1.5156e+00, 1.5229e+00,\n",
            "        8.7002e-01, 4.8300e+00, 3.9863e+00, 4.2384e+00, 2.4162e+01, 6.3917e-01,\n",
            "        4.9056e+00, 1.5856e+01, 3.0131e+00, 1.0072e-01, 1.8152e+01, 2.4274e+00,\n",
            "        6.4004e+00, 1.2485e+01, 6.7297e+00, 7.9066e+00, 8.4330e+00, 1.0663e+01,\n",
            "        1.1069e+01, 1.2991e+01, 8.6361e+00, 3.2028e+00, 7.2618e+00, 6.9928e-01,\n",
            "        6.5969e+00, 3.2337e+00, 5.8732e+00, 1.0544e+01, 2.4791e+00, 1.2538e+01,\n",
            "        9.6225e+00, 4.4413e+00, 8.3839e+00, 8.1418e+00, 8.0009e+00, 1.3519e+01,\n",
            "        2.1687e+01, 1.6809e+00, 4.5816e+00, 2.2548e+00, 7.7023e+00, 1.0370e+01,\n",
            "        2.1820e+00, 9.5682e+00, 5.9874e+00, 2.0237e-01, 2.3733e+00, 2.5768e+00,\n",
            "        3.2632e+01, 1.4588e+00, 3.6546e+01, 2.9543e+00, 1.9715e+00, 2.1048e+00,\n",
            "        6.1789e-02, 1.1155e+01, 3.5699e+00, 7.0656e+00, 6.7297e+00, 4.7141e+00,\n",
            "        4.3587e+00, 2.5020e+00, 8.4184e-01, 1.6854e+00, 8.5158e+00, 4.9957e+01,\n",
            "        7.8953e+00, 2.5836e+00, 2.8093e+01, 1.4144e+01, 2.2847e+01, 1.1915e+01,\n",
            "        3.1620e+01, 1.1957e+01, 2.3776e+00, 3.2926e+01, 6.4871e+00, 4.8785e+01,\n",
            "        8.9174e+00, 1.6688e+01, 7.1287e+00, 2.7146e+00, 6.5761e+00, 2.8008e+01,\n",
            "        2.0890e+00, 1.1873e+01, 1.2281e+01, 4.5011e+00, 5.0454e+00, 3.9734e+00,\n",
            "        7.8642e+00, 1.3506e+01, 6.5147e+00, 2.7939e+00, 2.5666e+01, 2.6860e+00,\n",
            "        1.2351e+00, 4.5506e+00, 5.6097e+00, 2.6898e+00, 2.4805e+00, 2.0505e+01,\n",
            "        2.6191e+00, 7.2651e+00, 1.6499e+01, 1.1516e+01, 2.2048e+01, 1.1722e+01,\n",
            "        1.1790e+01, 6.9714e+00, 8.4819e+00, 4.3310e+00, 1.8268e+01, 2.3732e+01,\n",
            "        9.9113e+00, 2.1052e+00, 3.3884e+00, 1.2961e+00, 7.2359e+00, 1.3909e+01,\n",
            "        2.8987e+01, 5.0921e+00, 1.0925e+01, 4.2368e-01, 1.1035e+00, 1.0721e+01,\n",
            "        3.5326e+00, 1.1083e+01, 3.3433e+01, 8.6034e+00, 6.5930e+00, 1.3526e+01,\n",
            "        3.7938e+01, 2.0675e+01, 2.3959e+01, 6.1270e+01, 2.7961e+01, 4.0969e+00,\n",
            "        3.7998e+00, 1.3615e+01, 4.1855e+00, 1.7249e+01, 1.1650e+01, 9.7621e-01,\n",
            "        6.9054e-01, 1.7611e-01, 1.7439e+00, 2.3177e+00, 1.4784e+01, 2.6940e+01,\n",
            "        3.1629e+01, 4.4852e+01, 6.3192e+01, 3.6578e+00, 4.5044e+00, 2.0671e+01,\n",
            "        3.0228e+01, 1.4939e+01, 1.0074e+01, 9.9581e+00, 6.0649e+00, 2.5823e+01,\n",
            "        1.0937e+01, 5.1590e+01, 3.3780e+01, 1.0017e+01, 2.1134e+01, 4.2890e+01,\n",
            "        2.7882e+01, 2.8893e+01, 3.9715e+00, 6.2804e+00, 5.0663e+00, 3.0349e+00,\n",
            "        6.0099e+00, 7.0130e+01, 8.5999e+00, 3.1225e+00, 1.3389e+00, 1.1612e+01,\n",
            "        9.4998e+00, 5.7301e+00, 3.0064e+01, 2.7229e+01, 1.1492e+00, 6.0264e+00,\n",
            "        1.6860e+01, 6.6014e+00, 3.2183e+01, 1.1755e+02, 1.0693e+01, 3.0450e+01,\n",
            "        2.1062e+01, 1.3086e+01, 2.2694e+01, 8.1055e+01, 3.8857e+01, 1.0950e+02,\n",
            "        7.9777e+01], grad_fn=<SumBackward1>)\n",
            "tensor([1.9569e+01, 2.9716e+00, 1.9845e+01, 3.8403e+01, 1.9902e+01, 4.0933e+00,\n",
            "        1.4094e+01, 2.7375e+00, 1.0843e+01, 9.6756e+00, 2.7357e+01, 9.8364e+00,\n",
            "        3.4031e+01, 7.9286e+00, 1.3923e+01, 5.9262e+00, 6.2454e+00, 6.2096e+00,\n",
            "        1.1216e+01, 4.2214e+00, 6.5252e+00, 2.7119e+01, 1.0647e+01, 2.3759e+01,\n",
            "        1.0824e+00, 1.6022e+01, 1.7367e+01, 2.0397e+01, 8.4432e+00, 5.3612e+00,\n",
            "        6.5191e+00, 1.0668e+00, 8.7097e-01, 9.5120e-01, 1.9643e+01, 2.9910e-01,\n",
            "        1.0001e+01, 1.6275e+01, 3.1469e+00, 8.2003e+00, 8.9312e+00, 1.7878e+00,\n",
            "        1.7302e+00, 1.0174e+01, 2.6652e-01, 1.7586e+00, 2.0345e+01, 3.7344e+01,\n",
            "        5.6770e+00, 9.9180e+00, 8.6155e+00, 6.3973e+00, 9.4971e+00, 1.0657e+01,\n",
            "        1.4022e+01, 4.4038e+00, 4.8742e+00, 1.3088e+01, 8.3705e+00, 2.4189e+01,\n",
            "        3.9297e+00, 4.5497e+00, 3.5681e+01, 1.8851e+01, 3.1583e+01, 8.2370e+00,\n",
            "        1.4812e+01, 1.5328e+01, 1.1267e+01, 1.5448e+01, 1.8553e+01, 6.7705e+00,\n",
            "        2.1913e+00, 4.1543e+00, 2.8318e+00, 1.5606e+01, 1.2921e+01, 1.0844e+01,\n",
            "        2.1115e+01, 3.1535e+00, 5.1790e+00, 6.3848e+00, 1.5913e+01, 2.2261e+01,\n",
            "        6.9069e+00, 5.1248e+00, 7.0856e+00, 3.3958e+01, 1.6243e+01, 2.8356e+01,\n",
            "        1.4166e+00, 2.4528e+00, 9.6595e-01, 3.5490e+00, 3.9095e+00, 1.3015e+01,\n",
            "        2.1730e+01, 2.1181e+01, 1.2916e+01, 1.1377e+01, 1.0569e+01, 2.7749e+01,\n",
            "        9.8838e+00, 9.2827e+01, 1.7574e+01, 3.1922e+01, 9.0393e+00, 8.8731e+00,\n",
            "        5.5769e-01, 2.9635e+00, 4.9913e+00, 5.2522e+00, 4.2495e-01, 6.8726e+00,\n",
            "        3.3767e+00, 2.8158e+00, 1.5208e+01, 1.2565e+01, 2.6002e+01, 1.8787e+01,\n",
            "        5.8117e+00, 1.8099e+01, 5.4223e+00, 1.1304e+01, 2.0181e+00, 3.5706e+00,\n",
            "        1.6499e+01, 7.2517e+00, 1.3096e+01, 6.7422e+00, 8.7603e+00, 1.1720e+01,\n",
            "        2.2255e+00, 2.8132e+00, 2.5802e+00, 1.3639e+01, 5.6201e-01, 1.1429e+01,\n",
            "        2.7697e+01, 3.6525e+01, 1.2580e+01, 2.5006e+01, 3.0717e+01, 3.3183e+01,\n",
            "        1.5432e+01, 1.0532e+01, 3.9786e+00, 2.3489e-01, 2.5649e+00, 8.1405e+00,\n",
            "        3.9435e+00, 2.2368e+01, 3.6569e+01, 5.5173e+01, 4.4910e+01, 1.3402e+01,\n",
            "        3.6086e+00, 2.9364e+00, 5.8526e+00, 8.0318e+00, 3.5664e+00, 2.4930e+00,\n",
            "        9.3420e+00, 1.0873e+00, 1.2744e+01, 1.0947e+01, 8.4332e+00, 2.2598e+01,\n",
            "        2.8479e+01, 4.0681e+01, 3.2341e+00, 1.6778e+01, 1.0183e+01, 2.0584e+01,\n",
            "        8.9238e+00, 1.4226e+01, 1.6969e+01, 1.7045e+01, 9.3546e+00, 1.7408e+01,\n",
            "        2.2031e+01, 2.0088e+01, 8.1072e+00, 5.2477e-01, 5.4603e-02, 6.4068e+01,\n",
            "        7.4056e+00, 9.5412e+00, 9.7428e+00, 1.4649e+01, 1.2229e+01, 9.4131e+00,\n",
            "        1.1302e+01, 2.5876e+00, 3.9531e+01, 6.8599e+01, 4.8958e+01, 2.9411e+01,\n",
            "        4.8494e+01, 3.8323e+01, 4.5577e+01, 1.5873e+01, 2.0896e+01, 8.1236e+00,\n",
            "        9.9448e+01, 5.4243e+01, 1.3745e+01, 5.2969e+01, 1.1551e+02, 2.1168e+01,\n",
            "        6.5530e+01, 5.2028e+01, 5.6180e+01, 1.0747e+02, 2.6281e+01, 1.6476e+01,\n",
            "        8.9214e+01, 1.0060e+02, 1.9907e+01, 1.8284e+01, 6.3830e+00, 2.0307e+01,\n",
            "        6.8376e+00, 3.4119e+00, 6.1272e+00, 1.3460e+00, 3.1367e+00, 9.9191e+00,\n",
            "        2.7582e+00, 1.9932e+01, 9.9510e-01, 1.9393e+00, 1.4950e+00, 1.2579e+00,\n",
            "        4.9626e+01, 2.1572e+01, 7.4158e+01, 9.2473e+01, 8.9517e+00, 1.0417e+01,\n",
            "        1.7233e+01, 5.8847e+01, 7.2253e+01, 4.8474e+01, 3.4044e+01, 6.0418e+00,\n",
            "        5.7196e+01, 6.1975e+01, 4.0840e+01, 4.2005e+00, 5.5849e+00, 2.2902e+01,\n",
            "        3.1155e+01, 6.5495e+01, 2.3939e+01, 6.9337e+01, 1.9529e+01, 1.4162e+01,\n",
            "        1.6878e+01, 4.8792e+01, 1.9505e+01, 7.5282e+01, 7.1108e+01, 2.8742e+01,\n",
            "        2.7945e+01], grad_fn=<SumBackward1>)\n",
            "tensor([4.5489e+00, 2.5943e+00, 7.6322e+00, 1.2708e+01, 4.8039e+00, 2.9605e+00,\n",
            "        9.8077e+00, 4.4958e+01, 1.0235e+01, 2.0006e+00, 8.1788e-01, 6.3337e+00,\n",
            "        2.7717e+00, 7.3794e+00, 5.1769e+00, 4.7738e+01, 1.4683e-02, 5.4281e-01,\n",
            "        0.0000e+00, 4.1855e+00, 2.1532e+00, 0.0000e+00, 2.9925e-03, 7.8242e-01,\n",
            "        1.1294e+00, 1.0303e+01, 9.7932e+00, 7.4527e+00, 1.2917e+01, 3.6875e+00,\n",
            "        1.5162e+01, 6.1928e+00, 5.6333e-01, 1.2372e+01, 1.0407e+00, 1.1437e+01,\n",
            "        4.3345e+00, 1.5423e+01, 4.5723e+01, 2.5572e+01, 2.5850e+01, 1.5436e+00,\n",
            "        3.8506e+00, 1.0062e+01, 1.7287e+01, 1.3984e+01, 8.5293e+00, 2.4686e+01,\n",
            "        5.9655e+01, 5.3276e+01, 2.0970e+01, 3.2161e+01, 1.0191e+00, 1.6518e+01,\n",
            "        7.9848e+00, 3.6582e+01, 2.1619e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 4.5891e+01, 3.3186e+00, 5.0358e+00, 5.3888e+00, 3.7523e+00,\n",
            "        3.4605e+00, 3.1789e-02, 9.6685e+00, 7.6613e+01, 2.6812e+02, 8.9133e+01,\n",
            "        7.5711e-01, 4.3679e+00, 4.5022e-04, 6.3240e+00, 6.6563e+00, 8.6791e+00,\n",
            "        4.0246e+00, 1.3976e+01, 2.2682e+00, 2.6705e+01, 4.0328e+01, 3.4069e+01,\n",
            "        4.6330e+01, 1.1054e+01, 4.1177e+00, 6.5867e+00, 2.8335e+00, 1.9055e+01,\n",
            "        7.9855e+00, 4.3501e+00, 2.1126e+01, 9.8411e+00, 1.3793e+01, 1.4809e+01,\n",
            "        1.4269e+01, 2.3908e+01, 2.9106e+01, 5.1024e+00, 2.4612e+01, 2.0289e+01,\n",
            "        6.2841e+00, 1.3413e+01, 8.9046e+00, 1.5367e+01, 2.2595e+01, 1.0285e+01,\n",
            "        4.6459e+00, 5.3116e+00, 2.3275e+01, 5.8698e+00, 8.3153e+01, 9.2346e+00,\n",
            "        1.0046e+01, 5.6941e+00, 4.3087e+00, 8.8355e+00, 1.0340e+00, 1.1650e+00,\n",
            "        3.1579e-01, 8.5767e+01, 1.9776e+01, 1.4915e+01, 2.6872e+01, 1.2308e+01,\n",
            "        2.7754e+01, 3.7853e+01, 2.9084e+01, 6.7284e+01, 6.5952e+01, 5.0936e+00,\n",
            "        9.1218e+00, 2.5358e+00, 2.0554e+01, 2.5600e+01, 9.5504e+00, 3.4161e+00,\n",
            "        7.1766e+00, 1.5844e+00, 3.7844e+00, 1.4796e+00, 3.6583e+00, 3.2218e+00,\n",
            "        3.9747e+01, 2.2461e+01, 3.4876e+01, 1.9008e+01, 8.2470e+01, 3.5150e+01,\n",
            "        6.7402e+01, 3.5757e+01, 6.1923e+01, 4.5800e+01, 3.2165e+01, 2.2266e+01,\n",
            "        3.9848e+00, 5.7619e+00, 1.2148e+01, 1.3479e+01, 1.7025e+01, 6.0358e+00,\n",
            "        1.2338e+01, 9.7342e+00, 3.3294e+01, 1.0621e+01, 1.0212e+01, 1.7024e+01,\n",
            "        1.8404e+01, 1.0852e+01, 2.4245e+01, 2.4545e+01, 1.0164e+00, 6.5708e+00,\n",
            "        7.5961e-01, 1.3517e+01, 2.0406e+01, 1.7176e+01, 2.2445e+01, 2.1772e+01,\n",
            "        3.3714e+01, 8.5207e+01, 9.4746e+00, 4.5548e+01, 3.0662e+01, 2.0551e+01,\n",
            "        1.9631e+01, 1.3142e+01, 1.8023e+01, 7.3378e+00, 2.3572e+00, 4.3274e+01,\n",
            "        2.7391e+01, 9.8838e+00, 2.2412e+01, 9.2974e+00, 1.7182e+01, 8.6671e+00,\n",
            "        8.6161e+00, 1.0483e+01, 4.0086e+00, 1.5544e+01, 2.7291e+01, 4.4418e+01,\n",
            "        3.1218e+01, 4.8248e+01, 5.6846e+01, 4.1339e+00, 6.7596e+01, 3.5038e+01,\n",
            "        3.7637e+01, 1.8242e+01, 5.4066e+01, 7.0426e+01, 3.0598e+01, 2.3896e+01,\n",
            "        2.2110e+01, 2.8024e+01, 2.4841e+01, 3.6029e+01, 7.0819e+01, 2.6731e+01,\n",
            "        1.8970e+01], grad_fn=<SumBackward1>)\n",
            "tensor([2.4387e+01, 2.6176e+01, 4.3698e+00, 4.4325e-01, 3.4620e+00, 8.3509e+00,\n",
            "        7.3764e+00, 3.7127e+00, 4.2925e+00, 1.4835e+01, 1.0500e+01, 1.5468e+00,\n",
            "        7.7472e+00, 3.9579e+00, 8.7967e-01, 1.8897e+00, 2.2137e+00, 1.0849e+01,\n",
            "        1.1981e+01, 4.4779e+01, 8.4075e-01, 2.2381e+01, 1.5728e+01, 1.2584e+01,\n",
            "        9.8659e+00, 1.4687e+01, 2.6157e+00, 3.7278e+00, 3.5648e+00, 1.5248e+01,\n",
            "        5.7327e+00, 1.5140e+00, 2.6772e+00, 2.6292e+00, 1.3288e+01, 4.8253e+00,\n",
            "        3.2030e+00, 8.8103e+00, 1.0796e+01, 1.6222e+00, 9.2099e-01, 1.1872e+00,\n",
            "        2.1721e+00, 5.3463e+00, 5.5513e+00, 4.5199e+00, 8.8927e+00, 2.3617e+01,\n",
            "        8.9309e+00, 3.3545e+01, 3.5107e+00, 1.0266e+01, 3.0406e+00, 1.5429e+01,\n",
            "        4.1576e+00, 1.6318e+01, 8.1990e+00, 3.3102e+00, 1.0338e+01, 1.6642e+01,\n",
            "        1.5539e+01, 1.6297e+01, 2.0574e+01, 2.4386e+01, 6.3239e+00, 2.0463e+00,\n",
            "        8.3802e+00, 2.8161e+01, 5.3218e+00, 9.5700e-02, 5.9827e-01, 4.0751e+00,\n",
            "        8.9587e+00, 9.8642e-01, 4.7214e+00, 1.4641e+01, 2.8027e+00, 1.3819e+01,\n",
            "        4.0281e+01, 2.5994e+00, 1.3486e+01, 1.0529e+01, 8.7247e+00, 1.8248e+01,\n",
            "        9.4576e+00, 1.0409e+01, 1.4958e+01, 6.3385e+00, 4.1751e+00, 5.3117e+00,\n",
            "        8.5383e+00, 1.3767e+01, 7.4256e+00, 9.7959e+00, 5.9784e+00, 1.3224e+01,\n",
            "        2.8255e+01, 1.1227e+01, 3.3573e+00, 3.9623e+00, 4.1812e-01, 2.1123e+00,\n",
            "        8.0564e+00, 5.5722e+00, 6.8376e+00, 1.4395e+01, 1.2165e+01, 2.4839e+01,\n",
            "        1.3529e+01, 6.0262e+01, 5.1390e+00, 9.4784e+00, 1.7608e+01, 1.3787e+01,\n",
            "        1.5337e+01, 5.7357e+00, 6.1416e+00, 1.0599e+00, 1.1016e+01, 1.4677e+01,\n",
            "        1.9089e+01, 1.1234e+01, 5.9824e+00, 4.8493e+00, 8.4253e+00, 1.3165e+01,\n",
            "        1.0219e+01, 1.4794e+01, 1.3481e+01, 1.6786e+01, 3.5130e+00, 2.5659e+01,\n",
            "        4.4701e+01, 2.9205e+00, 1.0086e+01, 1.2008e+01, 1.2722e+01, 1.7800e+01,\n",
            "        6.9532e+00, 5.6061e+00, 1.1746e+01, 1.2484e+01, 8.4877e+00, 2.7132e+01,\n",
            "        1.1368e+01, 5.3336e+01, 1.8126e+01, 2.1006e+01, 2.4969e+01, 5.5991e+01,\n",
            "        1.4770e+01, 1.0294e+01, 4.4864e+01, 2.6990e+01, 4.5277e+01, 3.8463e+02,\n",
            "        5.1581e+01, 7.0609e+01], grad_fn=<SumBackward1>)\n",
            "tensor([15.6179, 21.1719, 12.0767,  1.9345,  2.0873,  4.8904,  4.4330,  8.9021,\n",
            "         7.6242,  4.4344,  2.6486, 13.0161,  9.5949, 10.4397,  2.9814,  9.2600,\n",
            "        26.6698, 17.1144,  2.5684,  3.7620,  6.5263, 11.5404,  5.6256, 13.5305,\n",
            "        11.6159, 20.2916, 19.1202,  6.9207, 19.2578,  8.0499,  3.9370,  5.6220,\n",
            "         5.3412,  5.9195,  4.7662,  6.0920,  5.4256,  0.4220, 11.0990, 11.7823,\n",
            "         7.1465,  5.9468,  8.2283,  6.3757,  1.0428,  9.4012,  6.7319,  3.3812,\n",
            "         2.2666, 11.2452, 11.3454,  2.3709, 14.4967, 12.4265,  6.8181, 30.1700,\n",
            "        11.8669, 16.3738,  9.7007,  3.6315, 15.5319,  5.1529, 12.2382,  7.1729,\n",
            "         4.4433,  7.3341, 11.1563,  4.9999, 66.3739,  1.6067,  5.1033,  1.0149,\n",
            "         2.2360,  2.9122, 13.0623, 13.2468, 15.5676,  3.6369,  6.6807,  4.5412,\n",
            "         9.8561,  8.3134,  7.2182, 20.0826, 19.4111, 21.7505, 13.4159, 16.3805,\n",
            "        18.8419, 16.3121, 15.6223,  0.4687,  8.7427, 15.0872,  7.6306, 14.9036,\n",
            "         8.0557, 10.9097,  1.5301, 12.3755,  7.7265,  4.9083,  5.9059,  3.0917,\n",
            "         9.1706,  4.4981,  0.1706,  0.4214,  1.2917, 15.4120,  3.0146, 21.9447,\n",
            "         3.5948,  9.6504,  7.2646,  6.3595, 13.2437, 12.5360, 17.8503,  4.7108,\n",
            "         6.4697, 11.8339, 17.4924, 11.4612, 15.9254, 24.5556,  4.0116,  5.7643,\n",
            "        10.0981,  2.9328,  4.1892, 11.8467,  6.3440,  0.7126, 13.5166, 20.9536,\n",
            "         7.0553, 12.1157, 68.5323, 15.2203, 20.2559], grad_fn=<SumBackward1>)\n",
            "tensor([ 2.1461,  3.8701,  0.3560,  1.9748,  0.3501,  1.6595,  0.4619,  1.0482,\n",
            "         0.5769,  1.8453,  1.9495,  1.1510,  2.8860,  1.2027,  0.0787,  1.4945,\n",
            "         3.9168, 13.6632,  3.1647,  1.1097,  0.3809,  2.6408,  0.2839,  1.3435,\n",
            "         0.3778,  0.3638,  0.2606,  2.0335,  0.4880,  6.3560,  8.1215,  0.9622,\n",
            "         2.5524,  0.2564,  0.9845,  2.2204,  7.9777,  3.5555,  2.2408,  3.6120,\n",
            "         2.3234,  4.9361,  2.4933,  6.1459, 12.1877,  3.3136,  4.3146,  3.3113,\n",
            "         1.4938,  3.6720, 10.4821,  4.3034,  5.3749,  3.8178,  1.5417,  2.8519,\n",
            "         2.5454,  3.9156,  3.5537,  0.6292,  0.0375,  5.8181,  4.4405,  2.3257,\n",
            "         4.5808,  0.3267,  0.3195,  0.0907,  0.9765,  0.9545,  1.3543,  0.8024,\n",
            "         0.9792,  0.2584,  1.0558,  1.4137,  0.5147,  0.9156,  0.1398,  1.4915,\n",
            "         3.9443,  2.6293,  4.1992,  1.4992,  2.5374,  0.2099,  5.2419, 13.6467,\n",
            "         5.3010,  3.7608,  4.0466, 13.9282,  0.7540,  0.0361,  5.5520,  5.7761,\n",
            "         0.7795,  0.2843, 10.5428,  8.4358,  0.0000,  5.4626, 22.2099],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a5bb9ef1e437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                                             \u001b[0;34m,\u001b[0m \u001b[0mfalse_edges_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimesteps_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtimesteps_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                             \u001b[0;34m,\u001b[0m \u001b[0madj_dense_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimesteps_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtimesteps_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                             \u001b[0;34m,\u001b[0m \u001b[0mpriors_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                                             )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-58ec3efc398f>\u001b[0m in \u001b[0;36mget_eval_scores\u001b[0;34m(edges_pos, edges_neg, adj_dense_matrix, a_embs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mauc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_neg_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
          ]
        }
      ]
    }
  ]
}